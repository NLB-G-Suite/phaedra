{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479404\n",
      "158959\n",
      "29086\n",
      "40\n",
      "73\n",
      "304\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import codecs\n",
    "from utils import in_blocklist\n",
    "\n",
    "blocklist = []\n",
    "blocklist.append('AWAYLEN=307 MAXTARGETS=20 WALLCHOPS WATCH=128 WATCHOPTS=A SILENCE=15 MODES=12 CHANTYPES=# PREFIX')\n",
    "blocklist.append('- * -')\n",
    "blocklist.append('XxXChatters.Com')\n",
    "blocklist.append('XxXChatters')\n",
    "blocklist.append('This server was created')\n",
    "blocklist.append('operator(s) online')\n",
    "blocklist.append('is now your displayed host')\n",
    "blocklist.append('Caps set:')\n",
    "blocklist.append('MAXCHANNELS')\n",
    "blocklist.append('http')\n",
    "blocklist.append('type it in the main room or a pm to the')\n",
    "blocklist.append('The topic is')\n",
    "blocklist.append('Topic set by')\n",
    "\n",
    "all_separated = []\n",
    "all_posts = []\n",
    "all_posts_410 = []\n",
    "all_posts_501 = []\n",
    "authors = {}\n",
    "authors_410 = {}\n",
    "authors_501 = {}\n",
    "\n",
    "def parse_all(files, separate):\n",
    "    for file in glob.glob(files):\n",
    "        lines = codecs.open(file, mode='r', encoding='utf-8')\n",
    "\n",
    "        for line in lines:\n",
    "            if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "                if line[0] is '[' and line.find(\"<\") is not -1:\n",
    "                    s = line\n",
    "                    sentence = s[s.find(\">\")+2:]\n",
    "                    time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "                    author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                    \n",
    "                    p = (time+' ' + author, sentence.strip().lower())\n",
    "                    all_posts.append(p)\n",
    "                    if separate in file:\n",
    "                        all_separated.append(p)\n",
    "\n",
    "                    if author not in authors: # save things\n",
    "                        authors[author] = set() # init list for author\n",
    "                    authors[author].add(sentence.strip().lower()) \n",
    "                else:\n",
    "                    if line.find('*') is not -1 and \") has joined #\" not in line and \") Quit (\" not in line and \") has left #\" not in line and \"6A\u000314thena\u000314, \u000f\u000306y\u000314o\u000314u \u000f\u000306n\u000314eve\u000314r\" not in line and \" sets mode: +\" not in line and \" is now known as \" not in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        sentence = line[aa_end + 1:]\n",
    "                        \n",
    "                        p = (time+' ' + author, sentence.strip().lower())\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "\n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence.strip().lower()) \n",
    "                    if \") has joined #\" in line or \") Quit (\" in line or \") has left #\" in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        if \") has joined #\" in line:\n",
    "                            author += \" JOINED THE ROOM\"\n",
    "                        else:\n",
    "                            author += \" LEFT THE ROOM\"\n",
    "                        sentence = \"\"\n",
    "                        p = (time+' ' + author, sentence)\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "                            \n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence) \n",
    "\n",
    "    for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "        if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "            sentence = line[line.find(\":\"):]\n",
    "            all_posts.append((author, sentence.strip().lower()))\n",
    "            all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "            author = line[:line.find(\":\")]\n",
    "            if author not in authors: # save things\n",
    "                authors[author] = set() # init list for author\n",
    "            if author not in authors_410:\n",
    "                authors_410[author] = set()\n",
    "            authors[author].add(sentence.strip().lower())\n",
    "            authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "    for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "        if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "            if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "                sentence = s[s.find(\">\")+2:]\n",
    "                all_posts.append((author, sentence.strip().lower()))\n",
    "                all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "                author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                if author not in authors: # save things\n",
    "                    authors[author] = set() # init list for author\n",
    "                if author not in authors_501:\n",
    "                    authors_501[author] = set()\n",
    "                authors[author].add(sentence.strip().lower())\n",
    "                authors_501[author].add(sentence.strip().lower()) \n",
    "        \n",
    "parse_all(\"xxxchatters_logs/*.log\", \"#chat\")\n",
    "#for file in glob.glob(\"xxxchatters_logs/*.log\"):\n",
    "print(len(all_posts))\n",
    "print(len(all_separated))\n",
    "print(len(authors))\n",
    "print(len(authors_410))\n",
    "print(len(authors_501))\n",
    "print(len(all_posts_410))\n",
    "print(len(all_posts_501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing to bring in notices\n",
    "lines = codecs.open(\"1206/xxxchatters_original_0223_#chat.txt\", mode='r', encoding='utf-8')\n",
    "\n",
    "for line in lines:\n",
    "    if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "        if line[0] is '[' and line.find(\"<\") is not -1:\n",
    "            s = line\n",
    "            sentence = s[s.find(\">\")+2:]\n",
    "            time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "            author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "\n",
    "            p = (time+' ' + author, sentence.strip().lower())\n",
    "            #all_posts.append(p)\n",
    "            #if separate in file:\n",
    "            #    all_separated.append(p)\n",
    "\n",
    "            #if author not in authors: # save things\n",
    "            #    authors[author] = set() # init list for author\n",
    "            #authors[author].add(sentence.strip().lower()) \n",
    "        else:\n",
    "            if line.find('*') is not -1 and \") has joined #\" not in line and \") Quit (\" not in line and \") has left #\" not in line and \"6A\u000314thena\u000314, \u000f\u000306y\u000314o\u000314u \u000f\u000306n\u000314eve\u000314r\" not in line and \" sets mode: +\" not in line and \" is now known as \" not in line:\n",
    "                time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                author_beginning = line.find(\"* \")\n",
    "                aa = line[author_beginning+2:]\n",
    "                aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                author = line[author_beginning+2:aa_end]\n",
    "                sentence = line[aa_end + 1:]\n",
    "                print(author)\n",
    "                print(time)\n",
    "                print(sentence)\n",
    "                print(line)\n",
    "            #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in authors['carrol']:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49231\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import string\n",
    "word_counts = dict()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess_word(word):\n",
    "    word = word.replace('\\x02', '')\n",
    "    word = word.replace('\\x03', '')\n",
    "    word = word.replace('.', '')\n",
    "    word = ''.join([i for i in word if not i.isdigit()])\n",
    "\n",
    "    #word = word.translate(translator)\n",
    "    word = ''.join(e for e in word if e.isalnum())\n",
    "    return word\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    word_list = post[1].split()\n",
    "    for word in word_list:\n",
    "        word = preprocess_word(word)\n",
    "        #word = word.replace('\\x03', '')\n",
    "        #word = word.replace('.', '')\n",
    "\n",
    "        #word = word.translate(translator)\n",
    "        #word = ''.join(e for e in word if e.isalnum())\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "def get_sorted_word_counts():\n",
    "    return sorted_word_counts\n",
    "\n",
    "v = get_sorted_word_counts()\n",
    "print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'quickly', 'wine', 'irc', 'extra', 'large', 'arm', 'past', 'rl', 'size', 'summer', 'leaves', 'devil', 'jag', 'able', 'naderaax', 'dylan', 'firm', 'kinky']\n"
     ]
    }
   ],
   "source": [
    "def get_topn_percent(perc):\n",
    "    counts = get_sorted_word_counts()\n",
    "    l = len(counts)\n",
    "    max_index = int(perc*l)\n",
    "    top_n = []\n",
    "    #print(counts[:max_index])\n",
    "    for t in counts[:max_index]:\n",
    "        top_n.append(t[0])\n",
    "    return top_n\n",
    "\n",
    "def get_all_by_max_appearance(app):\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] >= app:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "\n",
    "def get_by_appearance_list(appearance):\n",
    "    # appearance = [2, 3, 4] # number indicating times of appearance\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] in appearance:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "    \n",
    "#topn = get_topn_percent(0.001)\n",
    "\n",
    "selected_appearances = get_by_appearance_list([110, 112])\n",
    "print(selected_appearances)\n",
    "\n",
    "#maxapp = get_all_by_max_appearance(2)\n",
    "#file = open(\"1106/chat_room_word_appearance.txt\", 'w')\n",
    "#for e in maxapp:\n",
    "#    try:\n",
    "#        file.write(e[0] + \", \" + str(e[1]) + '\\n')\n",
    "#    except:\n",
    "#        pass\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- to keep being evil ---- athena on the -------- with a new nerf bat\n",
      "--- to keep being evil<<<< ---- athena on the -------- with a new nerf bat\n"
     ]
    }
   ],
   "source": [
    "def remove_non_frequent(allowed_words, post, replace=True):\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        pword = preprocess_word(word)\n",
    "        if pword in allowed_words:\n",
    "            good_words.append(pword)\n",
    "        else:\n",
    "            if replace:\n",
    "                replacement = \"\"\n",
    "                for char in word:\n",
    "                    replacement += \"-\"\n",
    "                good_words.append(replacement)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "\n",
    "def remove_by_probability(allowed_words, always_allowed, post, chance, replace=True):\n",
    "    import random\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        #pword = preprocess_word(word)\n",
    "        if word in always_allowed:\n",
    "            good_words.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in allowed_words:\n",
    "            good_words.append(word)\n",
    "        else:\n",
    "            if replace:\n",
    "                rv = random.uniform(0.0, 1.0)\n",
    "                if rv < chance:\n",
    "                    replacement = \"\"\n",
    "                    for char in word:\n",
    "                        replacement += \"-\"\n",
    "                    good_words.append(replacement)\n",
    "                else:\n",
    "                    good_words.append(word)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "sentence_to_test = \"Jus to keep being evil<<<< wops Athena on the backside with a new nerf bat\"\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n",
    "\n",
    "print(remove_by_probability(get_topn_percent(0.1), get_topn_percent(0.01), sentence_to_test, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- saffron on the -------- with one of my new ---- ----\n",
      "---- saffron on the -------- with one of my new nerf ----\n",
      "bops saffron on the -------- with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n",
      "bops saffron on the backside with one of my new nerf bats\n"
     ]
    }
   ],
   "source": [
    "sentence_to_test = \"bops saffron on the backside with one of my new nerf bats\"\n",
    "\n",
    "v = 0.0\n",
    "while v <= 1.0:\n",
    "    v += 0.04\n",
    "    print(remove_non_frequent(get_topn_percent(v), sentence_to_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@erotic_kitty', 65), ('carrol', 53), ('+DJ`Mercury', 36), ('Louise', 28), ('@erotic_kitty{S}', 27), ('Valkyrie', 21), ('Threeleggedcat', 20), ('PlayfulBBC', 18), ('@saffron{WH}', 17), ('WhoGiveSaDamn', 15), ('Silver_haired`Fox', 14), ('sweet_teresa', 10), ('Jothom', 8), ('Woman', 8), ('guynextdoor', 7), ('Sanger', 7), ('Anastatia', 7), ('^fran', 7), ('Cruel`Intentions', 7), ('Jaems', 6), ('+DJ`South', 6), ('JFetish', 5), ('FunkyBoogieKing', 5), ('@DJ`liltech', 4), ('MeanMark1', 4), ('Handyman', 4), ('gracie', 4), ('saffron{WH}', 3), ('FriendlyMonster', 3), ('rst37', 3)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_by_probability(authors_list, count, topn = 30):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for authors in authors_list:\n",
    "        for key, value in authors.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "                \n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #print(author_probability_counts[:topn])\n",
    "\n",
    "    sum_sentences = 0\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        sum_sentences += v[1]\n",
    "\n",
    "    author_probability_final = {}\n",
    "    author_list = []\n",
    "    prob_list = []\n",
    "\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        author_probability_final[v[0]] = v[1]/float(sum_sentences)\n",
    "        author_list.append(v[0])\n",
    "        prob_list.append(v[1]/float(sum_sentences))\n",
    "\n",
    "    final_autor_list = np.random.choice(\n",
    "      author_list, \n",
    "      count,\n",
    "      p=prob_list\n",
    "    )\n",
    "\n",
    "    return final_autor_list\n",
    "\n",
    "def get_topn_authors(author_lists, n):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for author_list in author_lists:\n",
    "        for key, value in author_list.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "\n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return author_probability_counts[:n]\n",
    "\n",
    "#print(get_by_probability(authors, 20))\n",
    "#print(get_by_probability(authors_410, 20))\n",
    "#print(get_by_probability(authors_501, 20))\n",
    "\n",
    "print(get_topn_authors([authors_410,authors_501], 30))\n",
    "#print(get_by_probability([authors_410,authors_501], 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load osc config and parse for later use\n",
    "import csv\n",
    "\n",
    "def load_osc_config(file_name):\n",
    "    config = {}\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter='\\t', quotechar='|')\n",
    "        index = 0\n",
    "        for row in reader:\n",
    "            if index > 0:\n",
    "                #print(row)\n",
    "                for i in range(len(row)):\n",
    "                    config[i].append(row[i])\n",
    "            else:\n",
    "                config[0] = []\n",
    "                config[1] = []\n",
    "                config[2] = []\n",
    "                config[3] = []\n",
    "                config[4] = []\n",
    "            index += 1\n",
    "    \n",
    "    return config\n",
    "            \n",
    "osc_config = load_osc_config('osc_config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-212-871762c2362e>:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "def generate_lines_linear(count):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        perc = len(return_lines)/float(count)\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(perc)\n",
    "        #print(len(topn))\n",
    "        #print(sentence)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if len(stripped_post) > 20 and stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "        \n",
    "def generate_lines_from(fromp, step, count):\n",
    "    for i in range(count):\n",
    "        sentence = random.choice(all_posts)\n",
    "        perc = fromp + i * step\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(len(topn))\n",
    "        stripped_post = remove_non_frequent(topn, sentence[1].strip().lower())\n",
    "        print(\"{:.6f}\".format(perc) + \": \" + stripped_post)\n",
    "        \n",
    "def generate_by_osc(config, lists, offset=90):\n",
    "    assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n",
    "\n",
    "    index = 0\n",
    "    possible_words = []\n",
    "    # length of generated sentences is determined by osc/curve data\n",
    "    length = len(config[0])\n",
    "    for l in lists:\n",
    "        _config = config[index]\n",
    "        _min = l[0]\n",
    "        _max = l[1]\n",
    "        app_list = []\n",
    "        for i in range(_min, _max+1):\n",
    "            app_list.append(i)\n",
    "        \n",
    "        # all allowed words\n",
    "        selected_appearances = get_by_appearance_list(app_list)\n",
    "        possible_words.append((selected_appearances, _config))\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    # all words with min appearance of 500 are always allowed\n",
    "    always_allowed = get_all_by_max_appearance(500)\n",
    "        \n",
    "    finished_sentences = []\n",
    "    for i in range(length):\n",
    "        #s = random.choice(all_posts)[1]\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        # don't process djs\n",
    "        if 'DJ`Mercury' in a or 'DJ`Protea' in a:\n",
    "            finished_sentences.append((a, s))\n",
    "            continue\n",
    "            \n",
    "        for p in possible_words:\n",
    "            _words = p[0]\n",
    "            probability = float(p[1][i])\n",
    "            s = remove_by_probability(_words, always_allowed, s, probability)\n",
    "        \n",
    "        finished_sentences.append((a, s))\n",
    "    \n",
    "    return finished_sentences\n",
    "\n",
    "def render_plain(out_filename, line_count, offset):\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for i in range(line_count):\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        outfile.write(a + ': ' + s + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "def render_by_osc(osc_config, out_filename, offset):\n",
    "    lists = []\n",
    "    lists.append((1, 5))\n",
    "    lists.append((6, 20))\n",
    "    lists.append((21, 50))\n",
    "    #lists.append((51, 100))\n",
    "    #lists.append((101, 500))\n",
    "    osc_config = load_osc_config(osc_config)\n",
    "    generated = generate_by_osc(osc_config, lists, offset)\n",
    "\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for line in generated:\n",
    "        outfile.write(line[0] + ': ' + line[1] + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = get_by_probability([authors_410,authors_501], 300)\n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(1)\n",
    "\n",
    "def generate_lines_maxapp(count, maxapp):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        #perc = len(return_lines)/float(count)\n",
    "        #perc = 0.5\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        #topn = get_all_by_max_appearance(maxapp)\n",
    "        topn = get_topn_percent(0.025)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if len(stripped_post) > 80 and stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "outfile = codecs.open('1306_only_top0.025_longerthan80_01.txt', mode='w')\n",
    "\n",
    "# 20 top authors from 401 & 510\n",
    "auth_text = get_by_probability([authors_410,authors_501], 300, 20)\n",
    "auth_io = get_topn_authors([authors_410,authors_501], 30)[-10:] # get_by_probability([authors_410,authors_501], 50, 30)[-10:]\n",
    "#print(auth_text)\n",
    "#print(auth_io)\n",
    "\n",
    "# 300 lines\n",
    "index = 0\n",
    "lines = generate_lines_maxapp(50, 200)\n",
    "for line in lines:\n",
    "    l = ''.join(line)\n",
    "    outfile.write(auth[index] + ': ' + l + '\\n')\n",
    "    print(auth_text[index] + ': ' + l)\n",
    "    if random.uniform(0, 1) > 0.7:\n",
    "        lj = \"\"\n",
    "        if random.uniform(0, 1) > 0.5:\n",
    "            lj = \" LEFT THE ROOM\"\n",
    "        else:\n",
    "            lj = \" JOINED THE ROOM\"\n",
    "        print(random.choice(auth_io)[0] + lj)\n",
    "        outfile.write(random.choice(auth_io)[0] + lj + '\\n')\n",
    "    index += 1\n",
    "\n",
    "outfile.close()\n",
    "# random in/outs for last 10\n",
    "# top_n 2000 / 2500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no no no\n",
      "annnnoooooooorrrrrraaaaaaaaaaaaa\n",
      "i know you knew, i just wanna know the interest of it, knowing that you know\n",
      "we have snow nordicgoddess but not enough to be snowed in .... yet lol\n",
      "no pwincess_aria no no no no\n",
      "pretends to know nothing then no one expects me to know anything\n",
      "no nononononono\n",
      "nnnnnoooo more poucing, hehe\n",
      "nnnnnooooooooooooooooooooooo\n",
      "yeah i know dj`annora not a good thing right now\n",
      "\u0002\u000307,01 limits: \u000300 no pee and scat, no permanent injuries, no animals nor kids\n",
      "lol no no no no\n",
      "nnnnnnnnnnnooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "# attempt to find similar sentences by matching vocabulary \n",
    "\n",
    "#li = ['her', 'her', 'her', 'he']\n",
    "li = ['are', 'fake']\n",
    "li = ['what', 'she', 'does']\n",
    "#li = ['what', 'what', 'what']\n",
    "li = ['is', 'is', 'is', 'is']\n",
    "li = ['being'] * 3\n",
    "\n",
    "#li = ['good', 'good', 'am']\n",
    "#li = ['here', 'here', 'here']\n",
    "#li = ['nice', 'nice', 'very']\n",
    "#li = ['well', 'well']\n",
    "#li = ['ll', 'll', 'll', 'll']\n",
    "li = ['no'] * 4\n",
    "#li = ['just', 'well', 'i']\n",
    "index = 0\n",
    "for post in all_posts:\n",
    "    p = post[1]\n",
    "    #print(p)\n",
    "    success = True\n",
    "    for l in li:\n",
    "        if l in p:\n",
    "            index = p.find(l)\n",
    "            length = len(l)\n",
    "            endindex = index + length\n",
    "            subs = p[index:endindex] \n",
    "            leftover = p[:index] + p[endindex:]\n",
    "            p = leftover\n",
    "        else:\n",
    "            success = False\n",
    "            #print(post[1] + ' failed')\n",
    "    if success and len(post[1]) < 80: \n",
    "        #print(len(post[1]))\n",
    "        print(post[1])\n",
    "    index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: 1406\n",
    "# insert sentence into parser\n",
    "# extract gramatical structure\n",
    "# try to match through corpus\n",
    "# use pre-processed text, with dashes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finito\n",
      "0.000000: \n",
      "0.000100: \n",
      "0.000200: \n",
      "0.000300: ------- --- ---- --- --\n",
      "0.000400: \n",
      "0.000500: \n",
      "0.000600: \n",
      "0.000700: hey -----\n",
      "0.000800: \n",
      "0.000900: \n",
      "####################################################################################\n",
      "0.100000: \n",
      "0.100100: \n",
      "0.100200: \n",
      "0.100300: \n",
      "0.100400: \n",
      "0.100500: \n",
      "0.100600: ill pass\n",
      "0.100700: \n",
      "0.100800: hi matt\n",
      "0.100900: \n",
      "####################################################################################\n",
      "0.200000: ---------\n",
      "0.200100: --- radio meltdown djwildkat is playing -- rm dj promo - djcaveman\n",
      "0.200200: you two are disgusting\n",
      "0.200300: hugs biffyyy\n",
      "0.200400: \n",
      "0.200500: \n",
      "0.200600: hi ------------\n",
      "0.200700: radiomeltdown radio meltdown djbooya is off --- coming up radio meltdown djmercury\n",
      "0.200800: --- radio meltdown djwof is playing -- randy travis - three wooden crosses\n",
      "0.200900: oh hell\n",
      "####################################################################################\n",
      "0.500000: \n",
      "0.500100: --- radio meltdown djmercury is playing -- david bowie - china girl\n",
      "0.500200: morning aall\n",
      "0.500300: anyone else into cumshot porn pm me\n",
      "0.500400: hello boys\n",
      "0.500500: \n",
      "0.500600: shhh im listening to --- uhm\n",
      "0.500700: --- radio meltdown djshoo is playing -- rm info promo - age disclaimer bside\n",
      "0.500800: a nice bath for me bubble bath feels nice\n",
      "0.500900: \n",
      "####################################################################################\n",
      "0.800000: hugggsss djannora\n",
      "0.800100: \n",
      "0.800200: hehe\n",
      "0.800300: heya ned\n",
      "0.800400: -----\n",
      "0.800500: --- radio meltdown djmercury is playing -- the who - the kids are alright\n",
      "0.800600: what did michael jackson know about coming together\n",
      "0.800700: \n",
      "0.800800: \n",
      "0.800900: \n"
     ]
    }
   ],
   "source": [
    "# CURVES generation\n",
    "\n",
    "random.seed(1)\n",
    "#render_plain('1306_removed_digits_entrances_01_ORIGINAL.txt', 500, 3860)\n",
    "#render_by_osc('osc_config_500_late_slowexp.csv', '1306_cleaner_group1&2&3_01.txt', 3860)\n",
    "print('finito')\n",
    "#generate_lines_linear(20)\n",
    "\n",
    "generate_lines_from(0.00, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.10, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.20, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.50, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.80, 0.0001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving word groups\n",
    "lists = []\n",
    "#lists.append((1, 5))\n",
    "#lists.append((6, 20))\n",
    "#lists.append((21, 50))\n",
    "#lists.append((51, 100))\n",
    "#lists.append((101, 500))\n",
    "lists.append((3000, 10000))\n",
    "\n",
    "outfile = codecs.open('3000_10000_groups.txt', 'w')\n",
    "\n",
    "for l in lists:\n",
    "    possible_words = []\n",
    "    #_config = config[index]\n",
    "    _min = l[0]\n",
    "    _max = l[1]\n",
    "    app_list = []\n",
    "    for i in range(_min, _max+1):\n",
    "        app_list.append(i)\n",
    "\n",
    "    # all allowed words\n",
    "    selected_appearances = get_by_appearance_list(app_list)\n",
    "    outfile.write(' '.join(selected_appearances))\n",
    "    #possible_words.append((selected_appearances, _config))\n",
    "    \n",
    "outfile.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess act1&2\n",
    "\n",
    "for sentence in all_posts_501:\n",
    "    print(sentence[0] + \" -> \" + remove_non_frequent(get_topn_percent(0.01), sentence[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
