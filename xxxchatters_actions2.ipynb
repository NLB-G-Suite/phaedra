{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:04:08] Random_Chaos wondering who the violator and who the violatee\n",
      "\n",
      "[23:04:41] FunkyBoogieKing pounce-whomples erotic_kitty\n",
      "\n",
      "[23:06:26] T-Rex rolls in and strikes a pose\n",
      "\n",
      "[23:17:58] BigDickBBL is now known as BigDick.\n",
      "\n",
      "[23:30:49] Handyman shudders\n",
      "\n",
      "[23:33:08] Sanger{ek} softly pets my lil kitten and kisses between her ears\n",
      "\n",
      "[23:33:45] Fenderman1964 waves and sits down\n",
      "\n",
      "[23:35:54] Jothom looks around\n",
      "\n",
      "[23:37:30] Fenderman1964 waves and gets out of the way\n",
      "\n",
      "[23:40:55] Fenderman1964 is too old for that\n",
      "\n",
      "[23:46:08] Fenderman1964 minds his own business\n",
      "\n",
      "[23:51:49] Jothom lounges\n",
      "\n",
      "[23:55:58] erotic_kitty yawns and curls up on Sanger\n",
      "\n",
      "[23:56:21] Sanger snuggles my kitten\n",
      "\n",
      "[23:56:40] erotic_kitty purrssssssssssssssss\n",
      "\n",
      "[23:56:40] Fenderman1964 waves and sits out of the way\n",
      "\n",
      "[23:57:13] Sanger swats your bottom\n",
      "\n",
      "[23:57:43] Fenderman1964 shakes his head and quits\n",
      "\n",
      "[23:58:26] Sanger tucks my pet in all warm and cozy with her Smokey stuffy and reads her a bed time story from Asimovs Sci Fi\n",
      "\n",
      "[00:00:16] Fenderman1964 doesnt play and thus doesnt worry\n",
      "\n",
      "[00:02:07] DJ`liltech is now known as liltech.\n",
      "\n",
      "479403\n",
      "158959\n",
      "29066\n",
      "40\n",
      "55\n",
      "304\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import codecs\n",
    "from utils import in_blocklist\n",
    "\n",
    "blocklist = []\n",
    "blocklist.append('AWAYLEN=307 MAXTARGETS=20 WALLCHOPS WATCH=128 WATCHOPTS=A SILENCE=15 MODES=12 CHANTYPES=# PREFIX')\n",
    "blocklist.append('- * -')\n",
    "blocklist.append('XxXChatters.Com')\n",
    "blocklist.append('XxXChatters')\n",
    "blocklist.append('This server was created')\n",
    "blocklist.append('operator(s) online')\n",
    "blocklist.append('is now your displayed host')\n",
    "blocklist.append('Caps set:')\n",
    "blocklist.append('MAXCHANNELS')\n",
    "blocklist.append('http')\n",
    "blocklist.append('type it in the main room or a pm to the')\n",
    "blocklist.append('The topic is')\n",
    "blocklist.append('Topic set by')\n",
    "\n",
    "all_separated = []\n",
    "all_posts = []\n",
    "all_posts_410 = []\n",
    "all_posts_501 = []\n",
    "authors = {}\n",
    "authors_410 = {}\n",
    "authors_501 = {}\n",
    "\n",
    "def parse_all(files, separate):\n",
    "    for file in glob.glob(files):\n",
    "        lines = codecs.open(file, mode='r', encoding='utf-8')\n",
    "\n",
    "        for line in lines:\n",
    "            if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "                if line[0] is '[' and line.find(\"<\") is not -1:\n",
    "                    s = line\n",
    "                    sentence = s[s.find(\">\")+2:]\n",
    "                    time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "                    author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                    \n",
    "                    p = (time+' ' + author, sentence.strip().lower())\n",
    "                    all_posts.append(p)\n",
    "                    if separate in file:\n",
    "                        all_separated.append(p)\n",
    "\n",
    "                    if author not in authors: # save things\n",
    "                        authors[author] = set() # init list for author\n",
    "                    authors[author].add(sentence.strip().lower()) \n",
    "                else:\n",
    "                    if line.find('*') is not -1 and \") has joined #\" not in line and \") Quit (\" not in line and \") has left #\" not in line and \"6A\u000314thena\u000314, \u000f\u000306y\u000314o\u000314u \u000f\u000306n\u000314eve\u000314r\" not in line and \" sets mode: +\" not in line and \" is now known as \" not in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        sentence = line[aa_end + 1:]\n",
    "                        \n",
    "                        p = (time+' ' + author, sentence.strip().lower())\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "\n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence.strip().lower()) \n",
    "                    if \") has joined #\" in line or \") Quit (\" in line or \") has left #\" in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        if \") has joined #\" in line:\n",
    "                            author += \" JOINED THE ROOM\"\n",
    "                        else:\n",
    "                            author += \" LEFT THE ROOM\"\n",
    "                        sentence = \"\"\n",
    "                        p = (time+' ' + author, sentence)\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "                            \n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence) \n",
    "\n",
    "    for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "        if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "            sentence = line[line.find(\":\"):]\n",
    "            all_posts.append((author, sentence.strip().lower()))\n",
    "            all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "            author = line[:line.find(\":\")]\n",
    "            if author not in authors: # save things\n",
    "                authors[author] = set() # init list for author\n",
    "            if author not in authors_410:\n",
    "                authors_410[author] = set()\n",
    "            authors[author].add(sentence.strip().lower())\n",
    "            authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "    for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "        if ':' in s and '<' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "            if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "                sentence = s[s.find(\">\")+2:]\n",
    "                author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                if len(author) < 2:\n",
    "                    continue\n",
    "                \n",
    "                all_posts.append((author, sentence.strip().lower()))\n",
    "                all_posts_501.append((author, sentence.strip().lower()))\n",
    "                \n",
    "                if author not in authors: # save things\n",
    "                    authors[author] = set() # init list for author\n",
    "                if author not in authors_501:\n",
    "                    authors_501[author] = set()\n",
    "                authors[author].add(sentence.strip().lower())\n",
    "                authors_501[author].add(sentence.strip().lower()) \n",
    "        if ':' in s and '<' not in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s and ' sets mode ' not in s:\n",
    "            print(s)\n",
    "            author_beginning = s.find('] ')\n",
    "            aa = s[author_beginning+2:]\n",
    "            aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "            author = s[author_beginning+2:aa_end]\n",
    "            sentence = s[aa_end + 1:]\n",
    "            #print(author)\n",
    "            #print(sentence)\n",
    "            all_posts.append((author, sentence.strip().lower()))\n",
    "            all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "            if author not in authors: # save things\n",
    "                authors[author] = set() # init list for author\n",
    "            if author not in authors_501:\n",
    "                authors_501[author] = set()\n",
    "            authors[author].add(sentence.strip().lower())\n",
    "            authors_501[author].add(sentence.strip().lower()) \n",
    "            #print(author)\n",
    "            #print(sentence)\n",
    "            \n",
    "            \n",
    "parse_all(\"xxxchatters_logs/*.log\", \"#chat\")\n",
    "#for file in glob.glob(\"xxxchatters_logs/*.log\"):\n",
    "print(len(all_posts))\n",
    "print(len(all_separated))\n",
    "print(len(authors))\n",
    "print(len(authors_410))\n",
    "print(len(authors_501))\n",
    "print(len(all_posts_410))\n",
    "print(len(all_posts_501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'onetoquiet': {'hello random_chaos sir', 'heya erotic_kitty'}, 'FunkyBoogieKing': {'see you guys tomorrow.', \"she's a-one spicy meat-ah-ball!!!\", 'pounce-whomples erotic_kitty', \"i didn't break you!!! \\xa0>>\", 'while the mischievous slave is away, the mischievous master will play.', \"alright, everyone, it's nighty-night winky time for funkyboogieking\"}, '@erotic_kitty': {'', 'hiyas jakkiline-cd winterwhisper and mr_magoo', 'well sanger might not like it if i m broken', 'hiyas maidtiffani and dirtyoldperv', 'hiyas subgirl4bbc', 'you whomped me lao', 'hiyas avgjoe40', 'wb keynotespkr', 'hiyas pantyhosehighheelsman', 'whats wrong handyman?', 'hiyas atticus48', 'hiyas iwhisper2clits', 'hiyas zelda', 'wb fenderman1964', '!djrocks', 'old?', 'hiyas ant35', 'hiyas cyrune', 'no worries fenderman1964', 'welcome back simian hugssssssssss', 'wb eedwardgrey', 'hiyas shynica', 'hiyas masterpiece', 'hiyas hentaibaka34m', 'hiyas sub_f', 'hiyas littledick-bigheart', 'that was his one of his first ring names', 'hiyas jj_black', 'wb stoneheart', 'hiyas chad26', 'hiyas curvybeauty', 'pendragon', 'wb viking', 'hiyas lori19 and james44', 'wb country_boy30', 'hiyas ruinme03', 'hiyas texguy', 'everytime i see your nick meanmark1 i wonder if you\\x92re an undertaker fan', 'hiyas schweetjohn', 'hiyas thegreatswitchbandit', 'mwah love', 'hiyas badguy', 'night all hugsssssssssssssssssss', 'hiyas swflguy', 'for what fenderman1964', 'hiyas sol', 'hiyas slutfucker', 'hiyas mike30', 'hiyas journey7', 'hiyas vixenvictoriax', 'hiyas chad masterdrew xdom', 'hiyas fenderman1964', 'hiyas hockeyguy', 'hiyas bigfeller', 'hiyas eedwardgrey', 'hiyas comeonman', 'hiyas s', 'hiyas blackstone', 'wb meanmark', 'hiyas maisy', 'hiyas jothom', 'hiyas treason', 'welcome violatef', 'wb olderjon and hiheeled_ticklishlady', 'hiyas sole_man'}, 'Violatef': {'oh random_chaos \\xa0why?', 'heya erotic_kitty \\xa0ty'}, 'jakkiline-cd': {'hi erotic_kitty'}, '+DJ`Mercury': {'radio meltdown: dj`liltech is playing \\xa0rm sponsor channel promo - #trivia_playpen', 'radio meltdown: dj`liltech is playing', 'radio meltdown: dj`liltech is playing \\xa0van halen - dance the night away', 'radio meltdown: dj`liltech is off --> coming up: radio meltdown: dj`mercury]', 'erotic_kitty thinks radio meltdown: dj`liltech rocks!!', 'radio meltdown: dj`liltech is playing \\xa0yellow claw - run away', 'sanger thinks radio meltdown: dj`liltech rocks!!', 'radio meltdown: dj`liltech is playing \\xa0zz top - la grange', 'radio meltdown: dj`liltech is playing \\xa0queen - a kind of magic', 'radio meltdown: dj`liltech is playing \\xa0syn city cowboys - run for my life', 'radio meltdown: dj`liltech is playing \\xa0men without hats - safety dance', 'radio meltdown: dj`liltech is playing \\xa0the police - every little thing she does is magic', 'radio meltdown: dj`mercury is playing \\xa0elton john - philadelphia freedom', 'radio meltdown: dj`liltech is playing \\xa0zz top - sharp dressed man', 'radio meltdown: dj`liltech is playing \\xa0electric light orchestra - strange magic', 'radio meltdown: dj`liltech is playing \\xa0iron maiden - run to the hills', 'radio meltdown: dj`liltech is playing \\xa0rm sponsor channel promo - #vamps`cafe', 'radio meltdown: dj`liltech is playing \\xa0walk the moon - shut up and dance', \"radio meltdown: dj`liltech is playing \\xa0zz top - i'm bad, i'm nationwide\", 'radio meltdown: dj`liltech is playing \\xa0dnce - cake by the ocean'}, 'Random_Chaos': {'wondering who the violator and who the violatee'}, 'WaywardGentleman': {'have a great night'}, '@erotic_kitty{S}': {'hiyas kinkyslavegirl', 'wb keynotespkr', 'geeze', 'hiyas t-rex', '!djrocks', 'my pleasure', 'wb winterwhisper', 'hiyas salsaguy', 'hiyas sweetliz', 'wb stolen tomato', 'heyyyyyyyyyyyyyyyyyyyyyyyy', 'hiyas nick3 hugsssssssssss', 'hiyas heavyballs', 'hiyas handyman', 'night lao', 'hiyas helen19 hiheeled_ticklishlady itjustslipped jayprez', 'hiyas alicia1forf', 'hiyas springrain', 'wb joycegg', 'hiyas oldhorn57', 'hiyas rockcockinpa', 'hiyas dante', 'hiyas lil-tigress hugssssssssssssssssssssssssssssssssssss', 'wb bd hugssssssssssss', 'hiyas jillvalentine', 'hiyas chaz', 'hiyas julia_for_f'}, 'JoyceGG': {'ty \\xa0kitty'}, 'T-Rex': {'hi hi erotic_kitty', '.ö/', 'rolls in and strikes a pose'}, 'Sanger': {'good night folks \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 play nice', 'awesome song', 'hey there lil tigress', 'snuggles my kitten', 'swats your bottom', 'no breaking my kitty', 'tucks my pet in all warm and cozy with her smokey stuffy and reads her a bed time story from asimovs sci fi', '!djrocks', 'say your goodnights', 'yw'}, '+shyNica': {'heya erotic_kitty'}, 'lil-tigress': {'heya erotic_kitty. huggggssssss.', 'heya sanger sir.'}, 'keynotespkr': {'hi erotic kitty', 'thnx e kitty'}, 'rst37': {'hi springrain', 'hello ladies', 'hi jillvalentine'}, 'Nick`3': {'huggggggs erotic_kitty'}, 'EEdwardGrey': {'hello erotic_kitty'}, '@DJ`liltech': {'good night erotic_kitty', 'ty sanger', 'later sanger', 'ty erotic_kitty'}, 'Hockeyguy': {'hi erotic_kitty'}, 'JayPrez': {'hello'}, 'BigDickBBL': {'slips back into the room', 'is now known as bigdick.'}, 'SpringRain': {'hi erotic_kitty'}, 'BigDick': {'hi springrain huggggggggssss', 'ty kitty hon hugggggggsssssss'}, 'Alicia1forF': {'hi erotic_kitty'}, 'Handyman': {'cool, as long as they don\\x92t respond...verbally....', \"pussy farts is one thing...talking clits...that's quite another....\", 'yikes', 'shudders', 'hi erotic_kitty'}, 'kinkyslavegirl': {'hi erotic_kitty'}, 'EEdwardGrey^': {'thank you erotic_kitty'}, 'iWhisper2Clits': {'erotic_kitty: \\xa0hello there'}, 'Littledick-bigheart': {'hiya erotic_kitty', 'hi goddessb'}, 'SwflGuy': {'hey there erotic_kitty'}, 'sweet_teresa': {'', 'are we going disneylandddddddddddddd', 'then yes', 'do they talk back', 'maybe he\\x92s dyslexic like me', 'i ask if they talk back', 'bad mike', 'clits', 'well he whispers to them', 'no slash'}, 'Pendragon': {'good night erotic_kitty', 'welcome recentlyrapedf', 'g night sanger'}, 'JJ_Black': {'hi erotic_kitty'}, 'Jothom': {'hi vixenvictoriax', 'lounges', 'hi sub_f', 'heya erotic_kitty', 'hello blonde86nc', 'hi curvybeauty', 'how are you doing vixenvictoriax?', 'looks around', 'hi everyone', 'hi goddessb'}, 'Sanger{ek}': {'softly pets my lil kitten and kisses between her ears'}, 'Fenderman1964': {'old wrestlers', 'waves and sits out of the way', 'doesn\\x92t play and thus doesn\\x92t worry', 'shakes his head and quits', 'minds his own business', 'don\\x92t mind me', 'is too old for that', 'waves and gets out of the way', 'waves and sits down'}, 'RuinMe03': {\"hey y'all\"}, 'FriendlyMonster': {'hiya blonde86nc', 'hiya curvybeauty', 'hi there feminist_teen'}, 'GoddessB': {'hello again all'}, 'Comeonman': {'heyas erotic_kitty'}, 'MeanMark1': {'ty kitty', '', 'gotcha', 'ahh no'}, 'BigFeller': {'hi erotic_kitty'}, 'vixenvictoriax': {'hi jothom', 'hi erotic_kitty'}, 'Country_Boy30': {'we just might', 'anyone up for fun'}, 'MikeinMiami': {'join /gangbang', '#join /gangbang'}, 'simian': {'typing one handed', 'switch the / and the #'}, 'dante': {'hi sweet_teresa'}, 'SchweetJohn': {'yo'}, 'Masterpiece': {'hello erotic_kitty'}, 'Curvybeauty': {'hi'}, 'erotic_kitty': {'yawns and curls up on sanger', 'purrssssssssssssssss'}, 'Soul-of-Darkness': {'subgirl4bbc'}, 'Pwner': {'wake up'}, 'DJ`liltech': {'is now known as liltech.'}}\n",
      "410: \n",
      "carrol: 53\n",
      "Louise: 28\n",
      "Valkyrie: 21\n",
      "Threeleggedcat: 20\n",
      "PlayfulBBC: 18\n",
      "@saffron{WH}: 17\n",
      "+DJ`Mercury: 16\n",
      "WhoGiveSaDamn: 15\n",
      "Silver_haired`Fox: 14\n",
      "Woman: 8\n",
      "Cruel`Intentions: 7\n",
      "guynextdoor: 7\n",
      "Anastatia: 7\n",
      "^fran: 7\n",
      "Jaems: 6\n",
      "+DJ`South: 6\n",
      "JFetish: 5\n",
      "gracie: 4\n",
      "Athena: 3\n",
      "saffron{WH}: 3\n",
      "TricksyM: 3\n",
      "jessica110: 2\n",
      "rst36: 2\n",
      "SensualDom: 2\n",
      "OldMaster: 2\n",
      "+WolvenHeart: 2\n",
      "Tyrant: 2\n",
      "[M]yEvilTwin: 1\n",
      "lonelyhousewife: 1\n",
      "collegegirlrp19: 1\n",
      "+ DJ`Mercury: 1\n",
      "+DJ1Mercury: 1\n",
      "Stacyshusband: 1\n",
      "MasterRenegade: 1\n",
      "+DJMercury`: 1\n",
      "dark-reign: 1\n",
      "maddogmoe: 1\n",
      "StacysHusband: 1\n",
      "Mutter: 1\n",
      "natron: 1\n",
      "\n",
      "501: \n",
      "@erotic_kitty: 65\n",
      "@erotic_kitty{S}: 27\n",
      "+DJ`Mercury: 20\n",
      "Sanger: 10\n",
      "sweet_teresa: 10\n",
      "Jothom: 10\n",
      "Fenderman1964: 9\n",
      "FunkyBoogieKing: 6\n",
      "Handyman: 5\n",
      "@DJ`liltech: 4\n",
      "MeanMark1: 4\n",
      "T-Rex: 3\n",
      "rst37: 3\n",
      "Pendragon: 3\n",
      "FriendlyMonster: 3\n",
      "onetoquiet: 2\n",
      "Violatef: 2\n",
      "lil-tigress: 2\n",
      "keynotespkr: 2\n",
      "BigDickBBL: 2\n",
      "BigDick: 2\n",
      "Littledick-bigheart: 2\n",
      "vixenvictoriax: 2\n",
      "Country_Boy30: 2\n",
      "MikeinMiami: 2\n",
      "simian: 2\n",
      "erotic_kitty: 2\n",
      "jakkiline-cd: 1\n",
      "Random_Chaos: 1\n",
      "WaywardGentleman: 1\n",
      "JoyceGG: 1\n",
      "+shyNica: 1\n",
      "Nick`3: 1\n",
      "EEdwardGrey: 1\n",
      "Hockeyguy: 1\n",
      "JayPrez: 1\n",
      "SpringRain: 1\n",
      "Alicia1forF: 1\n",
      "kinkyslavegirl: 1\n",
      "EEdwardGrey^: 1\n",
      "iWhisper2Clits: 1\n",
      "SwflGuy: 1\n",
      "JJ_Black: 1\n",
      "Sanger{ek}: 1\n",
      "RuinMe03: 1\n",
      "GoddessB: 1\n",
      "Comeonman: 1\n",
      "BigFeller: 1\n",
      "dante: 1\n",
      "SchweetJohn: 1\n",
      "Masterpiece: 1\n",
      "Curvybeauty: 1\n",
      "Soul-of-Darkness: 1\n",
      "Pwner: 1\n",
      "DJ`liltech: 1\n",
      "\n",
      "combined: \n",
      "@erotic_kitty: 65\n",
      "carrol: 53\n",
      "+DJ`Mercury: 36\n",
      "Louise: 28\n",
      "@erotic_kitty{S}: 27\n",
      "Valkyrie: 21\n",
      "Threeleggedcat: 20\n",
      "PlayfulBBC: 18\n",
      "@saffron{WH}: 17\n",
      "WhoGiveSaDamn: 15\n",
      "Silver_haired`Fox: 14\n",
      "Sanger: 10\n",
      "sweet_teresa: 10\n",
      "Jothom: 10\n",
      "Fenderman1964: 9\n",
      "Woman: 8\n",
      "Cruel`Intentions: 7\n",
      "guynextdoor: 7\n",
      "Anastatia: 7\n",
      "^fran: 7\n",
      "Jaems: 6\n",
      "+DJ`South: 6\n",
      "FunkyBoogieKing: 6\n",
      "JFetish: 5\n",
      "Handyman: 5\n",
      "gracie: 4\n",
      "@DJ`liltech: 4\n",
      "MeanMark1: 4\n",
      "Athena: 3\n",
      "saffron{WH}: 3\n",
      "TricksyM: 3\n",
      "T-Rex: 3\n",
      "rst37: 3\n",
      "Pendragon: 3\n",
      "FriendlyMonster: 3\n",
      "jessica110: 2\n",
      "rst36: 2\n",
      "SensualDom: 2\n",
      "OldMaster: 2\n",
      "+WolvenHeart: 2\n",
      "Tyrant: 2\n",
      "onetoquiet: 2\n",
      "Violatef: 2\n",
      "lil-tigress: 2\n",
      "keynotespkr: 2\n",
      "BigDickBBL: 2\n",
      "BigDick: 2\n",
      "Littledick-bigheart: 2\n",
      "vixenvictoriax: 2\n",
      "Country_Boy30: 2\n",
      "MikeinMiami: 2\n",
      "simian: 2\n",
      "erotic_kitty: 2\n",
      "[M]yEvilTwin: 1\n",
      "lonelyhousewife: 1\n",
      "collegegirlrp19: 1\n",
      "+ DJ`Mercury: 1\n",
      "+DJ1Mercury: 1\n",
      "Stacyshusband: 1\n",
      "MasterRenegade: 1\n",
      "+DJMercury`: 1\n",
      "dark-reign: 1\n",
      "maddogmoe: 1\n",
      "StacysHusband: 1\n",
      "Mutter: 1\n",
      "natron: 1\n",
      "jakkiline-cd: 1\n",
      "Random_Chaos: 1\n",
      "WaywardGentleman: 1\n",
      "JoyceGG: 1\n",
      "+shyNica: 1\n",
      "Nick`3: 1\n",
      "EEdwardGrey: 1\n",
      "Hockeyguy: 1\n",
      "JayPrez: 1\n",
      "SpringRain: 1\n",
      "Alicia1forF: 1\n",
      "kinkyslavegirl: 1\n",
      "EEdwardGrey^: 1\n",
      "iWhisper2Clits: 1\n",
      "SwflGuy: 1\n",
      "JJ_Black: 1\n",
      "Sanger{ek}: 1\n",
      "RuinMe03: 1\n",
      "GoddessB: 1\n",
      "Comeonman: 1\n",
      "BigFeller: 1\n",
      "dante: 1\n",
      "SchweetJohn: 1\n",
      "Masterpiece: 1\n",
      "Curvybeauty: 1\n",
      "Soul-of-Darkness: 1\n",
      "Pwner: 1\n",
      "DJ`liltech: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "authors_410_second = []\n",
    "for a in authors_410:\n",
    "    authors_410_second.append((a, len(authors_410[a])))\n",
    "    \n",
    "authors_501_second = []\n",
    "for a in authors_501:\n",
    "    authors_501_second.append((a, len(authors_501[a])))\n",
    "    \n",
    "authors_410_second_sorted = sorted(authors_410_second, key=operator.itemgetter(1), reverse=True) \n",
    "authors_501_second_sorted = sorted(authors_501_second, key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print(authors_501)\n",
    "\n",
    "authors_merged = {}\n",
    "for a in authors_410:\n",
    "    authors_merged[a] = len(authors_410[a])\n",
    "    \n",
    "#print(authors_merged)\n",
    "for a in authors_501:\n",
    "    if a in authors_merged:\n",
    "        authors_merged[a] += len(authors_501[a])\n",
    "    if a not in authors_merged:\n",
    "        authors_merged[a] = len(authors_501[a])\n",
    "        \n",
    "#print(authors_merged)\n",
    "\n",
    "authors_merged_second = []\n",
    "for a in authors_merged:\n",
    "    authors_merged_second.append((a, authors_merged[a]))\n",
    "    \n",
    "authors_merged_second_sorted = sorted(authors_merged_second, key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print('410: ')\n",
    "for ps in authors_410_second_sorted:\n",
    "    print(ps[0] + ': ' + str(ps[1]))\n",
    "    \n",
    "print()\n",
    "print('501: ')\n",
    "for ps in authors_501_second_sorted:\n",
    "    print(ps[0] + ': ' + str(ps[1]))\n",
    "    \n",
    "\n",
    "print()\n",
    "print('combined: ')\n",
    "for ps in authors_merged_second_sorted:\n",
    "    print(ps[0] + ': ' + str(ps[1]))\n",
    "#authors_410.sort(key=takeSecond)\n",
    "#print(authors_410_sorted)\n",
    "#for a in authors_410:\n",
    "#    print(a + ': ' + str(len(authors_410[a])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in authors['carrol']:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49231\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import string\n",
    "word_counts = dict()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess_word(word):\n",
    "    word = word.replace('\\x02', '')\n",
    "    word = word.replace('\\x03', '')\n",
    "    word = word.replace('.', '')\n",
    "    word = ''.join([i for i in word if not i.isdigit()])\n",
    "\n",
    "    #word = word.translate(translator)\n",
    "    word = ''.join(e for e in word if e.isalnum())\n",
    "    return word\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    word_list = post[1].split()\n",
    "    for word in word_list:\n",
    "        word = preprocess_word(word)\n",
    "        #word = word.replace('\\x03', '')\n",
    "        #word = word.replace('.', '')\n",
    "\n",
    "        #word = word.translate(translator)\n",
    "        #word = ''.join(e for e in word if e.isalnum())\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "def get_sorted_word_counts():\n",
    "    return sorted_word_counts\n",
    "\n",
    "v = get_sorted_word_counts()\n",
    "print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extra', 'arm', 'large', 'past', 'quickly', 'irc', 'human', 'rl', 'size', 'wine', 'leaves', 'firm', 'able', 'summer', 'dylan', 'devil', 'naderaax', 'kinky', 'jag']\n"
     ]
    }
   ],
   "source": [
    "def get_topn_percent(perc):\n",
    "    counts = get_sorted_word_counts()\n",
    "    l = len(counts)\n",
    "    max_index = int(perc*l)\n",
    "    top_n = []\n",
    "    #print(counts[:max_index])\n",
    "    for t in counts[:max_index]:\n",
    "        top_n.append(t[0])\n",
    "    return top_n\n",
    "\n",
    "def get_all_by_max_appearance(app):\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] >= app:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "\n",
    "def get_by_appearance_list(appearance):\n",
    "    # appearance = [2, 3, 4] # number indicating times of appearance\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] in appearance:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "    \n",
    "#topn = get_topn_percent(0.001)\n",
    "\n",
    "selected_appearances = get_by_appearance_list([110, 112])\n",
    "print(selected_appearances)\n",
    "\n",
    "#maxapp = get_all_by_max_appearance(2)\n",
    "#file = open(\"1106/chat_room_word_appearance.txt\", 'w')\n",
    "#for e in maxapp:\n",
    "#    try:\n",
    "#        file.write(e[0] + \", \" + str(e[1]) + '\\n')\n",
    "#    except:\n",
    "#        pass\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- to keep being evil ---- athena on the -------- with a new nerf bat\n",
      "jus to keep being -------- ---- athena on the backside with a new nerf bat\n"
     ]
    }
   ],
   "source": [
    "def remove_non_frequent(allowed_words, post, replace=True):\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        pword = preprocess_word(word)\n",
    "        if pword in allowed_words:\n",
    "            good_words.append(pword)\n",
    "        else:\n",
    "            if replace:\n",
    "                replacement = \"\"\n",
    "                for char in word:\n",
    "                    replacement += \"-\"\n",
    "                good_words.append(replacement)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "\n",
    "def remove_by_probability(allowed_words, always_allowed, post, chance, replace=True):\n",
    "    import random\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        #pword = preprocess_word(word)\n",
    "        if word in always_allowed:\n",
    "            good_words.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in allowed_words:\n",
    "            good_words.append(word)\n",
    "        else:\n",
    "            if replace:\n",
    "                rv = random.uniform(0.0, 1.0)\n",
    "                if rv < chance:\n",
    "                    replacement = \"\"\n",
    "                    for char in word:\n",
    "                        replacement += \"-\"\n",
    "                    good_words.append(replacement)\n",
    "                else:\n",
    "                    good_words.append(word)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "sentence_to_test = \"Jus to keep being evil<<<< wops Athena on the backside with a new nerf bat\"\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n",
    "\n",
    "print(remove_by_probability(get_topn_percent(0.1), get_topn_percent(0.01), sentence_to_test, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ------- on the -------- with one of my new ---- ----\n",
      "bops saffron on the -------- with one of my new nerf bats\n"
     ]
    }
   ],
   "source": [
    "sentence_to_test = \"bops saffron on the backside with one of my new nerf bats\"\n",
    "print(remove_non_frequent(get_topn_percent(0.01), sentence_to_test))\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@erotic_kitty', 65), ('carrol', 53), ('+DJ`Mercury', 36), ('Louise', 28), ('@erotic_kitty{S}', 27), ('Valkyrie', 21), ('Threeleggedcat', 20), ('PlayfulBBC', 18), ('@saffron{WH}', 17), ('WhoGiveSaDamn', 15), ('Silver_haired`Fox', 14), ('Sanger', 10), ('sweet_teresa', 10), ('Jothom', 10), ('Fenderman1964', 9), ('Woman', 8), ('Cruel`Intentions', 7), ('guynextdoor', 7), ('Anastatia', 7), ('^fran', 7), ('Jaems', 6), ('+DJ`South', 6), ('FunkyBoogieKing', 6), ('JFetish', 5), ('Handyman', 5), ('gracie', 4), ('@DJ`liltech', 4), ('MeanMark1', 4), ('Athena', 3), ('saffron{WH}', 3)]\n",
      "['@erotic_kitty' 'carrol' '@erotic_kitty' '@erotic_kitty' '@erotic_kitty'\n",
      " 'carrol' '@erotic_kitty' '@erotic_kitty' 'carrol' 'carrol' 'carrol'\n",
      " 'carrol' '@erotic_kitty' '@erotic_kitty' 'carrol' '@erotic_kitty'\n",
      " '@erotic_kitty' 'carrol' '@erotic_kitty' '@erotic_kitty']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_by_probability(authors_list, count, topn = 30):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for authors in authors_list:\n",
    "        for key, value in authors.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "                \n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #print(author_probability_counts[:topn])\n",
    "\n",
    "    sum_sentences = 0\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        sum_sentences += v[1]\n",
    "\n",
    "    author_probability_final = {}\n",
    "    author_list = []\n",
    "    prob_list = []\n",
    "\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        author_probability_final[v[0]] = v[1]/float(sum_sentences)\n",
    "        author_list.append(v[0])\n",
    "        prob_list.append(v[1]/float(sum_sentences))\n",
    "\n",
    "    final_autor_list = np.random.choice(\n",
    "      author_list, \n",
    "      count,\n",
    "      p=prob_list\n",
    "    )\n",
    "\n",
    "    return final_autor_list\n",
    "\n",
    "def get_topn_authors(author_lists, n):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for author_list in author_lists:\n",
    "        for key, value in author_list.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "\n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return author_probability_counts[:n]\n",
    "\n",
    "#print(get_by_probability(authors, 20))\n",
    "#print(get_by_probability(authors_410, 20))\n",
    "#print(get_by_probability(authors_501, 20))\n",
    "\n",
    "print(get_topn_authors([authors_410,authors_501], 30))\n",
    "print(get_by_probability([authors_410,authors_501], 20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load osc config and parse for later use\n",
    "import csv\n",
    "\n",
    "def load_osc_config(file_name):\n",
    "    config = {}\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter='\\t', quotechar='|')\n",
    "        index = 0\n",
    "        for row in reader:\n",
    "            if index > 0:\n",
    "                #print(row)\n",
    "                for i in range(len(row)):\n",
    "                    config[i].append(row[i])\n",
    "            else:\n",
    "                config[0] = []\n",
    "                config[1] = []\n",
    "                config[2] = []\n",
    "                config[3] = []\n",
    "                config[4] = []\n",
    "            index += 1\n",
    "    \n",
    "    return config\n",
    "            \n",
    "osc_config = load_osc_config('osc_config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-47a6c1561ee5>:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "def generate_lines_linear(count):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        perc = len(return_lines)/float(count)\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(perc)\n",
    "        #print(len(topn))\n",
    "        #print(sentence)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if len(stripped_post) > 20 and stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "        \n",
    "def generate_lines_from(fromp, step, count):\n",
    "    for i in range(count):\n",
    "        sentence = random.choice(all_posts)\n",
    "        perc = fromp + i * step\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(len(topn))\n",
    "        stripped_post = remove_non_frequent(topn, sentence[1].strip().lower())\n",
    "        print(\"{:.6f}\".format(perc) + \": \" + stripped_post)\n",
    "        \n",
    "def generate_by_osc(config, lists, offset=90):\n",
    "    assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n",
    "\n",
    "    index = 0\n",
    "    possible_words = []\n",
    "    # length of generated sentences is determined by osc/curve data\n",
    "    length = len(config[0])\n",
    "    for l in lists:\n",
    "        _config = config[index]\n",
    "        _min = l[0]\n",
    "        _max = l[1]\n",
    "        app_list = []\n",
    "        for i in range(_min, _max+1):\n",
    "            app_list.append(i)\n",
    "        \n",
    "        # all allowed words\n",
    "        selected_appearances = get_by_appearance_list(app_list)\n",
    "        possible_words.append((selected_appearances, _config))\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    # all words with min appearance of 500 are always allowed\n",
    "    always_allowed = get_all_by_max_appearance(500)\n",
    "        \n",
    "    finished_sentences = []\n",
    "    for i in range(length):\n",
    "        #s = random.choice(all_posts)[1]\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        # don't process djs\n",
    "        if 'DJ`Mercury' in a or 'DJ`Protea' in a:\n",
    "            finished_sentences.append((a, s))\n",
    "            continue\n",
    "            \n",
    "        for p in possible_words:\n",
    "            _words = p[0]\n",
    "            probability = float(p[1][i])\n",
    "            s = remove_by_probability(_words, always_allowed, s, probability)\n",
    "        \n",
    "        finished_sentences.append((a, s))\n",
    "    \n",
    "    return finished_sentences\n",
    "\n",
    "def render_plain(out_filename, line_count, offset):\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for i in range(line_count):\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        outfile.write(a + ': ' + s + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "def render_by_osc(osc_config, out_filename, offset):\n",
    "    lists = []\n",
    "    lists.append((1, 5))\n",
    "    lists.append((6, 20))\n",
    "    lists.append((21, 50))\n",
    "    #lists.append((51, 100))\n",
    "    #lists.append((101, 500))\n",
    "    osc_config = load_osc_config(osc_config)\n",
    "    generated = generate_by_osc(osc_config, lists, offset)\n",
    "\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for line in generated:\n",
    "        outfile.write(line[0] + ': ' + line[1] + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@erotic_kitty' '@erotic_kitty' '@DJ`liltech']\n"
     ]
    }
   ],
   "source": [
    "auth = get_by_probability([authors_410,authors_501], 300)\n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% (126709 of 130000) |############## | Elapsed Time: 20:20:13 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# samples random lines from corpus, add a author from act1&2 by distribution and outout after a certain length of the line\n",
    "# randomly add entrences and exits from the last 10 of the top active 30 authors of act1&2\n",
    "# entire corpus processed and saved in one 7.5mb text file\n",
    "\n",
    "#random.seed(1)\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "max_count = 170000\n",
    "#auth = get_by_probability([authors_410,authors_501], max_count)\n",
    "\n",
    "def generate_lines_maxapp(count, maxapp):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        #perc = len(return_lines)/float(count)\n",
    "        #perc = 0.5\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        #topn = get_all_by_max_appearance(maxapp)\n",
    "        topn = get_topn_percent(maxapp)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        #if len(stripped_post) > 20 and len(stripped_post) < 50 and stripped_post not in return_lines:\n",
    "        if stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "def process_all(maxapp):\n",
    "    return_lines = []\n",
    "    for p in all_posts:\n",
    "        sentence = p[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        topn = get_topn_percent(maxapp)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "outfile = codecs.open('1606_only_top0.025_all_authordistfix.txt', mode='w')\n",
    "\n",
    "# 20 top authors from 401 & 510\n",
    "auth_text = get_by_probability([authors_410,authors_501], max_count, 20)\n",
    "auth_io = get_topn_authors([authors_410,authors_501], 30)[-10:] # get_by_probability([authors_410,authors_501], 50, 30)[-10:]\n",
    "\n",
    "index = 0\n",
    "#lines = generate_lines_maxapp(max_count, 0.025)\n",
    "lines = process_all(0.025)\n",
    "print(len(lines))\n",
    "for line in lines:\n",
    "    l = ''.join(line)\n",
    "    outfile.write(auth_text[index] + ': ' + l + '\\n')\n",
    "    pbar.update(index)\n",
    "    if random.uniform(0, 1) > 0.7:\n",
    "        lj = \"\"\n",
    "        if random.uniform(0, 1) > 0.5:\n",
    "            lj = \" LEFT THE ROOM\"\n",
    "        else:\n",
    "            lj = \" JOINED THE ROOM\"\n",
    "        outfile.write(random.choice(auth_io)[0] + lj + '\\n')\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "#pbar.update(max_count)\n",
    "outfile.close()\n",
    "print('finished')\n",
    "# random in/outs for last 10\n",
    "# top_n 2000 / 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no no no\n",
      "annnnoooooooorrrrrraaaaaaaaaaaaa\n",
      "i know you knew, i just wanna know the interest of it, knowing that you know\n",
      "we have snow nordicgoddess but not enough to be snowed in .... yet lol\n",
      "no pwincess_aria no no no no\n",
      "pretends to know nothing then no one expects me to know anything\n",
      "no nononononono\n",
      "nnnnnoooo more poucing, hehe\n",
      "nnnnnooooooooooooooooooooooo\n",
      "yeah i know dj`annora not a good thing right now\n",
      "\u0002\u000307,01 limits: \u000300 no pee and scat, no permanent injuries, no animals nor kids\n",
      "lol no no no no\n",
      "nnnnnnnnnnnooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "# attempt to find similar sentences by matching vocabulary\n",
    "# FAILED NOT USED\n",
    "\n",
    "#li = ['her', 'her', 'her', 'he']\n",
    "li = ['are', 'fake']\n",
    "li = ['what', 'she', 'does']\n",
    "#li = ['what', 'what', 'what']\n",
    "li = ['is', 'is', 'is', 'is']\n",
    "li = ['being'] * 3\n",
    "\n",
    "#li = ['good', 'good', 'am']\n",
    "#li = ['here', 'here', 'here']\n",
    "#li = ['nice', 'nice', 'very']\n",
    "#li = ['well', 'well']\n",
    "#li = ['ll', 'll', 'll', 'll']\n",
    "li = ['no'] * 4\n",
    "#li = ['just', 'well', 'i']\n",
    "index = 0\n",
    "for post in all_posts:\n",
    "    p = post[1]\n",
    "    #print(p)\n",
    "    success = True\n",
    "    for l in li:\n",
    "        if l in p:\n",
    "            index = p.find(l)\n",
    "            length = len(l)\n",
    "            endindex = index + length\n",
    "            subs = p[index:endindex] \n",
    "            leftover = p[:index] + p[endindex:]\n",
    "            p = leftover\n",
    "        else:\n",
    "            success = False\n",
    "    if success and len(post[1]) < 80: \n",
    "        print(post[1])\n",
    "    index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: to be ---- to see\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1281 of 164986) |                  | Elapsed Time: 0:00:15 ETA:   0:27:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Athena: -------- i enjoy the open air ---------- such as ------ or ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (3173 of 164986) |                  | Elapsed Time: 0:00:34 ETA:   0:26:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 1: Jothom: --- why im ------------ tired and body -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (16781 of 164986) |#                | Elapsed Time: 0:02:55 ETA:   0:25:18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 0: +DJ`Mercury: so by the ----- of ---- --- and she was --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% (45377 of 164986) |####             | Elapsed Time: 0:07:50 ETA:   0:21:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Silver_haired`Fox: damn i thought i was ---- thin at --- and ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (164985 of 164986) |############### | Elapsed Time: 0:29:25 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Athena: -------- i enjoy the open air ---------- such as ------ or ---------\n",
      "matched pattern 1: Jothom: --- why im ------------ tired and body -----\n",
      "matched pattern 0: +DJ`Mercury: so by the ----- of ---- --- and she was --\n",
      "matched pattern 2: Silver_haired`Fox: damn i thought i was ---- thin at --- and ---\n"
     ]
    }
   ],
   "source": [
    "# todo: 1406\n",
    "# insert sentence into parser\n",
    "# extract gramatical structure\n",
    "# try to match through corpus\n",
    "# use pre-processed text, with dashes \n",
    "# FAILED NOT USED\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "# punctation is NFP or PUNCT\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_tag_sequence(sen, detailed=False):\n",
    "    doc = nlp(sen)\n",
    "    sentence_pos_list = []\n",
    "    sentence_tag_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        sentence_pos_list.append(token.pos_)\n",
    "        sentence_tag_list.append(token.tag_)\n",
    "    \n",
    "    if detailed:\n",
    "        return sentence_tag_list\n",
    "    \n",
    "    return sentence_pos_list\n",
    "\n",
    "def match_sequence(seq_to_match, seq_of_target):\n",
    "    to_match = ''.join(seq_to_match)\n",
    "    of_target = ''.join(seq_of_target)\n",
    "    if to_match in of_target:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_sequences(sequences, seq_of_target):\n",
    "    mmm = []\n",
    "    for s in sequences:\n",
    "        mmm.append(match_sequence(s, seq_of_target))\n",
    "    return mmm\n",
    "\n",
    "    \n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "\n",
    "detailed = True    \n",
    "\n",
    "ssss = []\n",
    "ssss.append('---- and i was ----')\n",
    "ssss.append('---- green and red ----')\n",
    "ssss.append('---- such as ---- or ----')\n",
    "ssss.append('---- hi ---- and ----')\n",
    "ssss.append('---- of the wall ---- of the bank')\n",
    "taglist = []\n",
    "for s in ssss:\n",
    "    taglist.append(get_tag_sequence(s, detailed))\n",
    "    \n",
    "print('input: ' + sentence)\n",
    "matched_sentences = []\n",
    "index = 0\n",
    "bar = ProgressBar(widget=[Percentage(), Bar()], maxval=len(input_file)).start()\n",
    "for line in input_file:\n",
    "    if len(line) > 2:\n",
    "        tseq = get_tag_sequence(line.rstrip(), detailed)\n",
    "        matched = match_sequences(taglist, tseq)\n",
    "        ii = 0\n",
    "        for m in matched:\n",
    "            if m:\n",
    "                matched_sentences.append('matched pattern ' + str(ii) + ': ' + line.rstrip())\n",
    "                print('matched pattern ' + str(ii) + ': ' + line.rstrip())\n",
    "            else:\n",
    "                pass\n",
    "            ii += 1\n",
    "        index += 1\n",
    "        #if index > 2000:\n",
    "        #    break\n",
    "        bar.update(index)\n",
    "for m in matched_sentences:\n",
    "    print(m)\n",
    "if len(matched_sentences) == 0:\n",
    "    print('no results :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses big corpus of dashed out text and matches by percent the occence of a special list of very common words\n",
    "# kinda FAILED not used\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "max_line_counter = 0\n",
    "out_file = open('1406_min5_words_relative0.45.txt', 'w')\n",
    "perc = 0.45\n",
    "\n",
    "\n",
    "for line in input_file:\n",
    "    counter = 0\n",
    "    wordcount = len(line.split())\n",
    "    \n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "    if counter/float(wordcount) > perc and wordcount > 10:\n",
    "        out_file.write(line)\n",
    "        print(line)\n",
    "    max_line_counter += 1\n",
    "    #if max_line_counter > 90000:\n",
    "    #    break\n",
    "    \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure ONE\n",
    "\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "\n",
    "#input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "input_file = open('1606_only_top0.025_all_authordistfix.txt', mode='r', encoding='utf8').readlines()\n",
    "max_line_counter = 0\n",
    "out_file = open('1506_structure_01.txt', 'w')\n",
    "outfile.write('ACT 3 - STRUCTURE 1\\n')\n",
    "\n",
    "perc = 0.3\n",
    "chosen = []\n",
    "wordcount_min = 10\n",
    "\n",
    "for line in input_file:\n",
    "    #perc += 0.002\n",
    "    counter = 0\n",
    "    #print(line)\n",
    "    line = line.split(':')\n",
    "    \n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    auth = line[0]\n",
    "    line = line[1]\n",
    "    if len(line) < 3:\n",
    "        continue\n",
    "    wordcount = len(line.split())\n",
    "\n",
    "\n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "\n",
    "    if counter / float(wordcount) > perc and wordcount > wordcount_min:\n",
    "        perc += 0.002\n",
    "        chosen.append(line)\n",
    "        out_file.write(auth + ': ' + line)\n",
    "        print(str(perc) + ': ' + line.rstrip())\n",
    "        \n",
    "    if len(chosen) > 150:\n",
    "        break\n",
    "\n",
    "print('###########################################################################################################################')\n",
    "print('second part')\n",
    "print('###########################################################################################################################')\n",
    "\n",
    "short_chosen = []\n",
    "min_len = 50\n",
    "max_len = 140\n",
    "for line in input_file:\n",
    "    line = line.split(':')\n",
    "    if len(line) < 2:\n",
    "        continue \n",
    "    auth = line[0]\n",
    "    line = line[1]\n",
    "    if len(line) < 3:\n",
    "        continue\n",
    "    counter = 0\n",
    "    wordcount = len(line.split())\n",
    "    linelen = len(line)\n",
    "    \n",
    "    if wordcount == 0:\n",
    "        continue\n",
    "    \n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "                \n",
    "    \n",
    "    if counter / float(wordcount) > perc and linelen < max_len and linelen > min_len:\n",
    "        min_len -= 1\n",
    "        max_len -= 2\n",
    "        perc += 0.002\n",
    "        short_chosen.append(line)\n",
    "        out_file.write(auth + ': ' + line)\n",
    "        print(str(perc) + ': ' + line.rstrip())\n",
    "        \n",
    "    if len(short_chosen) > 70:\n",
    "        break\n",
    "    #max_line_counter += 1\n",
    "\n",
    "print(len(short_chosen))\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its okay if she needs to -------- the words i ----------- but ----- wont be as --------- in his ------ but --- all the ---------- my --------\n",
      " ------- ------ ass with the dildo as i orgasm all over her face moaning into her clit my fingers --------- in n out of her ---- as i -------\n",
      " nods again -------- on his -------- rolling on to his -------- i see\n",
      " well i have a dress just let me know when i should be there for the -------\n",
      " ------- limits --- no men i am for women only let me help you ------- a ----- of your ----------- ------ no ----- no ------------ no --------- ------ no ----------- or ------------ not much into pain but can give if its ------- or i get ---------- no ----- nothing in my bottom we can ------- ------ that dont bid with the ------ of making me ------\n",
      " ------------- please do not ask for or joke --------- ----- this is a no pm channel\n",
      " battlecat im not ------ a -------- with ----- on the ------\n",
      " truth what is the one thing ------ sex that you really are not ----------- with\n",
      " kisses mrtlwolf --------------- her tongue into his mouth -------- with his\n",
      " as long as you dont start -------- ------- ------ or at least\n",
      " if you are super ------- and cant do that then go back to enjoy it\n",
      " holds up both hands and makes ------- with his fingers towards ------- dont make me\n",
      " oh get up pwincessaria come on no ------ your a ------ we are ------\n",
      " are you trying to ------- me in hope or just ------- me up to let my ----- down\n",
      " never --- into that ---------- which might be why its --------\n",
      " ----------------------- hey hey watch out for my feet please\n",
      " yeah sorry no ------- but other ----------- just dont do it for me\n",
      " lets out a soft ---- as he feels her through the thin --------- -------- slowly up at her -------- his fingers against her back slowly i think we might have some fun when ---- is up\n",
      " ----------------------- dragonfly kisses her cheek and smiles at her\n",
      " slowly ---- her head up and down feeling him ---- as she pushes her lips ------- up and down his cock\n",
      " hehe she is as much as id like to warm her on the inside gotta ------- -----\n",
      " yea its late im not usually hear this late but im on leave this week\n",
      " ------ at his cock ---------- by its -------- and -------- for her\n",
      " ------- his cock with her face again feeling it ------- her\n",
      " takes his cock -------- into her mouth taking his tip into her throat and holding herself down on him\n",
      " - i have no one to send --------------------- or ---------------- to\n",
      " valkyrie not at all i ------ myself and ------- trouble always finds me me\n",
      " im going to leave this channel as long as carla is here i dont feel ----------- here\n",
      " more like that one would be ----- - -- is good but --------- and -------- ---- like that are just so ------ hot\n",
      " i ---- they -------- you around so you dont feel your on hold for so long\n",
      " smiles at ----- as she bites her ----- lip as she ----- from her ------ ------\n",
      " though you might have been here when i got in chuckles i dont have ---------- ------- in channel so no ----- but you said hello so hello smiles\n",
      " nothing someone said it was --- until ---------- but thats not what it means for me\n",
      " kisses mrtlwolf --------------- her tongue into his mouth -------- with his\n",
      " shes been nothing but nice to me but well keep that our dirty ------\n",
      " is she doing something we should all be watching ----------\n",
      " carrol if you dont have ------ what makes you so horny p ----\n",
      " well it ----- if you dont just ---- at our tits but actually look at our -----\n",
      " no one is ------- you ------- ---- we really just dont care either way\n",
      " is she doing something we should all be watching ----------\n",
      " its always at your ------- its not gonna be at mine whats the point of that invictus lol\n",
      " pretty well ---------- we were just talking about ------ want one\n",
      " --------- she should probably have ------- her ------------\n",
      " also good here thanks - hope its something good nudisthusband\n",
      " im sure she is a ------ but im good you two ----- on with your ------ ------\n",
      " well i have a -------- for a reason if he was interested in that he would have ----- me but hey\n",
      " slides his tongue against her open ----- ------- her as she pulls him into her -------- here ------- -----\n",
      " its too late for ---------- for me haha --------- but --------- ------\n",
      " leans her back into his touch ------- him ---- her dress off\n",
      " - i didnt say i didnt have anyone on ------------- just no one in here\n",
      " but indeed not all dominant women like the ---- -------- for ------- --------\n",
      " ------- his cock -------- with her face as she ---- at his ----- looking up at him\n",
      " mm i dunno if youre ----- all that good if youre ------- cock\n",
      " more like that one would be ----- - -- is good but --------- and -------- ---- like that are just so ------ hot\n",
      " licks her lips as she looks up at ----- cock now in front of her\n",
      " says ------- ---- her with his drink -------- into his -----\n",
      " truth name one of your bad --------- and one of your good ------------\n",
      " what it was all nice things i was saying about you -------------\n",
      " kisses --------- as she -------- you look so hot as she back away ------ down on her ----- ------ ------\n",
      " im sure she is a ------ but im good you two ----- on with your ------ ------\n",
      " yawns and drops down to her usual couch --------- out as she --------- what to do with her day\n",
      " - but i have come to the -------------------- that im just not pretty\n",
      " - first of all ------------- ---- that was not -------------- at you\n",
      " ----- ------------- no sit at the bar my -------- are back here\n",
      " no it was not it was just a question what you wanna to do smiles\n",
      " hold on guys ------------ might not be your ---- but -- ------- that its quite the show\n",
      " still ------ up but i have other ------ im ------- with now that are more ---------\n",
      " ------- her into his leg and keeps her ------- before ------- her up and ------- her on his lap letting her ---- her face and ------- her ----------\n",
      " there was only one man that ------ this world that was perfect\n",
      " well i have a -------- for a reason if he was interested in that he would have ----- me but hey\n",
      " no hi no how are you - no dinner or ------ just straight to it\n",
      " how do i have any ---- as to ------- or not that one person out of the -------- on here is someone i should ----- to\n",
      " see you all are nice ------ no wonder they dont let me be in -----------\n",
      " what it was all nice things i was saying about you -------------\n",
      " -------- around with her ------- on his cock ------- her off\n",
      " what are you ------------- seyyal and ------ for some not so fine\n",
      " now we need to be ----------- ------- its not ---- its ---------\n",
      " truth if there was no such thing as money what would you do with your life\n",
      " is that on block or on the block ready for off with his head\n",
      " kisses mrtlwolf --------------- her tongue into his mouth -------- with his\n",
      " ---- -------- on his cock --------- herself as he ---- her throat so well\n",
      " looking up with ----- eyes as he ------ her face with his ------ -------\n",
      " there was a --- in ------- for a time that ------ that for me\n",
      " no one in here ----- with me because i ------- ------- at the -----\n",
      " not much going to get ---------- ass up so we can get out and do something its nice out today\n",
      " falls into her arms -------- seyyal back ------- up against her\n",
      " all the ------ ----- are here is that what youre ------- me\n",
      " it hurt for like two ------- and im still not back at -----\n",
      " - im not that ---------------------- ----------------------\n",
      " well one that was easy or --------- but if you dont have one its cool\n",
      " -------------------- i dont have that on my -----------------\n",
      " - but i have come to the -------------------- that im just not pretty\n",
      " if any ladies are on im ready for just about any roleplay idea dont be ------ to hit me up\n",
      " her back --------- ----- a tight ---- as her head falls back with ------ pulling back ------- his cock with her ass\n",
      " its always at your ------- its not gonna be at mine whats the point of that invictus lol\n",
      " ty ---------------------- - i like yours as well how are you\n",
      " we dont have face ------- ------- that will suck out your -----\n",
      " is that on block or on the block ready for off with his head\n",
      " -------- around with her ------- on his cock ------- her off\n",
      " ----- as the -------- makes her ---- her back and ------ her ---- out as she ------ up into his ------ eyes she licks her lips again\n",
      " last year there --------------- was not as good as it use to be\n",
      " what is your name what is your ------ what is your -------- -----\n",
      " curls up with her master his arms ------- around her as she ------ her eyes\n",
      " words are as fun as they are ------- or some ------ shit like that\n",
      " i dont have a ----- if i had one its not me thats gonna be in there\n",
      " these guys have a good ----- but its just not that guys -----\n",
      " words are as fun as they are ------- or some ------ shit like that\n",
      " description for annora im not interested its not you its me im an ----- bitch\n",
      " well yeah she said she was ------ out of --------- didnt she\n",
      " - so ----------------------- what was your ---------------- like any -------------------\n",
      " no not enough to keep me on too much longer but might check on back later on\n",
      " was kicked by ---------------------- we do not ------- in here\n",
      " --------- all this ------ has to ---- out you all be good dont do anything id do\n",
      " all the ------ ----- are here is that what youre ------- me\n",
      " ---- that ----- would have to be here just as much to know that\n",
      " ------- are not as nice to look at - they just do their job\n",
      " how do people think with a name like battlecat that im female\n",
      " not yet but its ------ up on another --- for later roughone\n",
      " ------- her eyes on his ---------- again at his request sucking so hard on his tip as she -------\n",
      " well one that was easy or --------- but if you dont have one its cool\n",
      " we dont have face ------- ------- that will suck out your -----\n",
      " if im not -------------- ------- me for --------- on this ------------- but its open jenn are you -------- is that what youre getting at\n",
      " we dont have face ------- ------- that will suck out your -----\n",
      " ------- her eyes on his ---------- again at his request sucking so hard on his tip as she -------\n",
      " was kicked by ---------------------- we do not ------- in here\n",
      " ------- are not as nice to look at - they just do their job\n",
      " okay im not feeling so well today ----- its the no coffee im ----------\n",
      " in -------- --------- we have ---------- as well as ----- here\n",
      " we dont have face ------- ------- that will suck out your -----\n",
      " ---- that ----- would have to be here just as much to know that\n",
      "###########################################################################################################################\n",
      " words are as fun as they are ------- or some ------ shit like that\n",
      " she was so ------------ when she -------- it was just -------\n",
      " i was ------- waiting for your show djawesome but im just ----\n",
      " well im around ------------ not that it ------- all that much -\n",
      " no like i dont dog people if they dont like to read but\n",
      " some come on guys are we gonna fuck sara or are we not\n",
      " its all good ------------- just dont call me late to -----\n",
      " no like i dont dog people if they dont like to read but\n",
      " bites her lip ------ ----- that all eyes are on her\n",
      " what do you have ------- for your morning --------------\n",
      " well what they came at me with was pretty ---- shit so\n",
      " well what do you ---- on doing here ------------\n",
      " but its not ------- here and we are --------- p\n",
      " wish she did not have that panties on -----------\n",
      " smiles at the man as she grins so how are you\n",
      " well that is one thing we do agree on mindseye lol\n",
      " i dont care dont bring me into your --------\n",
      " ---------- with what youre good at madmick\n",
      " but if your here im just ------- ------ you\n",
      " i dont care dont bring me into your --------\n",
      " just be glad you dont like in --- like i do\n",
      " if its not as good as it was -------- can\n",
      " its not that im into it im just ----- lolol\n",
      " good ------------------------- and all that\n",
      " but ------------- out cause here we come\n",
      " i was ------- at ----- with that at least\n",
      " smiles at --------- as she ------ me back\n",
      " hmm that -------- what do you do\n",
      " hello ---------------- how are you\n",
      " im well what ------ you here today\n",
      " good luck with that ---------------\n",
      " we have ----- that ----- to that\n",
      " well no it might just be quiet\n",
      " we have so much good ------\n",
      " what no dj what are we to do\n",
      " or they have me on ------ oo\n",
      " no no just --------- ------\n",
      " im not american its ass\n",
      " im not on as much as you p\n",
      " its good to have -----\n",
      " or maybe thats just me\n",
      " just like that ----\n",
      " are we ------- or what\n",
      " what ya doing with that\n",
      " women one was good\n",
      " but its your body\n",
      " well thats no good\n",
      " no view here\n",
      " nice one ----\n",
      " all over what\n",
      " no that wont do\n",
      " is just here\n",
      " but so -------\n",
      " yawns as well\n",
      " we have\n",
      " nope im here\n",
      " what is up\n",
      " so im good\n",
      " hey hey\n",
      " for what\n",
      " do me\n",
      " or\n",
      " its\n",
      " nice\n",
      " all\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "# structure TWO\n",
    "# uses top 0.025 dashed out sentences \n",
    "# part 1: 130 lines: gradient from percentage of most common words from 0.3 to 0.6 and minimum sentence length of 60\n",
    "# part 2: 70 lines: percentage increased linearly and reducing word length baries towards very short sentences\n",
    "# uses original authors from big dashed out sentence database\n",
    "import random\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "\n",
    "def checkline(line):\n",
    "    line = line.split(':')\n",
    "    if len(line) < 2:\n",
    "        return False\n",
    "    line = line[1]\n",
    "    if len(line) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "dofirst = True\n",
    "\n",
    "#input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "input_file = open('1606_only_top0.025_all_authordistfix.txt', mode='r', encoding='utf8').readlines()\n",
    "finlines = []\n",
    "perc = 0.3\n",
    "perc_increase = 0.0015\n",
    "wordlen_min = 60\n",
    "if dofirst:\n",
    "    for i in range(130):\n",
    "        while True:\n",
    "            line = random.choice(input_file)\n",
    "            if not checkline(line):\n",
    "                line = random.choice(input_file)\n",
    "                continue   \n",
    "            auth = line.split(':')[0]\n",
    "            line = line.split(':')[1]\n",
    "\n",
    "            counter = 0\n",
    "            wordcount = len(line.split())\n",
    "\n",
    "            if wordcount == 0:\n",
    "                continue\n",
    "\n",
    "            linelen = len(line)\n",
    "            for word in line.split():\n",
    "                for p in possible:\n",
    "                    if word == p:\n",
    "                        counter += 1\n",
    "\n",
    "            if counter / float(wordcount) > perc and linelen > wordlen_min and line not in finlines: \n",
    "                print(auth + ': ' + line.rstrip())\n",
    "                finlines.append(auth + ': ' + line)\n",
    "                perc += perc_increase\n",
    "                break\n",
    "\n",
    "print('###########################################################################################################################')\n",
    "print('second part')\n",
    "print('###########################################################################################################################')\n",
    "\n",
    "wordlen_min = 60\n",
    "wordlen_max = 70\n",
    "wordlen_decrease = 1\n",
    "for i in range(70):\n",
    "    tried = 0\n",
    "    while True and tried < 10000:\n",
    "        tried += 1\n",
    "        line = random.choice(input_file)\n",
    "        if not checkline(line):\n",
    "            line = random.choice(input_file)\n",
    "            continue   \n",
    "        auth = line.split(':')[0]\n",
    "        line = line.split(':')[1]\n",
    "            \n",
    "        counter = 0\n",
    "        wordcount = len(line.split())\n",
    "        \n",
    "        if wordcount == 0:\n",
    "            continue\n",
    "        \n",
    "        linelen = len(line)\n",
    "        for word in line.split():\n",
    "            for p in possible:\n",
    "                if word == p:\n",
    "                    counter += 1\n",
    "        \n",
    "        if counter / float(wordcount) > perc and linelen > wordlen_min and linelen < wordlen_max and line not in finlines: \n",
    "            print(auth + ': ' + line.rstrip())\n",
    "            finlines.append(auth + ': ' + line)\n",
    "            perc += perc_increase\n",
    "            wordlen_min -= wordlen_decrease\n",
    "            wordlen_max -= wordlen_decrease\n",
    "            break\n",
    "            \n",
    "print(len(finlines))\n",
    "\n",
    "outfile = open('1606_structure_02_01.txt', 'w')\n",
    "outfile.write('ACT 3 - STRUCTURE 2\\n')\n",
    "for line in finlines:\n",
    "    outfile.write(line)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "111\n",
      "125\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# count i/s in 410/501\n",
    "leaves410 = 0\n",
    "entrances410 = 0\n",
    "leaves501 = 0\n",
    "entrances501 = 0\n",
    "\n",
    "for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "    if 'has left' in line or 'has quit' in line:\n",
    "        leaves410 += 1\n",
    "    if 'joined the channel' in line:\n",
    "        entrances410 += 1\n",
    "    #if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "        #sentence = line[line.find(\":\"):]\n",
    "        #all_posts.append((author, sentence.strip().lower()))\n",
    "        #all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "        #author = line[:line.find(\":\")]\n",
    "        #if author not in authors: # save things\n",
    "            #authors[author] = set() # init list for author\n",
    "        #if author not in authors_410:\n",
    "            #authors_410[author] = set()\n",
    "        #authors[author].add(sentence.strip().lower())\n",
    "        #authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "    if 'has left' in s or 'has quit' in s:\n",
    "        leaves501 += 1\n",
    "    if 'joined the channel' in s:\n",
    "        entrances501 += 1\n",
    "    #if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "        #if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "            #sentence = s[s.find(\">\")+2:]\n",
    "            #all_posts.append((author, sentence.strip().lower()))\n",
    "            #all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "            #author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "            #if author not in authors: # save things\n",
    "            #    authors[author] = set() # init list for author\n",
    "            #if author not in authors_501:\n",
    "            #    authors_501[author] = set()\n",
    "            #authors[author].add(sentence.strip().lower())\n",
    "            #authors_501[author].add(sentence.strip().lower()) \n",
    "            \n",
    "            \n",
    "         \n",
    "print(leaves410)\n",
    "print(entrances410)\n",
    "print(leaves501)\n",
    "print(entrances501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "\n",
    "def get_random_syn(word):\n",
    "    r = word\n",
    "    syns = wn.synsets(word)\n",
    "    lemmas = []\n",
    "    for ss in syns:\n",
    "        for lem in ss.lemma_names():\n",
    "            lemmas.append(lem)\n",
    "            #r = lem\n",
    "            #if lem is not word:\n",
    "            #    return lem\n",
    "    return lemmas\n",
    "\n",
    "def replace_sentence(sentence):\n",
    "    return_sentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        lemmas = get_random_syn(word)\n",
    "        if len(lemmas) > 0:\n",
    "            #print(word + str(lemmas))\n",
    "            r = random.choice(lemmas)\n",
    "            if r is \"atomic_number_53\":\n",
    "                r = 'I'\n",
    "            return_sentence += r + \" \"\n",
    "        else:\n",
    "            return_sentence += word + ' '\n",
    "    return return_sentence\n",
    "\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    sen = random.choice(all_posts)\n",
    "    if len(sen[1]) > 10:\n",
    "        print('-------------------------')\n",
    "        print(sen[1])\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURVES generation\n",
    "\n",
    "random.seed(1)\n",
    "#render_plain('1306_removed_digits_entrances_01_ORIGINAL.txt', 500, 3860)\n",
    "#render_by_osc('osc_config_500_late_slowexp.csv', '1306_cleaner_group1&2&3_01.txt', 3860)\n",
    "print('finito')\n",
    "#generate_lines_linear(20)\n",
    "\n",
    "generate_lines_from(0.00, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.10, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.20, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.50, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.80, 0.0001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving word groups\n",
    "lists = []\n",
    "#lists.append((1, 5))\n",
    "#lists.append((6, 20))\n",
    "#lists.append((21, 50))\n",
    "#lists.append((51, 100))\n",
    "#lists.append((101, 500))\n",
    "lists.append((3000, 10000))\n",
    "\n",
    "outfile = codecs.open('3000_10000_groups.txt', 'w')\n",
    "\n",
    "for l in lists:\n",
    "    possible_words = []\n",
    "    #_config = config[index]\n",
    "    _min = l[0]\n",
    "    _max = l[1]\n",
    "    app_list = []\n",
    "    for i in range(_min, _max+1):\n",
    "        app_list.append(i)\n",
    "\n",
    "    # all allowed words\n",
    "    selected_appearances = get_by_appearance_list(app_list)\n",
    "    outfile.write(' '.join(selected_appearances))\n",
    "    #possible_words.append((selected_appearances, _config))\n",
    "    \n",
    "outfile.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess act1&2\n",
    "\n",
    "for sentence in all_posts_501:\n",
    "    print(sentence[0] + \" -> \" + remove_non_frequent(get_topn_percent(0.01), sentence[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
