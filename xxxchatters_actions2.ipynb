{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:04:08] Random_Chaos wondering who the violator and who the violatee\n",
      "\n",
      "[23:04:41] FunkyBoogieKing pounce-whomples erotic_kitty\n",
      "\n",
      "[23:06:26] T-Rex rolls in and strikes a pose\n",
      "\n",
      "[23:17:58] BigDickBBL is now known as BigDick.\n",
      "\n",
      "[23:30:49] Handyman shudders\n",
      "\n",
      "[23:33:08] Sanger{ek} softly pets my lil kitten and kisses between her ears\n",
      "\n",
      "[23:33:45] Fenderman1964 waves and sits down\n",
      "\n",
      "[23:35:54] Jothom looks around\n",
      "\n",
      "[23:37:30] Fenderman1964 waves and gets out of the way\n",
      "\n",
      "[23:40:55] Fenderman1964 is too old for that\n",
      "\n",
      "[23:46:08] Fenderman1964 minds his own business\n",
      "\n",
      "[23:51:49] Jothom lounges\n",
      "\n",
      "[23:55:58] erotic_kitty yawns and curls up on Sanger\n",
      "\n",
      "[23:56:21] Sanger snuggles my kitten\n",
      "\n",
      "[23:56:40] erotic_kitty purrssssssssssssssss\n",
      "\n",
      "[23:56:40] Fenderman1964 waves and sits out of the way\n",
      "\n",
      "[23:57:13] Sanger swats your bottom\n",
      "\n",
      "[23:57:43] Fenderman1964 shakes his head and quits\n",
      "\n",
      "[23:58:26] Sanger tucks my pet in all warm and cozy with her Smokey stuffy and reads her a bed time story from Asimovs Sci Fi\n",
      "\n",
      "[00:00:16] Fenderman1964 doesnt play and thus doesnt worry\n",
      "\n",
      "[00:02:07] DJ`liltech is now known as liltech.\n",
      "\n",
      "479403\n",
      "158959\n",
      "29066\n",
      "40\n",
      "55\n",
      "304\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import codecs\n",
    "from utils import in_blocklist\n",
    "\n",
    "blocklist = []\n",
    "blocklist.append('AWAYLEN=307 MAXTARGETS=20 WALLCHOPS WATCH=128 WATCHOPTS=A SILENCE=15 MODES=12 CHANTYPES=# PREFIX')\n",
    "blocklist.append('- * -')\n",
    "blocklist.append('XxXChatters.Com')\n",
    "blocklist.append('XxXChatters')\n",
    "blocklist.append('This server was created')\n",
    "blocklist.append('operator(s) online')\n",
    "blocklist.append('is now your displayed host')\n",
    "blocklist.append('Caps set:')\n",
    "blocklist.append('MAXCHANNELS')\n",
    "blocklist.append('http')\n",
    "blocklist.append('type it in the main room or a pm to the')\n",
    "blocklist.append('The topic is')\n",
    "blocklist.append('Topic set by')\n",
    "\n",
    "all_separated = []\n",
    "all_posts = []\n",
    "all_posts_410 = []\n",
    "all_posts_501 = []\n",
    "authors = {}\n",
    "authors_410 = {}\n",
    "authors_501 = {}\n",
    "\n",
    "def parse_all(files, separate):\n",
    "    for file in glob.glob(files):\n",
    "        lines = codecs.open(file, mode='r', encoding='utf-8')\n",
    "\n",
    "        for line in lines:\n",
    "            if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "                if line[0] is '[' and line.find(\"<\") is not -1:\n",
    "                    s = line\n",
    "                    sentence = s[s.find(\">\")+2:]\n",
    "                    time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "                    author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                    \n",
    "                    p = (time+' ' + author, sentence.strip().lower())\n",
    "                    all_posts.append(p)\n",
    "                    if separate in file:\n",
    "                        all_separated.append(p)\n",
    "\n",
    "                    if author not in authors: # save things\n",
    "                        authors[author] = set() # init list for author\n",
    "                    authors[author].add(sentence.strip().lower()) \n",
    "                else:\n",
    "                    if line.find('*') is not -1 and \") has joined #\" not in line and \") Quit (\" not in line and \") has left #\" not in line and \"6A\u000314thena\u000314, \u000f\u000306y\u000314o\u000314u \u000f\u000306n\u000314eve\u000314r\" not in line and \" sets mode: +\" not in line and \" is now known as \" not in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        sentence = line[aa_end + 1:]\n",
    "                        \n",
    "                        p = (time+' ' + author, sentence.strip().lower())\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "\n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence.strip().lower()) \n",
    "                    if \") has joined #\" in line or \") Quit (\" in line or \") has left #\" in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        if \") has joined #\" in line:\n",
    "                            author += \" JOINED THE ROOM\"\n",
    "                        else:\n",
    "                            author += \" LEFT THE ROOM\"\n",
    "                        sentence = \"\"\n",
    "                        p = (time+' ' + author, sentence)\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "                            \n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence) \n",
    "\n",
    "    for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "        if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "            sentence = line[line.find(\":\"):]\n",
    "            all_posts.append((author, sentence.strip().lower()))\n",
    "            all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "            author = line[:line.find(\":\")]\n",
    "            if author not in authors: # save things\n",
    "                authors[author] = set() # init list for author\n",
    "            if author not in authors_410:\n",
    "                authors_410[author] = set()\n",
    "            authors[author].add(sentence.strip().lower())\n",
    "            authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "    for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "        if ':' in s and '<' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "            if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "                sentence = s[s.find(\">\")+2:]\n",
    "                author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                if len(author) < 2:\n",
    "                    continue\n",
    "                \n",
    "                all_posts.append((author, sentence.strip().lower()))\n",
    "                all_posts_501.append((author, sentence.strip().lower()))\n",
    "                \n",
    "                if author not in authors: # save things\n",
    "                    authors[author] = set() # init list for author\n",
    "                if author not in authors_501:\n",
    "                    authors_501[author] = set()\n",
    "                authors[author].add(sentence.strip().lower())\n",
    "                authors_501[author].add(sentence.strip().lower()) \n",
    "        if ':' in s and '<' not in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s and ' sets mode ' not in s:\n",
    "            print(s)\n",
    "            author_beginning = s.find('] ')\n",
    "            aa = s[author_beginning+2:]\n",
    "            aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "            author = s[author_beginning+2:aa_end]\n",
    "            sentence = s[aa_end + 1:]\n",
    "            #print(author)\n",
    "            #print(sentence)\n",
    "            all_posts.append((author, sentence.strip().lower()))\n",
    "            all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "            if author not in authors: # save things\n",
    "                authors[author] = set() # init list for author\n",
    "            if author not in authors_501:\n",
    "                authors_501[author] = set()\n",
    "            authors[author].add(sentence.strip().lower())\n",
    "            authors_501[author].add(sentence.strip().lower()) \n",
    "            #print(author)\n",
    "            #print(sentence)\n",
    "            \n",
    "            \n",
    "parse_all(\"xxxchatters_logs/*.log\", \"#chat\")\n",
    "#for file in glob.glob(\"xxxchatters_logs/*.log\"):\n",
    "print(len(all_posts))\n",
    "print(len(all_separated))\n",
    "print(len(authors))\n",
    "print(len(authors_410))\n",
    "print(len(authors_501))\n",
    "print(len(all_posts_410))\n",
    "print(len(all_posts_501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'onetoquiet': {'hello random_chaos sir', 'heya erotic_kitty'}, 'FunkyBoogieKing': {'see you guys tomorrow.', \"she's a-one spicy meat-ah-ball!!!\", 'pounce-whomples erotic_kitty', \"i didn't break you!!! \\xa0>>\", 'while the mischievous slave is away, the mischievous master will play.', \"alright, everyone, it's nighty-night winky time for funkyboogieking\"}, '@erotic_kitty': {'', 'hiyas jakkiline-cd winterwhisper and mr_magoo', 'well sanger might not like it if i m broken', 'hiyas maidtiffani and dirtyoldperv', 'hiyas subgirl4bbc', 'you whomped me lao', 'hiyas avgjoe40', 'wb keynotespkr', 'hiyas pantyhosehighheelsman', 'whats wrong handyman?', 'hiyas atticus48', 'hiyas iwhisper2clits', 'hiyas zelda', 'wb fenderman1964', '!djrocks', 'old?', 'hiyas ant35', 'hiyas cyrune', 'no worries fenderman1964', 'welcome back simian hugssssssssss', 'wb eedwardgrey', 'hiyas shynica', 'hiyas masterpiece', 'hiyas hentaibaka34m', 'hiyas sub_f', 'hiyas littledick-bigheart', 'that was his one of his first ring names', 'hiyas jj_black', 'wb stoneheart', 'hiyas chad26', 'hiyas curvybeauty', 'pendragon', 'wb viking', 'hiyas lori19 and james44', 'wb country_boy30', 'hiyas ruinme03', 'hiyas texguy', 'everytime i see your nick meanmark1 i wonder if you\\x92re an undertaker fan', 'hiyas schweetjohn', 'hiyas thegreatswitchbandit', 'mwah love', 'hiyas badguy', 'night all hugsssssssssssssssssss', 'hiyas swflguy', 'for what fenderman1964', 'hiyas sol', 'hiyas slutfucker', 'hiyas mike30', 'hiyas journey7', 'hiyas vixenvictoriax', 'hiyas chad masterdrew xdom', 'hiyas fenderman1964', 'hiyas hockeyguy', 'hiyas bigfeller', 'hiyas eedwardgrey', 'hiyas comeonman', 'hiyas s', 'hiyas blackstone', 'wb meanmark', 'hiyas maisy', 'hiyas jothom', 'hiyas treason', 'welcome violatef', 'wb olderjon and hiheeled_ticklishlady', 'hiyas sole_man'}, 'Violatef': {'oh random_chaos \\xa0why?', 'heya erotic_kitty \\xa0ty'}, 'jakkiline-cd': {'hi erotic_kitty'}, '+DJ`Mercury': {'radio meltdown: dj`liltech is playing \\xa0rm sponsor channel promo - #trivia_playpen', 'radio meltdown: dj`liltech is playing', 'radio meltdown: dj`liltech is playing \\xa0van halen - dance the night away', 'radio meltdown: dj`liltech is off --> coming up: radio meltdown: dj`mercury]', 'erotic_kitty thinks radio meltdown: dj`liltech rocks!!', 'radio meltdown: dj`liltech is playing \\xa0yellow claw - run away', 'sanger thinks radio meltdown: dj`liltech rocks!!', 'radio meltdown: dj`liltech is playing \\xa0zz top - la grange', 'radio meltdown: dj`liltech is playing \\xa0queen - a kind of magic', 'radio meltdown: dj`liltech is playing \\xa0syn city cowboys - run for my life', 'radio meltdown: dj`liltech is playing \\xa0men without hats - safety dance', 'radio meltdown: dj`liltech is playing \\xa0the police - every little thing she does is magic', 'radio meltdown: dj`mercury is playing \\xa0elton john - philadelphia freedom', 'radio meltdown: dj`liltech is playing \\xa0zz top - sharp dressed man', 'radio meltdown: dj`liltech is playing \\xa0electric light orchestra - strange magic', 'radio meltdown: dj`liltech is playing \\xa0iron maiden - run to the hills', 'radio meltdown: dj`liltech is playing \\xa0rm sponsor channel promo - #vamps`cafe', 'radio meltdown: dj`liltech is playing \\xa0walk the moon - shut up and dance', \"radio meltdown: dj`liltech is playing \\xa0zz top - i'm bad, i'm nationwide\", 'radio meltdown: dj`liltech is playing \\xa0dnce - cake by the ocean'}, 'Random_Chaos': {'wondering who the violator and who the violatee'}, 'WaywardGentleman': {'have a great night'}, '@erotic_kitty{S}': {'hiyas kinkyslavegirl', 'wb keynotespkr', 'geeze', 'hiyas t-rex', '!djrocks', 'my pleasure', 'wb winterwhisper', 'hiyas salsaguy', 'hiyas sweetliz', 'wb stolen tomato', 'heyyyyyyyyyyyyyyyyyyyyyyyy', 'hiyas nick3 hugsssssssssss', 'hiyas heavyballs', 'hiyas handyman', 'night lao', 'hiyas helen19 hiheeled_ticklishlady itjustslipped jayprez', 'hiyas alicia1forf', 'hiyas springrain', 'wb joycegg', 'hiyas oldhorn57', 'hiyas rockcockinpa', 'hiyas dante', 'hiyas lil-tigress hugssssssssssssssssssssssssssssssssssss', 'wb bd hugssssssssssss', 'hiyas jillvalentine', 'hiyas chaz', 'hiyas julia_for_f'}, 'JoyceGG': {'ty \\xa0kitty'}, 'T-Rex': {'hi hi erotic_kitty', '.ö/', 'rolls in and strikes a pose'}, 'Sanger': {'good night folks \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 play nice', 'awesome song', 'hey there lil tigress', 'snuggles my kitten', 'swats your bottom', 'no breaking my kitty', 'tucks my pet in all warm and cozy with her smokey stuffy and reads her a bed time story from asimovs sci fi', '!djrocks', 'say your goodnights', 'yw'}, '+shyNica': {'heya erotic_kitty'}, 'lil-tigress': {'heya erotic_kitty. huggggssssss.', 'heya sanger sir.'}, 'keynotespkr': {'hi erotic kitty', 'thnx e kitty'}, 'rst37': {'hi springrain', 'hello ladies', 'hi jillvalentine'}, 'Nick`3': {'huggggggs erotic_kitty'}, 'EEdwardGrey': {'hello erotic_kitty'}, '@DJ`liltech': {'good night erotic_kitty', 'ty sanger', 'later sanger', 'ty erotic_kitty'}, 'Hockeyguy': {'hi erotic_kitty'}, 'JayPrez': {'hello'}, 'BigDickBBL': {'slips back into the room', 'is now known as bigdick.'}, 'SpringRain': {'hi erotic_kitty'}, 'BigDick': {'hi springrain huggggggggssss', 'ty kitty hon hugggggggsssssss'}, 'Alicia1forF': {'hi erotic_kitty'}, 'Handyman': {'cool, as long as they don\\x92t respond...verbally....', \"pussy farts is one thing...talking clits...that's quite another....\", 'yikes', 'shudders', 'hi erotic_kitty'}, 'kinkyslavegirl': {'hi erotic_kitty'}, 'EEdwardGrey^': {'thank you erotic_kitty'}, 'iWhisper2Clits': {'erotic_kitty: \\xa0hello there'}, 'Littledick-bigheart': {'hiya erotic_kitty', 'hi goddessb'}, 'SwflGuy': {'hey there erotic_kitty'}, 'sweet_teresa': {'', 'are we going disneylandddddddddddddd', 'then yes', 'do they talk back', 'maybe he\\x92s dyslexic like me', 'i ask if they talk back', 'bad mike', 'clits', 'well he whispers to them', 'no slash'}, 'Pendragon': {'good night erotic_kitty', 'welcome recentlyrapedf', 'g night sanger'}, 'JJ_Black': {'hi erotic_kitty'}, 'Jothom': {'hi vixenvictoriax', 'lounges', 'hi sub_f', 'heya erotic_kitty', 'hello blonde86nc', 'hi curvybeauty', 'how are you doing vixenvictoriax?', 'looks around', 'hi everyone', 'hi goddessb'}, 'Sanger{ek}': {'softly pets my lil kitten and kisses between her ears'}, 'Fenderman1964': {'old wrestlers', 'waves and sits out of the way', 'doesn\\x92t play and thus doesn\\x92t worry', 'shakes his head and quits', 'minds his own business', 'don\\x92t mind me', 'is too old for that', 'waves and gets out of the way', 'waves and sits down'}, 'RuinMe03': {\"hey y'all\"}, 'FriendlyMonster': {'hiya blonde86nc', 'hiya curvybeauty', 'hi there feminist_teen'}, 'GoddessB': {'hello again all'}, 'Comeonman': {'heyas erotic_kitty'}, 'MeanMark1': {'ty kitty', '', 'gotcha', 'ahh no'}, 'BigFeller': {'hi erotic_kitty'}, 'vixenvictoriax': {'hi jothom', 'hi erotic_kitty'}, 'Country_Boy30': {'we just might', 'anyone up for fun'}, 'MikeinMiami': {'join /gangbang', '#join /gangbang'}, 'simian': {'typing one handed', 'switch the / and the #'}, 'dante': {'hi sweet_teresa'}, 'SchweetJohn': {'yo'}, 'Masterpiece': {'hello erotic_kitty'}, 'Curvybeauty': {'hi'}, 'erotic_kitty': {'yawns and curls up on sanger', 'purrssssssssssssssss'}, 'Soul-of-Darkness': {'subgirl4bbc'}, 'Pwner': {'wake up'}, 'DJ`liltech': {'is now known as liltech.'}}\n",
      "410: \n",
      "carrol: 53\n",
      "Louise: 28\n",
      "Valkyrie: 21\n",
      "Threeleggedcat: 20\n",
      "PlayfulBBC: 18\n",
      "@saffron{WH}: 17\n",
      "+DJ`Mercury: 16\n",
      "WhoGiveSaDamn: 15\n",
      "Silver_haired`Fox: 14\n",
      "Woman: 8\n",
      "Cruel`Intentions: 7\n",
      "guynextdoor: 7\n",
      "Anastatia: 7\n",
      "^fran: 7\n",
      "Jaems: 6\n",
      "+DJ`South: 6\n",
      "JFetish: 5\n",
      "gracie: 4\n",
      "Athena: 3\n",
      "saffron{WH}: 3\n",
      "TricksyM: 3\n",
      "jessica110: 2\n",
      "rst36: 2\n",
      "SensualDom: 2\n",
      "OldMaster: 2\n",
      "+WolvenHeart: 2\n",
      "Tyrant: 2\n",
      "[M]yEvilTwin: 1\n",
      "lonelyhousewife: 1\n",
      "collegegirlrp19: 1\n",
      "+ DJ`Mercury: 1\n",
      "+DJ1Mercury: 1\n",
      "Stacyshusband: 1\n",
      "MasterRenegade: 1\n",
      "+DJMercury`: 1\n",
      "dark-reign: 1\n",
      "maddogmoe: 1\n",
      "StacysHusband: 1\n",
      "Mutter: 1\n",
      "natron: 1\n",
      "\n",
      "501: \n",
      "@erotic_kitty: 65\n",
      "@erotic_kitty{S}: 27\n",
      "+DJ`Mercury: 20\n",
      "Sanger: 10\n",
      "sweet_teresa: 10\n",
      "Jothom: 10\n",
      "Fenderman1964: 9\n",
      "FunkyBoogieKing: 6\n",
      "Handyman: 5\n",
      "@DJ`liltech: 4\n",
      "MeanMark1: 4\n",
      "T-Rex: 3\n",
      "rst37: 3\n",
      "Pendragon: 3\n",
      "FriendlyMonster: 3\n",
      "onetoquiet: 2\n",
      "Violatef: 2\n",
      "lil-tigress: 2\n",
      "keynotespkr: 2\n",
      "BigDickBBL: 2\n",
      "BigDick: 2\n",
      "Littledick-bigheart: 2\n",
      "vixenvictoriax: 2\n",
      "Country_Boy30: 2\n",
      "MikeinMiami: 2\n",
      "simian: 2\n",
      "erotic_kitty: 2\n",
      "jakkiline-cd: 1\n",
      "Random_Chaos: 1\n",
      "WaywardGentleman: 1\n",
      "JoyceGG: 1\n",
      "+shyNica: 1\n",
      "Nick`3: 1\n",
      "EEdwardGrey: 1\n",
      "Hockeyguy: 1\n",
      "JayPrez: 1\n",
      "SpringRain: 1\n",
      "Alicia1forF: 1\n",
      "kinkyslavegirl: 1\n",
      "EEdwardGrey^: 1\n",
      "iWhisper2Clits: 1\n",
      "SwflGuy: 1\n",
      "JJ_Black: 1\n",
      "Sanger{ek}: 1\n",
      "RuinMe03: 1\n",
      "GoddessB: 1\n",
      "Comeonman: 1\n",
      "BigFeller: 1\n",
      "dante: 1\n",
      "SchweetJohn: 1\n",
      "Masterpiece: 1\n",
      "Curvybeauty: 1\n",
      "Soul-of-Darkness: 1\n",
      "Pwner: 1\n",
      "DJ`liltech: 1\n",
      "\n",
      "combined: \n",
      "@erotic_kitty: 65\n",
      "carrol: 53\n",
      "+DJ`Mercury: 36\n",
      "Louise: 28\n",
      "@erotic_kitty{S}: 27\n",
      "Valkyrie: 21\n",
      "Threeleggedcat: 20\n",
      "PlayfulBBC: 18\n",
      "@saffron{WH}: 17\n",
      "WhoGiveSaDamn: 15\n",
      "Silver_haired`Fox: 14\n",
      "Sanger: 10\n",
      "sweet_teresa: 10\n",
      "Jothom: 10\n",
      "Fenderman1964: 9\n",
      "Woman: 8\n",
      "Cruel`Intentions: 7\n",
      "guynextdoor: 7\n",
      "Anastatia: 7\n",
      "^fran: 7\n",
      "Jaems: 6\n",
      "+DJ`South: 6\n",
      "FunkyBoogieKing: 6\n",
      "JFetish: 5\n",
      "Handyman: 5\n",
      "gracie: 4\n",
      "@DJ`liltech: 4\n",
      "MeanMark1: 4\n",
      "Athena: 3\n",
      "saffron{WH}: 3\n",
      "TricksyM: 3\n",
      "T-Rex: 3\n",
      "rst37: 3\n",
      "Pendragon: 3\n",
      "FriendlyMonster: 3\n",
      "jessica110: 2\n",
      "rst36: 2\n",
      "SensualDom: 2\n",
      "OldMaster: 2\n",
      "+WolvenHeart: 2\n",
      "Tyrant: 2\n",
      "onetoquiet: 2\n",
      "Violatef: 2\n",
      "lil-tigress: 2\n",
      "keynotespkr: 2\n",
      "BigDickBBL: 2\n",
      "BigDick: 2\n",
      "Littledick-bigheart: 2\n",
      "vixenvictoriax: 2\n",
      "Country_Boy30: 2\n",
      "MikeinMiami: 2\n",
      "simian: 2\n",
      "erotic_kitty: 2\n",
      "[M]yEvilTwin: 1\n",
      "lonelyhousewife: 1\n",
      "collegegirlrp19: 1\n",
      "+ DJ`Mercury: 1\n",
      "+DJ1Mercury: 1\n",
      "Stacyshusband: 1\n",
      "MasterRenegade: 1\n",
      "+DJMercury`: 1\n",
      "dark-reign: 1\n",
      "maddogmoe: 1\n",
      "StacysHusband: 1\n",
      "Mutter: 1\n",
      "natron: 1\n",
      "jakkiline-cd: 1\n",
      "Random_Chaos: 1\n",
      "WaywardGentleman: 1\n",
      "JoyceGG: 1\n",
      "+shyNica: 1\n",
      "Nick`3: 1\n",
      "EEdwardGrey: 1\n",
      "Hockeyguy: 1\n",
      "JayPrez: 1\n",
      "SpringRain: 1\n",
      "Alicia1forF: 1\n",
      "kinkyslavegirl: 1\n",
      "EEdwardGrey^: 1\n",
      "iWhisper2Clits: 1\n",
      "SwflGuy: 1\n",
      "JJ_Black: 1\n",
      "Sanger{ek}: 1\n",
      "RuinMe03: 1\n",
      "GoddessB: 1\n",
      "Comeonman: 1\n",
      "BigFeller: 1\n",
      "dante: 1\n",
      "SchweetJohn: 1\n",
      "Masterpiece: 1\n",
      "Curvybeauty: 1\n",
      "Soul-of-Darkness: 1\n",
      "Pwner: 1\n",
      "DJ`liltech: 1\n"
     ]
    }
   ],
   "source": [
    "authors_410_second = []\n",
    "for a in authors_410:\n",
    "    authors_410_second.append((a, len(authors_410[a])))\n",
    "    \n",
    "authors_501_second = []\n",
    "for a in authors_501:\n",
    "    authors_501_second.append((a, len(authors_501[a])))\n",
    "    \n",
    "authors_410_second_sorted = sorted(authors_410_second, key=operator.itemgetter(1), reverse=True) \n",
    "authors_501_second_sorted = sorted(authors_501_second, key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print(authors_501)\n",
    "\n",
    "authors_merged = {}\n",
    "for a in authors_410:\n",
    "    authors_merged[a] = len(authors_410[a])\n",
    "    \n",
    "#print(authors_merged)\n",
    "for a in authors_501:\n",
    "    if a in authors_merged:\n",
    "        authors_merged[a] += len(authors_501[a])\n",
    "    if a not in authors_merged:\n",
    "        authors_merged[a] = len(authors_501[a])\n",
    "        \n",
    "#print(authors_merged)\n",
    "\n",
    "authors_merged_second = []\n",
    "for a in authors_merged:\n",
    "    authors_merged_second.append((a, authors_merged[a]))\n",
    "    \n",
    "authors_merged_second_sorted = sorted(authors_merged_second, key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "print('410: ')\n",
    "for ps in authors_410_second_sorted:\n",
    "    print(ps[0] + ': ' + str(ps[1]))\n",
    "    \n",
    "print()\n",
    "print('501: ')\n",
    "for ps in authors_501_second_sorted:\n",
    "    print(ps[0] + ': ' + str(ps[1]))\n",
    "    \n",
    "\n",
    "print()\n",
    "print('combined: ')\n",
    "for ps in authors_merged_second_sorted:\n",
    "    print(ps[0] + ': ' + str(ps[1]))\n",
    "#authors_410.sort(key=takeSecond)\n",
    "#print(authors_410_sorted)\n",
    "#for a in authors_410:\n",
    "#    print(a + ': ' + str(len(authors_410[a])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in authors['carrol']:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49231\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import string\n",
    "word_counts = dict()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess_word(word):\n",
    "    word = word.replace('\\x02', '')\n",
    "    word = word.replace('\\x03', '')\n",
    "    word = word.replace('.', '')\n",
    "    word = ''.join([i for i in word if not i.isdigit()])\n",
    "\n",
    "    #word = word.translate(translator)\n",
    "    word = ''.join(e for e in word if e.isalnum())\n",
    "    return word\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    word_list = post[1].split()\n",
    "    for word in word_list:\n",
    "        word = preprocess_word(word)\n",
    "        #word = word.replace('\\x03', '')\n",
    "        #word = word.replace('.', '')\n",
    "\n",
    "        #word = word.translate(translator)\n",
    "        #word = ''.join(e for e in word if e.isalnum())\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "def get_sorted_word_counts():\n",
    "    return sorted_word_counts\n",
    "\n",
    "v = get_sorted_word_counts()\n",
    "print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extra', 'arm', 'large', 'past', 'quickly', 'irc', 'human', 'rl', 'size', 'wine', 'leaves', 'firm', 'able', 'summer', 'dylan', 'devil', 'naderaax', 'kinky', 'jag']\n"
     ]
    }
   ],
   "source": [
    "def get_topn_percent(perc):\n",
    "    counts = get_sorted_word_counts()\n",
    "    l = len(counts)\n",
    "    max_index = int(perc*l)\n",
    "    top_n = []\n",
    "    #print(counts[:max_index])\n",
    "    for t in counts[:max_index]:\n",
    "        top_n.append(t[0])\n",
    "    return top_n\n",
    "\n",
    "def get_all_by_max_appearance(app):\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] >= app:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "\n",
    "def get_by_appearance_list(appearance):\n",
    "    # appearance = [2, 3, 4] # number indicating times of appearance\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] in appearance:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "    \n",
    "#topn = get_topn_percent(0.001)\n",
    "\n",
    "selected_appearances = get_by_appearance_list([110, 112])\n",
    "print(selected_appearances)\n",
    "\n",
    "#maxapp = get_all_by_max_appearance(2)\n",
    "#file = open(\"1106/chat_room_word_appearance.txt\", 'w')\n",
    "#for e in maxapp:\n",
    "#    try:\n",
    "#        file.write(e[0] + \", \" + str(e[1]) + '\\n')\n",
    "#    except:\n",
    "#        pass\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- to keep being evil ---- athena on the -------- with a new nerf bat\n",
      "jus to keep being -------- ---- athena on the backside with a new nerf bat\n"
     ]
    }
   ],
   "source": [
    "def remove_non_frequent(allowed_words, post, replace=True):\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        pword = preprocess_word(word)\n",
    "        if pword in allowed_words:\n",
    "            good_words.append(pword)\n",
    "        else:\n",
    "            if replace:\n",
    "                replacement = \"\"\n",
    "                for char in word:\n",
    "                    replacement += \"-\"\n",
    "                good_words.append(replacement)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "\n",
    "def remove_by_probability(allowed_words, always_allowed, post, chance, replace=True):\n",
    "    import random\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        #pword = preprocess_word(word)\n",
    "        if word in always_allowed:\n",
    "            good_words.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in allowed_words:\n",
    "            good_words.append(word)\n",
    "        else:\n",
    "            if replace:\n",
    "                rv = random.uniform(0.0, 1.0)\n",
    "                if rv < chance:\n",
    "                    replacement = \"\"\n",
    "                    for char in word:\n",
    "                        replacement += \"-\"\n",
    "                    good_words.append(replacement)\n",
    "                else:\n",
    "                    good_words.append(word)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "sentence_to_test = \"Jus to keep being evil<<<< wops Athena on the backside with a new nerf bat\"\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n",
    "\n",
    "print(remove_by_probability(get_topn_percent(0.1), get_topn_percent(0.01), sentence_to_test, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ------- on the -------- with one of my new ---- ----\n",
      "bops saffron on the -------- with one of my new nerf bats\n"
     ]
    }
   ],
   "source": [
    "sentence_to_test = \"bops saffron on the backside with one of my new nerf bats\"\n",
    "print(remove_non_frequent(get_topn_percent(0.01), sentence_to_test))\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@erotic_kitty', 65), ('carrol', 53), ('+DJ`Mercury', 36), ('Louise', 28), ('@erotic_kitty{S}', 27), ('Valkyrie', 21), ('Threeleggedcat', 20), ('PlayfulBBC', 18), ('@saffron{WH}', 17), ('WhoGiveSaDamn', 15), ('Silver_haired`Fox', 14), ('Sanger', 10), ('sweet_teresa', 10), ('Jothom', 10), ('Fenderman1964', 9), ('Woman', 8), ('Cruel`Intentions', 7), ('guynextdoor', 7), ('Anastatia', 7), ('^fran', 7), ('Jaems', 6), ('+DJ`South', 6), ('FunkyBoogieKing', 6), ('JFetish', 5), ('Handyman', 5), ('gracie', 4), ('@DJ`liltech', 4), ('MeanMark1', 4), ('Athena', 3), ('saffron{WH}', 3)]\n",
      "['@erotic_kitty' 'carrol' '@erotic_kitty' '@erotic_kitty' '@erotic_kitty'\n",
      " 'carrol' '@erotic_kitty' '@erotic_kitty' 'carrol' 'carrol' 'carrol'\n",
      " 'carrol' '@erotic_kitty' '@erotic_kitty' 'carrol' '@erotic_kitty'\n",
      " '@erotic_kitty' 'carrol' '@erotic_kitty' '@erotic_kitty']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_by_probability(authors_list, count, topn = 30):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for authors in authors_list:\n",
    "        for key, value in authors.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "                \n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #print(author_probability_counts[:topn])\n",
    "\n",
    "    sum_sentences = 0\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        sum_sentences += v[1]\n",
    "\n",
    "    author_probability_final = {}\n",
    "    author_list = []\n",
    "    prob_list = []\n",
    "\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        author_probability_final[v[0]] = v[1]/float(sum_sentences)\n",
    "        author_list.append(v[0])\n",
    "        prob_list.append(v[1]/float(sum_sentences))\n",
    "\n",
    "    final_autor_list = np.random.choice(\n",
    "      author_list, \n",
    "      count,\n",
    "      p=prob_list\n",
    "    )\n",
    "\n",
    "    return final_autor_list\n",
    "\n",
    "def get_topn_authors(author_lists, n):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for author_list in author_lists:\n",
    "        for key, value in author_list.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "\n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return author_probability_counts[:n]\n",
    "\n",
    "#print(get_by_probability(authors, 20))\n",
    "#print(get_by_probability(authors_410, 20))\n",
    "#print(get_by_probability(authors_501, 20))\n",
    "\n",
    "print(get_topn_authors([authors_410,authors_501], 30))\n",
    "print(get_by_probability([authors_410,authors_501], 20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load osc config and parse for later use\n",
    "import csv\n",
    "\n",
    "def load_osc_config(file_name):\n",
    "    config = {}\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter='\\t', quotechar='|')\n",
    "        index = 0\n",
    "        for row in reader:\n",
    "            if index > 0:\n",
    "                #print(row)\n",
    "                for i in range(len(row)):\n",
    "                    config[i].append(row[i])\n",
    "            else:\n",
    "                config[0] = []\n",
    "                config[1] = []\n",
    "                config[2] = []\n",
    "                config[3] = []\n",
    "                config[4] = []\n",
    "            index += 1\n",
    "    \n",
    "    return config\n",
    "            \n",
    "osc_config = load_osc_config('osc_config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-47a6c1561ee5>:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "def generate_lines_linear(count):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        perc = len(return_lines)/float(count)\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(perc)\n",
    "        #print(len(topn))\n",
    "        #print(sentence)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if len(stripped_post) > 20 and stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "        \n",
    "def generate_lines_from(fromp, step, count):\n",
    "    for i in range(count):\n",
    "        sentence = random.choice(all_posts)\n",
    "        perc = fromp + i * step\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(len(topn))\n",
    "        stripped_post = remove_non_frequent(topn, sentence[1].strip().lower())\n",
    "        print(\"{:.6f}\".format(perc) + \": \" + stripped_post)\n",
    "        \n",
    "def generate_by_osc(config, lists, offset=90):\n",
    "    assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n",
    "\n",
    "    index = 0\n",
    "    possible_words = []\n",
    "    # length of generated sentences is determined by osc/curve data\n",
    "    length = len(config[0])\n",
    "    for l in lists:\n",
    "        _config = config[index]\n",
    "        _min = l[0]\n",
    "        _max = l[1]\n",
    "        app_list = []\n",
    "        for i in range(_min, _max+1):\n",
    "            app_list.append(i)\n",
    "        \n",
    "        # all allowed words\n",
    "        selected_appearances = get_by_appearance_list(app_list)\n",
    "        possible_words.append((selected_appearances, _config))\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    # all words with min appearance of 500 are always allowed\n",
    "    always_allowed = get_all_by_max_appearance(500)\n",
    "        \n",
    "    finished_sentences = []\n",
    "    for i in range(length):\n",
    "        #s = random.choice(all_posts)[1]\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        # don't process djs\n",
    "        if 'DJ`Mercury' in a or 'DJ`Protea' in a:\n",
    "            finished_sentences.append((a, s))\n",
    "            continue\n",
    "            \n",
    "        for p in possible_words:\n",
    "            _words = p[0]\n",
    "            probability = float(p[1][i])\n",
    "            s = remove_by_probability(_words, always_allowed, s, probability)\n",
    "        \n",
    "        finished_sentences.append((a, s))\n",
    "    \n",
    "    return finished_sentences\n",
    "\n",
    "def render_plain(out_filename, line_count, offset):\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for i in range(line_count):\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        outfile.write(a + ': ' + s + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "def render_by_osc(osc_config, out_filename, offset):\n",
    "    lists = []\n",
    "    lists.append((1, 5))\n",
    "    lists.append((6, 20))\n",
    "    lists.append((21, 50))\n",
    "    #lists.append((51, 100))\n",
    "    #lists.append((101, 500))\n",
    "    osc_config = load_osc_config(osc_config)\n",
    "    generated = generate_by_osc(osc_config, lists, offset)\n",
    "\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for line in generated:\n",
    "        outfile.write(line[0] + ': ' + line[1] + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@erotic_kitty' '@erotic_kitty' '@DJ`liltech']\n"
     ]
    }
   ],
   "source": [
    "auth = get_by_probability([authors_410,authors_501], 300)\n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% (126709 of 130000) |############## | Elapsed Time: 20:20:13 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# samples random lines from corpus, add a author from act1&2 by distribution and outout after a certain length of the line\n",
    "# randomly add entrences and exits from the last 10 of the top active 30 authors of act1&2\n",
    "# entire corpus processed and saved in one 7.5mb text file\n",
    "\n",
    "#random.seed(1)\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "max_count = 170000\n",
    "#auth = get_by_probability([authors_410,authors_501], max_count)\n",
    "\n",
    "def generate_lines_maxapp(count, maxapp):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        #perc = len(return_lines)/float(count)\n",
    "        #perc = 0.5\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        #topn = get_all_by_max_appearance(maxapp)\n",
    "        topn = get_topn_percent(maxapp)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        #if len(stripped_post) > 20 and len(stripped_post) < 50 and stripped_post not in return_lines:\n",
    "        if stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "def process_all(maxapp):\n",
    "    return_lines = []\n",
    "    for p in all_posts:\n",
    "        sentence = p[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        topn = get_topn_percent(maxapp)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "outfile = codecs.open('1606_only_top0.025_all_authordistfix.txt', mode='w')\n",
    "\n",
    "# 20 top authors from 401 & 510\n",
    "auth_text = get_by_probability([authors_410,authors_501], max_count, 20)\n",
    "auth_io = get_topn_authors([authors_410,authors_501], 30)[-10:] # get_by_probability([authors_410,authors_501], 50, 30)[-10:]\n",
    "\n",
    "index = 0\n",
    "#lines = generate_lines_maxapp(max_count, 0.025)\n",
    "lines = process_all(0.025)\n",
    "print(len(lines))\n",
    "for line in lines:\n",
    "    l = ''.join(line)\n",
    "    outfile.write(auth_text[index] + ': ' + l + '\\n')\n",
    "    pbar.update(index)\n",
    "    if random.uniform(0, 1) > 0.7:\n",
    "        lj = \"\"\n",
    "        if random.uniform(0, 1) > 0.5:\n",
    "            lj = \" LEFT THE ROOM\"\n",
    "        else:\n",
    "            lj = \" JOINED THE ROOM\"\n",
    "        outfile.write(random.choice(auth_io)[0] + lj + '\\n')\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "#pbar.update(max_count)\n",
    "outfile.close()\n",
    "print('finished')\n",
    "# random in/outs for last 10\n",
    "# top_n 2000 / 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no no no\n",
      "annnnoooooooorrrrrraaaaaaaaaaaaa\n",
      "i know you knew, i just wanna know the interest of it, knowing that you know\n",
      "we have snow nordicgoddess but not enough to be snowed in .... yet lol\n",
      "no pwincess_aria no no no no\n",
      "pretends to know nothing then no one expects me to know anything\n",
      "no nononononono\n",
      "nnnnnoooo more poucing, hehe\n",
      "nnnnnooooooooooooooooooooooo\n",
      "yeah i know dj`annora not a good thing right now\n",
      "\u0002\u000307,01 limits: \u000300 no pee and scat, no permanent injuries, no animals nor kids\n",
      "lol no no no no\n",
      "nnnnnnnnnnnooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "# attempt to find similar sentences by matching vocabulary\n",
    "# FAILED NOT USED\n",
    "\n",
    "#li = ['her', 'her', 'her', 'he']\n",
    "li = ['are', 'fake']\n",
    "li = ['what', 'she', 'does']\n",
    "#li = ['what', 'what', 'what']\n",
    "li = ['is', 'is', 'is', 'is']\n",
    "li = ['being'] * 3\n",
    "\n",
    "#li = ['good', 'good', 'am']\n",
    "#li = ['here', 'here', 'here']\n",
    "#li = ['nice', 'nice', 'very']\n",
    "#li = ['well', 'well']\n",
    "#li = ['ll', 'll', 'll', 'll']\n",
    "li = ['no'] * 4\n",
    "#li = ['just', 'well', 'i']\n",
    "index = 0\n",
    "for post in all_posts:\n",
    "    p = post[1]\n",
    "    #print(p)\n",
    "    success = True\n",
    "    for l in li:\n",
    "        if l in p:\n",
    "            index = p.find(l)\n",
    "            length = len(l)\n",
    "            endindex = index + length\n",
    "            subs = p[index:endindex] \n",
    "            leftover = p[:index] + p[endindex:]\n",
    "            p = leftover\n",
    "        else:\n",
    "            success = False\n",
    "    if success and len(post[1]) < 80: \n",
    "        print(post[1])\n",
    "    index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: to be ---- to see\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1281 of 164986) |                  | Elapsed Time: 0:00:15 ETA:   0:27:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Athena: -------- i enjoy the open air ---------- such as ------ or ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (3173 of 164986) |                  | Elapsed Time: 0:00:34 ETA:   0:26:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 1: Jothom: --- why im ------------ tired and body -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (16781 of 164986) |#                | Elapsed Time: 0:02:55 ETA:   0:25:18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 0: +DJ`Mercury: so by the ----- of ---- --- and she was --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% (45377 of 164986) |####             | Elapsed Time: 0:07:50 ETA:   0:21:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Silver_haired`Fox: damn i thought i was ---- thin at --- and ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (164985 of 164986) |############### | Elapsed Time: 0:29:25 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Athena: -------- i enjoy the open air ---------- such as ------ or ---------\n",
      "matched pattern 1: Jothom: --- why im ------------ tired and body -----\n",
      "matched pattern 0: +DJ`Mercury: so by the ----- of ---- --- and she was --\n",
      "matched pattern 2: Silver_haired`Fox: damn i thought i was ---- thin at --- and ---\n"
     ]
    }
   ],
   "source": [
    "# todo: 1406\n",
    "# insert sentence into parser\n",
    "# extract gramatical structure\n",
    "# try to match through corpus\n",
    "# use pre-processed text, with dashes \n",
    "# FAILED NOT USED\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "# punctation is NFP or PUNCT\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_tag_sequence(sen, detailed=False):\n",
    "    doc = nlp(sen)\n",
    "    sentence_pos_list = []\n",
    "    sentence_tag_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        sentence_pos_list.append(token.pos_)\n",
    "        sentence_tag_list.append(token.tag_)\n",
    "    \n",
    "    if detailed:\n",
    "        return sentence_tag_list\n",
    "    \n",
    "    return sentence_pos_list\n",
    "\n",
    "def match_sequence(seq_to_match, seq_of_target):\n",
    "    to_match = ''.join(seq_to_match)\n",
    "    of_target = ''.join(seq_of_target)\n",
    "    if to_match in of_target:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_sequences(sequences, seq_of_target):\n",
    "    mmm = []\n",
    "    for s in sequences:\n",
    "        mmm.append(match_sequence(s, seq_of_target))\n",
    "    return mmm\n",
    "\n",
    "    \n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "\n",
    "detailed = True    \n",
    "\n",
    "ssss = []\n",
    "ssss.append('---- and i was ----')\n",
    "ssss.append('---- green and red ----')\n",
    "ssss.append('---- such as ---- or ----')\n",
    "ssss.append('---- hi ---- and ----')\n",
    "ssss.append('---- of the wall ---- of the bank')\n",
    "taglist = []\n",
    "for s in ssss:\n",
    "    taglist.append(get_tag_sequence(s, detailed))\n",
    "    \n",
    "print('input: ' + sentence)\n",
    "matched_sentences = []\n",
    "index = 0\n",
    "bar = ProgressBar(widget=[Percentage(), Bar()], maxval=len(input_file)).start()\n",
    "for line in input_file:\n",
    "    if len(line) > 2:\n",
    "        tseq = get_tag_sequence(line.rstrip(), detailed)\n",
    "        matched = match_sequences(taglist, tseq)\n",
    "        ii = 0\n",
    "        for m in matched:\n",
    "            if m:\n",
    "                matched_sentences.append('matched pattern ' + str(ii) + ': ' + line.rstrip())\n",
    "                print('matched pattern ' + str(ii) + ': ' + line.rstrip())\n",
    "            else:\n",
    "                pass\n",
    "            ii += 1\n",
    "        index += 1\n",
    "        #if index > 2000:\n",
    "        #    break\n",
    "        bar.update(index)\n",
    "for m in matched_sentences:\n",
    "    print(m)\n",
    "if len(matched_sentences) == 0:\n",
    "    print('no results :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses big corpus of dashed out text and matches by percent the occence of a special list of very common words\n",
    "# kinda FAILED not used\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "max_line_counter = 0\n",
    "out_file = open('1406_min5_words_relative0.45.txt', 'w')\n",
    "perc = 0.45\n",
    "\n",
    "\n",
    "for line in input_file:\n",
    "    counter = 0\n",
    "    wordcount = len(line.split())\n",
    "    \n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "    if counter/float(wordcount) > perc and wordcount > 10:\n",
    "        out_file.write(line)\n",
    "        print(line)\n",
    "    max_line_counter += 1\n",
    "    #if max_line_counter > 90000:\n",
    "    #    break\n",
    "    \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure ONE\n",
    "\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "\n",
    "#input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "input_file = open('1606_only_top0.025_all_authordistfix.txt', mode='r', encoding='utf8').readlines()\n",
    "max_line_counter = 0\n",
    "out_file = open('1506_structure_01.txt', 'w')\n",
    "outfile.write('ACT 3 - STRUCTURE 1\\n')\n",
    "\n",
    "perc = 0.3\n",
    "chosen = []\n",
    "wordcount_min = 10\n",
    "\n",
    "for line in input_file:\n",
    "    #perc += 0.002\n",
    "    counter = 0\n",
    "    #print(line)\n",
    "    line = line.split(':')\n",
    "    \n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    auth = line[0]\n",
    "    line = line[1]\n",
    "    if len(line) < 3:\n",
    "        continue\n",
    "    wordcount = len(line.split())\n",
    "\n",
    "\n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "\n",
    "    if counter / float(wordcount) > perc and wordcount > wordcount_min:\n",
    "        perc += 0.002\n",
    "        chosen.append(line)\n",
    "        out_file.write(auth + ': ' + line)\n",
    "        print(str(perc) + ': ' + line.rstrip())\n",
    "        \n",
    "    if len(chosen) > 150:\n",
    "        break\n",
    "\n",
    "print('###########################################################################################################################')\n",
    "print('second part')\n",
    "print('###########################################################################################################################')\n",
    "\n",
    "short_chosen = []\n",
    "min_len = 50\n",
    "max_len = 140\n",
    "for line in input_file:\n",
    "    line = line.split(':')\n",
    "    if len(line) < 2:\n",
    "        continue \n",
    "    auth = line[0]\n",
    "    line = line[1]\n",
    "    if len(line) < 3:\n",
    "        continue\n",
    "    counter = 0\n",
    "    wordcount = len(line.split())\n",
    "    linelen = len(line)\n",
    "    \n",
    "    if wordcount == 0:\n",
    "        continue\n",
    "    \n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "                \n",
    "    \n",
    "    if counter / float(wordcount) > perc and linelen < max_len and linelen > min_len:\n",
    "        min_len -= 1\n",
    "        max_len -= 2\n",
    "        perc += 0.002\n",
    "        short_chosen.append(line)\n",
    "        out_file.write(auth + ': ' + line)\n",
    "        print(str(perc) + ': ' + line.rstrip())\n",
    "        \n",
    "    if len(short_chosen) > 70:\n",
    "        break\n",
    "    #max_line_counter += 1\n",
    "\n",
    "print(len(short_chosen))\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3;0.308;74;60;900;@erotic_kitty;------ -------- or ------- that will ------- in all -------- -- on -------\n",
      "0.301;0.364;99;60;900;@erotic_kitty;moans into his cock ------- her face up and down the ------ as she sucks it in and out of her mouth\n",
      "0.303;0.5;74;60;900;@erotic_kitty;------------ back into ------------- ----- in your ------- well n all that\n",
      "0.304;0.333;99;60;900;@erotic_kitty;throws some --------- at cammy here so you dont have to be naked when you ------- get out of bed --\n",
      "0.306;0.5;67;60;900;@erotic_kitty;------- his cock back into her face and holds her on his cock again\n",
      "0.307;0.333;91;60;900;@erotic_kitty;no i like to play sometimes but i have a bad cold and i just dont have the ------ tonight p\n",
      "0.309;0.333;80;60;900;sweet_teresa;super ----- himself to ---------- ------ for ------- wont do you any good at all\n",
      "0.31;0.35;95;60;900;Fenderman1964;----- her fingers around his cock at the ---- and then rubs her fingers up and down on his cock\n",
      "0.312;0.318;110;60;900;@erotic_kitty;just grins ------- her head no ive had a coffee or two i guess as she -------- her last -------- ----------- d\n",
      "0.314;0.364;65;60;900;@erotic_kitty;just like any country when you dont ------ ----- with -----------\n",
      "0.315;0.385;66;60;900;@erotic_kitty;hey be --------- she could have left your eyes in the ----- ------\n",
      "0.317;0.357;66;60;900;@erotic_kitty;wait what did i do djawesome -- i was being good and well --------\n",
      "0.318;0.467;62;60;900;@erotic_kitty;no hi no how are you - no dinner or ------ just straight to it\n",
      "0.32;0.333;71;60;900;@erotic_kitty;sorry ---------------------- but i chat in here but ty for asking first\n",
      "0.321;0.333;168;60;900;@erotic_kitty;---- and i cant stop thinking of -------- how all her -------- have this smile that is her for ------- smile how i know what her real face looks like the --------- ones\n",
      "0.323;0.429;68;60;900;@erotic_kitty;well have to give her a ------ -------- to make good on her --------\n",
      "0.324;0.462;70;60;900;@erotic_kitty;i dont really do the -------- ------- -------------- so its all good d\n",
      "0.326;0.364;64;60;900;Louise;------ out what --------------- has on when she ------- the room\n",
      "0.327;0.333;63;60;900;@erotic_kitty;well it ------------- ------- be your first name or last name -\n",
      "0.329;0.385;62;60;900;@erotic_kitty;yeah ---- work well with some ------ but not to just drink ---\n",
      "0.33;0.4;83;60;900;@erotic_kitty;waves at jenn giving her an ------------ --------- as she ---- ------- on her chair\n",
      "0.332;0.368;81;60;900;Fenderman1964;how do you ---- with that just --- it onto the old ------ when you run out of one\n",
      "0.333;0.353;94;60;900;@erotic_kitty;good there are others like some here --- can help ----- ------------ your --------- is not new\n",
      "0.335;0.385;64;60;900;@erotic_kitty;at least its not ---------- that would kill me id die of -------\n",
      "0.336;0.462;67;60;900;@erotic_kitty;im an ------ at ------ ----- back together with ----- if that -----\n",
      "0.338;0.357;63;60;900;@erotic_kitty;i just ---- from a --- so i ------- not ----------- its just us\n",
      "0.339;0.438;65;60;900;@erotic_kitty;well im off for the ----- you know how to ----- me if you need me\n",
      "0.341;0.385;63;60;900;@erotic_kitty;------ a ---- back at night what road are we going down already\n",
      "0.342;0.429;67;60;900;@erotic_kitty;-- ------- ------ than here so at least you have that going for you\n",
      "0.344;0.4;94;60;900;@erotic_kitty;as fun as it sounds i dont know if i have the attention ---- for something like that right now\n",
      "0.345;0.385;63;60;900;@erotic_kitty;i ------------ when i was -------- but its not where i ----- up\n",
      "0.347;0.357;67;60;900;@erotic_kitty;------- herself onto ----- as i dance with her my hands on her hips\n",
      "0.348;0.385;64;60;900;@erotic_kitty;- laughing out loud no he didnt do that but he is --------------\n",
      "0.35;0.357;77;60;900;@erotic_kitty;- not that i know of but then im not in the ----------------- ---------------\n",
      "0.351;0.417;68;60;900;@erotic_kitty;its not but its not ---- to ----- of other -------- ----------------\n",
      "0.353;0.364;62;60;900;@erotic_kitty;--------- all the ones under ------------ dont work for me ---\n",
      "0.354;0.385;99;60;900;@erotic_kitty;just dont let them touch ----------------- be like ------ and ------------------ be ---------------\n",
      "0.356;0.375;78;60;900;@erotic_kitty;the weather here has been all ----- of ------ up as well cold night here again\n",
      "0.357;0.368;94;60;900;^fran;i dont ----- an -------- that dont make sounds at me sounds like ------ -------- to me etc etc\n",
      "0.359;0.364;96;60;900;carrol;well so we got --------- we got ----- what ya into what else do i need to --- down about ya then\n",
      "0.36;0.375;102;60;900;@erotic_kitty;well thats i must of ------ him off i told you all its not safe for me to be naked but you dont listen\n",
      "0.362;0.385;128;60;900;@erotic_kitty;it is ------ what they can do with ---------- and we all know its coming with all the -------- they have done ---- far but still\n",
      "0.363;0.438;64;60;900;@erotic_kitty;not for a young ---- like me justme lol -- is that what you do p\n",
      "0.365;0.5;77;60;900;@erotic_kitty;but hey if you dont feel like fucking or sucking i mean its your world ------\n",
      "0.366;0.385;68;60;900;@erotic_kitty;------------- its nice to know that someone in here is ----- than me\n",
      "0.368;0.389;91;60;900;@erotic_kitty;walks back over to her -------- chair and ------ at his feet smiling as she ------- his leg\n",
      "0.369;0.375;72;60;900;@erotic_kitty;its actually a pretty nice day here its -------- to get up to the s here\n",
      "0.371;0.5;63;60;900;@erotic_kitty;---- that ----- would have to be here just as much to know that\n",
      "0.372;0.375;122;60;900;@erotic_kitty;----- out ------- her back hard as he sucks her clit and ----- her up with two fingers sliding deep inside her tight pussy\n",
      "0.374;0.429;68;60;900;@erotic_kitty;shes been nothing but nice to me but well keep that our dirty ------\n",
      "0.375;0.417;65;60;900;@erotic_kitty;im at work -------------- only for another half ---- or so though\n",
      "0.377;0.444;88;60;900;@erotic_kitty;its always at your ------- its not gonna be at mine whats the point of that invictus lol\n",
      "0.378;0.385;139;60;900;PlayfulBBC;bites softly on on your tip ------- your --------- as i suck your cock out of your pants slowly ------- your band down that --------- shaft\n",
      "0.38;0.385;62;60;900;@erotic_kitty;----- her one of the -------- for the --- ------ im working on\n",
      "0.381;0.429;72;60;900;@erotic_kitty;nice smile ----------- - might just have to take --- up on that --------\n",
      "0.383;0.438;64;60;900;@erotic_kitty;im all right i mean im ----- so cant be all that bad right - ---\n",
      "0.384;0.385;66;60;900;carrol;smiles as she slowly works out any ----- of his back and ---------\n",
      "0.386;0.462;64;60;900;@erotic_kitty;if you are ---------- you are ------- with where and how you are\n",
      "0.387;0.429;66;60;900;@erotic_kitty;hey whats good for the ----- should be good for the ------ too lol\n",
      "0.389;0.429;67;60;900;Louise;those are nice lips ----- but its not enough to bring kymmi back --\n",
      "0.39;0.419;141;60;900;@erotic_kitty;she was all like thank you for ------- me up it was really nice of you and im like yeah yeah ------------ get out my --- you ----- like a bar\n",
      "0.392;0.5;62;60;900;@erotic_kitty;nope not my way take me as i am and all of me or not at all --\n",
      "0.393;0.429;63;60;900;@erotic_kitty;well im sorry but i cant feel bad for you with that problem lol\n",
      "0.395;0.4;94;60;900;@erotic_kitty;------- as ------------- ------- into her neck and smiles as she ------- back -------- -------\n",
      "0.396;0.4;63;60;900;@erotic_kitty;----- her ------- harder ----------- deeper into her with moans\n",
      "0.398;0.462;68;60;900;@erotic_kitty;------- his cock ------ as she ------- her ------- looking up at him\n",
      "0.399;0.5;136;60;900;@erotic_kitty;if im not -------------- ------- me for --------- on this ------------- but its open jenn are you -------- is that what youre getting at\n",
      "0.401;0.417;63;60;900;@erotic_kitty;----- ------------- no sit at the bar my -------- are back here\n",
      "0.402;0.462;64;60;900;@erotic_kitty;but no if we really get into it the ------ --------- are -------\n",
      "0.404;0.419;160;60;900;@erotic_kitty;throws his head back as that cock ------- his balls ---------- up as he ----- his ----- hot ---- into her mouth -------- her pink tongue with that hot white cum\n",
      "0.405;0.417;66;60;900;@erotic_kitty;there could be ------------ or something like that and it wasnt me\n",
      "0.407;0.412;82;60;900;@erotic_kitty;we just have these other ---- to please men that dont hurt like ---- hell at first\n",
      "0.408;0.462;64;60;900;DJ`Mercury;im good thanks just -------- just got home from work how are you\n",
      "0.41;0.462;63;60;900;DJ`Mercury;gets back on all ------ looking at him and licking up his shaft\n",
      "0.411;0.467;62;60;900;@erotic_kitty;hey i just said what i do when ----- do with it what ya ------\n",
      "0.413;0.438;85;60;900;@erotic_kitty;- well ---------- only have her word that her master ------------- her off to be used\n",
      "0.414;0.429;73;60;900;@erotic_kitty;-------- your hair around his fingers as he ---- your mouth with his cock\n",
      "0.416;0.5;71;60;900;@erotic_kitty;if biffyyy did any ------- ---- with what i have now she be on her butt\n",
      "0.417;0.615;66;60;900;@erotic_kitty;gracie she -------- be a ------- ------- but we have one like that\n",
      "0.419;0.5;62;60;900;^fran;was kicked by ---------------------- we do not ------- in here\n",
      "0.42;0.444;89;60;900;@erotic_kitty;i dont know how to do that but the first thing that came up was ------------ ---- -------\n",
      "0.422;0.429;73;60;900;@erotic_kitty;no ----- on ----- --------- so long as your cunt is --------- for -------\n",
      "0.423;0.462;67;60;900;@erotic_kitty;blushes -------- as she does his hand ------- her cheek as she does\n",
      "0.425;0.526;94;60;900;@erotic_kitty;------- her head on his -------- ------- his cock again but not if its going to come with that\n",
      "0.426;0.429;66;60;900;@saffron{WH};its not tea if u dont sugar it then its just ----- --------- water\n",
      "0.428;0.455;64;60;900;@erotic_kitty;im just --------- if that ---- ------- at ------- actually works\n",
      "0.429;0.471;75;60;900;@erotic_kitty;simian thank you for your words i dont know how i get here lol its all good\n",
      "0.431;0.462;63;60;900;@erotic_kitty;last year there --------------- was not as good as it use to be\n",
      "0.432;0.467;67;60;900;WhoGiveSaDamn;not sure it ------- here as many things are not what they use to be\n",
      "0.434;0.5;64;60;900;@erotic_kitty;winks at --------------- as her body shakes as -- works her good\n",
      "0.435;0.467;67;60;900;@erotic_kitty;grins at her his hand on her --- for a moment before she moves away\n",
      "0.437;0.462;83;60;900;@erotic_kitty;smiles at ------------------ as she hugs ---------- with one ---- holding her close\n",
      "0.438;0.5;90;60;900;@erotic_kitty;if any ladies are on im ready for just about any roleplay idea dont be ------ to hit me up\n",
      "0.44;0.455;104;60;900;@erotic_kitty;---- her --- down to her thighs as she curls up ------- her ------- from her face as she ------ her eyes\n",
      "0.441;0.5;66;60;900;PlayfulBBC;what is that face for - looks like your ----- are -------- out ---\n",
      "0.443;0.5;83;60;900;@erotic_kitty;but im not ----------- we always went --- here cause of how much was ---------- out\n",
      "0.444;0.467;64;60;900;Valkyrie;- in here we talk to all not just --- or straight female or male\n",
      "0.446;0.455;101;60;900;@erotic_kitty;----- sucking his ball so that she can lick his --- as her hand keeps ------- up and down on his cock\n",
      "0.447;0.467;72;60;900;@erotic_kitty;see you all are nice ------ no wonder they dont let me be in -----------\n",
      "0.449;0.462;75;60;900;@erotic_kitty;its because your ----- anastatia folks dont ---- with people they dont like\n",
      "0.45;0.455;63;60;900;@erotic_kitty;or ------------------------- but i have to be ---- on the block\n",
      "0.452;0.455;62;60;900;@erotic_kitty;if she starts taking off her ------- that will be ------------\n",
      "0.453;0.471;90;60;900;@erotic_kitty;laughs at ----------- no need to roll your eyes we are all just making -------- ----- here\n",
      "0.455;0.5;63;60;900;Cruel`Intentions;------- her head as much as she can as her ass is --------- out\n",
      "0.456;0.643;62;60;900;@erotic_kitty;maybe its two but its one of those that look like its just one\n",
      "0.458;0.5;69;60;900;@erotic_kitty;i have heard her ------------------- almost as good as you are ------\n",
      "0.459;0.471;80;60;900;@erotic_kitty;--------- all this ------ has to ---- out you all be good dont do anything id do\n",
      "0.461;0.5;88;60;900;WhoGiveSaDamn;- so ----------------------- what was your ---------------- like any -------------------\n",
      "0.462;0.533;75;60;900;@erotic_kitty;curls up with her master his arms ------- around her as she ------ her eyes\n",
      "0.464;0.5;74;60;900;@erotic_kitty;truth if there was no such thing as money what would you do with your life\n",
      "0.465;0.467;81;60;900;@erotic_kitty;well -------------- do here what you did there so you dont get ------ here either\n",
      "0.467;0.5;64;60;900;@erotic_kitty;hey ------------- hey -------- hey ------- hey ------- hey -----\n",
      "0.468;0.5;75;60;900;@erotic_kitty;if all the ----- are more -------- that we dont then one would ------------\n",
      "0.47;0.471;67;60;900;@erotic_kitty;i dont have a ----- if i had one its not me thats gonna be in there\n",
      "0.471;0.733;69;60;900;@erotic_kitty;well one that was easy or --------- but if you dont have one its cool\n",
      "0.473;0.5;63;60;900;@erotic_kitty;well im around ------------ not that it ------- all that much -\n",
      "0.474;0.5;66;60;900;@erotic_kitty;words are as fun as they are ------- or some ------ shit like that\n",
      "0.476;0.529;82;60;900;@erotic_kitty;------- his cock -------- with her face as she ---- at his ----- looking up at him\n",
      "0.477;0.5;61;60;900;@erotic_kitty;-------------------- i dont have that on my -----------------\n",
      "0.479;0.529;80;60;900;@erotic_kitty;we were talking about how she cant see what she is or what she really looks like\n",
      "0.48;0.5;61;60;900;@erotic_kitty;how do people think with a name like battlecat that im female\n",
      "0.482;0.538;63;60;900;Sanger;------ not ------- for that much here and your just ----- of me\n",
      "0.483;0.533;77;60;900;@erotic_kitty;description for annora im not interested its not you its me im an ----- bitch\n",
      "0.485;0.538;66;60;900;@erotic_kitty;yeah sorry no ------- but other ----------- just dont do it for me\n",
      "0.486;0.5;71;60;900;@erotic_kitty;okay im not feeling so well today ----- its the no coffee im ----------\n",
      "0.488;0.5;69;60;900;DJ`Mercury;im not one for -------- just ------ and then only if its ------ right\n",
      "0.489;0.5;68;60;900;@erotic_kitty;or at least you should have used to be seeing as how its --- in here\n",
      "0.491;0.5;62;60;900;@erotic_kitty;i was ------- waiting for your show djawesome but im just ----\n",
      "0.492;0.545;61;60;900;@erotic_kitty;she was so ------------ when she -------- it was just -------\n",
      "0.494;0.526;97;60;900;@erotic_kitty;------- her eyes on his ---------- again at his request sucking so hard on his tip as she -------\n",
      "###########################################################################################################################\n",
      "second part\n",
      "###########################################################################################################################\n",
      "0.495;0.538;61;60;70;@erotic_kitty;these guys have a good ----- but its just not that guys -----\n",
      "0.497;0.556;61;59;69;@erotic_kitty;-------------------- good ------- as we should all be -------\n",
      "0.498;0.5;59;58;68;@erotic_kitty;but i didnt read into how they came up with that ----------\n",
      "0.5;0.5;60;57;67;Fenderman1964;well yeah she said she was ------ out of --------- didnt she\n",
      "0.501;0.545;62;56;66;@erotic_kitty;in -------- --------- we have ---------- as well as ----- here\n",
      "0.503;0.6;58;55;65;@erotic_kitty;not at all ------------- was just -------- something out p\n",
      "0.504;0.538;55;54;64;@erotic_kitty;no like i dont dog people if they dont like to read but\n",
      "0.505;0.545;57;53;63;@erotic_kitty;well well ---- was --------- were huggss and all that was\n",
      "0.507;0.556;56;52;62;@erotic_kitty;------------------ dont be pushing your ------ on me ---\n",
      "0.508;0.545;57;51;61;@erotic_kitty;but its not doing that now because its -------- your ----\n",
      "0.51;0.643;56;50;60;@erotic_kitty;i ----- she would like for me to do that im not her type\n",
      "0.511;0.545;52;49;59;@erotic_kitty;well your wrong nick so you just ------ we are right\n",
      "0.513;0.538;54;48;58;@erotic_kitty;some come on guys are we gonna fuck sara or are we not\n",
      "0.514;0.545;50;47;57;@erotic_kitty;so i do wonder what ------ are fucking up with now\n",
      "0.516;0.615;47;46;56;Anastatia;she need us so we did what we had to do for her\n",
      "0.517;0.556;54;45;55;@erotic_kitty;im sure its just a ---------------- with your --------\n",
      "###########################################################################################################################\n",
      "third part\n",
      "###########################################################################################################################\n",
      "0.519;0.667;35;30;40;Fenderman1964;marriedvixen up what are your -----\n",
      "0.52;0.571;33;29.5;39.5;@erotic_kitty;check out the ------- on that one\n",
      "0.522;0.667;34;29.0;39.0;@erotic_kitty;its not just for --------- anymore\n",
      "0.523;0.571;33;28.5;38.5;@erotic_kitty;im here to do the ---------- here\n",
      "0.525;0.625;35;28.0;38.0;@erotic_kitty;and your not even very good at that\n",
      "0.526;0.571;35;27.5;37.5;@erotic_kitty;im ------ here but have been around\n",
      "0.528;0.571;30;27.0;37.0;@erotic_kitty;id send her back if she doesnt\n",
      "0.529;0.7;36;26.5;36.5;@erotic_kitty;im not sure if i was ---- or she was\n",
      "0.531;0.714;32;26.0;36.0;@erotic_kitty;hey hey --------- so far so good\n",
      "0.532;0.714;27;25.5;35.5;@erotic_kitty;but not just to ask her out\n",
      "0.534;0.6;28;25.0;35.0;@erotic_kitty;hey hey hey sarahgood ------\n",
      "0.535;0.6;26;24.5;34.5;@erotic_kitty;pushes back into her mouth\n",
      "0.537;0.571;26;24.0;34.0;@erotic_kitty;i dont do well im the ----\n",
      "0.538;0.6;31;23.5;33.5;@erotic_kitty;im sorry that was -------------\n",
      "0.54;0.6;29;23.0;33.0;@erotic_kitty;was just thinking that though\n",
      "0.541;0.6;27;22.5;32.5;@erotic_kitty;she should be back mindseye\n",
      "0.543;0.571;24;22.0;32.0;@erotic_kitty;its as good as any - lol\n",
      "0.544;0.6;31;21.5;31.5;@erotic_kitty;im like that -------------- lol\n",
      "0.546;0.6;25;21.0;31.0;@erotic_kitty;at least its not --------\n",
      "0.547;0.75;28;20.5;30.5;@erotic_kitty;what no dj what are we to do\n",
      "0.549;0.6;24;20.0;30.0;@erotic_kitty;pretty good here for now\n",
      "0.55;0.8;21;19.5;29.5;@erotic_kitty;dont all ----- out at\n",
      "0.552;0.6;27;19.0;29.0;@erotic_kitty;no no just --------- ------\n",
      "0.553;0.8;25;18.5;28.5;@erotic_kitty;dont -------- its just me\n",
      "0.555;0.75;22;18.0;28.0;@erotic_kitty;well that was --------\n",
      "0.556;0.75;23;17.5;27.5;@erotic_kitty;what are your ---------\n",
      "0.558;0.75;18;17.0;27.0;@erotic_kitty;but its not really\n",
      "0.559;0.8;17;16.5;26.5;@erotic_kitty;so be nice to her\n",
      "0.561;0.6;22;16.0;26.0;Fenderman1964;good ty just ------ up\n",
      "0.562;0.667;21;15.5;25.5;@erotic_kitty;------------ nice one\n",
      "0.564;0.75;20;15.0;25.0;@erotic_kitty;dont we all --------\n",
      "0.565;0.6;18;14.5;24.5;@erotic_kitty;not my wall at all\n",
      "0.567;0.75;15;14.0;24.0;Cruel`Intentions;or hey no -----\n",
      "0.568;0.6;20;13.5;23.5;@erotic_kitty;im great how are you\n",
      "0.57;0.6;18;13.0;23.0;@erotic_kitty;hey goo how are ya\n",
      "0.571;0.667;13;12.5;22.5;@erotic_kitty;im so -------\n",
      "0.573;0.667;14;12.0;22.0;@erotic_kitty;im good thanks\n",
      "0.574;0.667;16;11.5;21.5;@erotic_kitty;that your ------\n",
      "0.576;0.667;17;11.0;21.0;Louise;hello good people\n",
      "0.577;0.667;13;10.5;20.5;@erotic_kitty;well hello --\n",
      "0.579;0.667;18;10.0;20.0;@erotic_kitty;for what fenderman\n",
      "0.58;0.667;13;9.5;19.5;@erotic_kitty;no just naked\n",
      "0.582;0.75;16;9.0;19.0;Fenderman1964;she is like that\n",
      "0.583;0.75;14;8.5;18.5;@erotic_kitty;but a good one\n",
      "0.585;0.667;11;8.0;18.0;@erotic_kitty;that you do\n",
      "0.586;0.667;15;7.5;17.5;@erotic_kitty;little but nice\n",
      "0.588;0.667;10;7.0;17.0;@erotic_kitty;no not ---\n",
      "0.589;0.667;12;6.5;16.5;@erotic_kitty;its me again\n",
      "0.591;0.667;10;6.0;16.0;@erotic_kitty;ok im back\n",
      "0.592;0.667;11;5.5;15.5;@erotic_kitty;they do not\n",
      "0.594;0.667;10;5.0;15.0;Sanger;or suck we\n",
      "0.595;0.667;13;4.5;14.5;sweet_teresa;busy but good\n",
      "0.597;0.75;12;4.0;14.0;Valkyrie;i do as well\n",
      "0.598;0.75;13;3.5;13.5;Threeleggedcat;not at all ms\n",
      "0.6;0.667;10;3.0;13.0;@erotic_kitty;hey all xx\n",
      "0.601;0.667;12;2.5;12.5;@erotic_kitty;not bad here\n",
      "0.603;0.667;9;2.0;12.0;@erotic_kitty;how are u\n",
      "0.604;0.667;11;1.5;11.5;Threeleggedcat;is all good\n",
      "0.606;0.667;9;1.0;11.0;@erotic_kitty;no it not\n",
      "0.607;1.0;3;0.5;10.5;@erotic_kitty;her\n",
      "0.609;1.0;7;0.0;10.0;@erotic_kitty;im well\n",
      "0.61;1.0;8;0;9.5;@erotic_kitty;me me me\n",
      "0.612;1.0;8;0;9.0;@erotic_kitty;all that\n",
      "0.613;1.0;7;0;8.5;@erotic_kitty;do that\n",
      "0.615;1.0;7;0;8.0;@erotic_kitty;was not\n",
      "0.616;1.0;5;0;7.5;@erotic_kitty;no no\n",
      "0.618;1.0;6;0;7.0;@erotic_kitty;its me\n",
      "0.619;1.0;3;0;6.5;@erotic_kitty;its\n",
      "0.621;1.0;2;0;6.0;@erotic_kitty;be\n",
      "0.622;1.0;5;0;5.5;guynextdoor;as if\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "# structure TWOII2\n",
    "# uses top 0.025 dashed out sentences \n",
    "# part 1: 130 lines: gradient from percentage of most common words from 0.3 to 0.6 and minimum sentence length of 60\n",
    "# part 2: 30 lines: percentage increased linearly and reducing word length baries towards very short sentences\n",
    "# part 3: 70 lines: slowly decline of sentence lengths barriers\n",
    "# uses original authors from big dashed out sentence database\n",
    "\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "\n",
    "def checkline(line):\n",
    "    line = line.split(':')\n",
    "    if len(line) < 2:\n",
    "        return False\n",
    "    line = line[1]\n",
    "    if len(line) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "input_file = open('1606_only_top0.025_all_authordistfix.txt', mode='r', encoding='utf8').readlines()\n",
    "\n",
    "add_prameters = True\n",
    "finlines = []\n",
    "finlines_print = []\n",
    "perc = 0.3\n",
    "perc_increase = 0.0015\n",
    "linelength_min = 60\n",
    "linelength_max = 900\n",
    "linecount_first = 130\n",
    "linecount_second = 30\n",
    "linecount_third = 70\n",
    "\n",
    "authors = {}\n",
    "authors['@erotic_kitty'] = [''] * 100\n",
    "authors['carrol'] = [''] * 1\n",
    "authors['DJ`Mercury'] = [''] * 1\n",
    "authors['Louise'] = [''] * 1\n",
    "authors['Valkyrie'] = [''] * 1\n",
    "authors['Threeleggedcat'] = [''] * 1\n",
    "authors['PlayfulBBC'] = [''] * 1\n",
    "authors['@saffron{WH}'] = [''] * 1\n",
    "authors['WhoGiveSaDamn'] = [''] * 1\n",
    "authors['Silver_haired`Fox'] = [''] * 1\n",
    "authors['Sanger'] = [''] * 1\n",
    "authors['sweet_teresa'] = [''] * 1\n",
    "authors['Jothom'] = [''] * 1\n",
    "authors['Fenderman1964'] = [''] * 1\n",
    "authors['Woman'] = [''] * 1\n",
    "authors['Cruel`Intentions'] = [''] * 1\n",
    "authors['guynextdoor'] = [''] * 1\n",
    "authors['Anastatia'] = [''] * 1\n",
    "authors['^fran'] = [''] * 1\n",
    "authors['Jaems'] = [''] * 1\n",
    "\n",
    "authors_probabilities = get_by_probability([authors], 230, 20)\n",
    "#print(authors_probabilities)\n",
    "author_index = 0\n",
    "\n",
    "for i in range(linecount_first):\n",
    "    while True:\n",
    "        line = random.choice(input_file)\n",
    "        if not checkline(line):\n",
    "            line = random.choice(input_file)\n",
    "            continue   \n",
    "        auth = line.split(':')[0]\n",
    "        line = line.split(':')[1].strip()\n",
    "\n",
    "        counter = 0\n",
    "        wordcount = len(line.split())\n",
    "\n",
    "        if wordcount == 0:\n",
    "            continue\n",
    "\n",
    "        linelen = len(line)\n",
    "        for word in line.split():\n",
    "            for p in possible:\n",
    "                if word == p:\n",
    "                    counter += 1\n",
    "        currentperc = counter / float(wordcount)\n",
    "        if currentperc > perc and linelen > linelength_min and linelen < linelength_max and line not in finlines:\n",
    "            finlines.append(line.rstrip())\n",
    "            a = authors_probabilities[author_index]\n",
    "            author_index += 1\n",
    "            if add_prameters:\n",
    "                print(str(round(perc, 3)) + ';' + str(round(currentperc, 3)) + ';' + str(linelen) + ';' + str(linelength_min) + ';' + str(linelength_max) + ';' + a + ';' + line.rstrip())\n",
    "                finlines_print.append(str(round(perc, 3)) + ';' + str(round(currentperc, 3)) + ';' + str(linelen) + ';' + str(linelength_min) + ';' + str(linelength_max) + ';' + a + ';' + line + '\\n')\n",
    "            else:\n",
    "                print(a + ': ' + line.rstrip())\n",
    "                finlines_print.append(a + ': ' + line)\n",
    "            perc += perc_increase\n",
    "            break\n",
    "\n",
    "print('###########################################################################################################################')\n",
    "print('second part')\n",
    "print('###########################################################################################################################')\n",
    "\n",
    "linelength_min = 60\n",
    "linelength_max = 70\n",
    "linelength_decrease = 1\n",
    "for i in range(linecount_second):\n",
    "    tried = 0\n",
    "    while True and tried < 10000:\n",
    "        tried += 1\n",
    "        line = random.choice(input_file)\n",
    "        if not checkline(line):\n",
    "            line = random.choice(input_file)\n",
    "            continue   \n",
    "        auth = line.split(':')[0]\n",
    "        line = line.split(':')[1].strip()\n",
    "            \n",
    "        counter = 0\n",
    "        wordcount = len(line.split())\n",
    "        \n",
    "        if wordcount == 0:\n",
    "            continue\n",
    "        \n",
    "        linelen = len(line)\n",
    "        for word in line.split():\n",
    "            for p in possible:\n",
    "                if word == p:\n",
    "                    counter += 1\n",
    "        currentperc = counter / float(wordcount)\n",
    "        if currentperc > perc and linelen > linelength_min and linelen < linelength_max and line not in finlines: \n",
    "            finlines.append(line.rstrip())\n",
    "            a = authors_probabilities[author_index]\n",
    "            author_index += 1\n",
    "            if add_prameters:\n",
    "                print(str(round(perc, 3)) + ';' + str(round(currentperc, 3)) + ';' + str(linelen) + ';' + str(linelength_min) + ';' + str(linelength_max) + ';' + a + ';' + line.rstrip())\n",
    "                finlines_print.append(str(round(perc, 3)) + ';' + str(round(currentperc, 3)) + ';' + str(linelen) + ';' + str(linelength_min) + ';' + str(linelength_max) + ';' + a + ';' + line + '\\n')\n",
    "            else:\n",
    "                print(a + ': ' + line.rstrip())\n",
    "                finlines_print.append(a + ': ' + line)\n",
    "            perc += perc_increase\n",
    "            linelength_min -= linelength_decrease\n",
    "            linelength_min = max(linelength_min, 0)\n",
    "            linelength_max -= linelength_decrease\n",
    "            linelength_max = max(linelength_max, 4)\n",
    "            break\n",
    "            \n",
    "\n",
    "print('###########################################################################################################################')\n",
    "print('third part')\n",
    "print('###########################################################################################################################')\n",
    "\n",
    "linelength_min = 30\n",
    "linelength_max = 40\n",
    "linelength_decrease = 0.5\n",
    "for i in range(linecount_third):\n",
    "    tried = 0\n",
    "    while True and tried < 10000:\n",
    "        tried += 1\n",
    "        line = random.choice(input_file)\n",
    "        if not checkline(line):\n",
    "            line = random.choice(input_file)\n",
    "            continue   \n",
    "        auth = line.split(':')[0]\n",
    "        line = line.split(':')[1].strip()\n",
    "            \n",
    "        counter = 0\n",
    "        wordcount = len(line.split())\n",
    "        \n",
    "        if wordcount == 0:\n",
    "            continue\n",
    "        \n",
    "        linelen = len(line)\n",
    "        for word in line.split():\n",
    "            for p in possible:\n",
    "                if word == p:\n",
    "                    counter += 1\n",
    "        currentperc = counter / float(wordcount)\n",
    "        if currentperc > perc and linelen > linelength_min and linelen < linelength_max and line not in finlines: \n",
    "            finlines.append(line.rstrip())\n",
    "            a = authors_probabilities[author_index]\n",
    "            author_index += 1\n",
    "            if add_prameters:\n",
    "                print(str(round(perc, 3)) + ';' + str(round(currentperc, 3)) + ';' + str(linelen) + ';' + str(linelength_min) + ';' + str(linelength_max) + ';' + a + ';' + line.rstrip())\n",
    "                finlines_print.append(str(round(perc, 3)) + ';' + str(round(currentperc, 3)) + ';' + str(linelen) + ';' + str(linelength_min) + ';' + str(linelength_max) + ';' + a + ';' + line + '\\n')\n",
    "            else:\n",
    "                print(a + ': ' + line.rstrip())\n",
    "                finlines_print.append(a + ': ' + line)\n",
    "            perc += perc_increase\n",
    "            linelength_min -= linelength_decrease\n",
    "            linelength_min = max(linelength_min, 0)\n",
    "            linelength_max -= linelength_decrease\n",
    "            linelength_max = max(linelength_max, 4)\n",
    "            break\n",
    "            \n",
    "print(len(finlines))\n",
    "\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H%M%S')\n",
    "\n",
    "outfile = open('1606_structure_02_01_'+st+'.txt', 'w')\n",
    "outfile.write('ACT 3 - STRUCTURE 2\\n')\n",
    "outfile.write('currentperc;perc;linelength;linelength_min;linelength_max;author;text\\n')\n",
    "for line in finlines_print:\n",
    "    outfile.write(line)\n",
    "outfile.close()\n",
    "\n",
    "# legend\n",
    "# perc = minimum percent of top20 words in the sentence\n",
    "# currentperc = current percent of top20 words in the sentence\n",
    "# length = character length of the sentence\n",
    "# linelength_min = minimum char length \n",
    "# linelength_max = maximum char length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "111\n",
      "125\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# count i/s in 410/501\n",
    "leaves410 = 0\n",
    "entrances410 = 0\n",
    "leaves501 = 0\n",
    "entrances501 = 0\n",
    "\n",
    "for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "    if 'has left' in line or 'has quit' in line:\n",
    "        leaves410 += 1\n",
    "    if 'joined the channel' in line:\n",
    "        entrances410 += 1\n",
    "    #if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "        #sentence = line[line.find(\":\"):]\n",
    "        #all_posts.append((author, sentence.strip().lower()))\n",
    "        #all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "        #author = line[:line.find(\":\")]\n",
    "        #if author not in authors: # save things\n",
    "            #authors[author] = set() # init list for author\n",
    "        #if author not in authors_410:\n",
    "            #authors_410[author] = set()\n",
    "        #authors[author].add(sentence.strip().lower())\n",
    "        #authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "    if 'has left' in s or 'has quit' in s:\n",
    "        leaves501 += 1\n",
    "    if 'joined the channel' in s:\n",
    "        entrances501 += 1\n",
    "    #if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "        #if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "            #sentence = s[s.find(\">\")+2:]\n",
    "            #all_posts.append((author, sentence.strip().lower()))\n",
    "            #all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "            #author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "            #if author not in authors: # save things\n",
    "            #    authors[author] = set() # init list for author\n",
    "            #if author not in authors_501:\n",
    "            #    authors_501[author] = set()\n",
    "            #authors[author].add(sentence.strip().lower())\n",
    "            #authors_501[author].add(sentence.strip().lower()) \n",
    "            \n",
    "            \n",
    "         \n",
    "print(leaves410)\n",
    "print(entrances410)\n",
    "print(leaves501)\n",
    "print(entrances501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "\n",
    "def get_random_syn(word):\n",
    "    r = word\n",
    "    syns = wn.synsets(word)\n",
    "    lemmas = []\n",
    "    for ss in syns:\n",
    "        for lem in ss.lemma_names():\n",
    "            lemmas.append(lem)\n",
    "            #r = lem\n",
    "            #if lem is not word:\n",
    "            #    return lem\n",
    "    return lemmas\n",
    "\n",
    "def replace_sentence(sentence):\n",
    "    return_sentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        lemmas = get_random_syn(word)\n",
    "        if len(lemmas) > 0:\n",
    "            #print(word + str(lemmas))\n",
    "            r = random.choice(lemmas)\n",
    "            if r is \"atomic_number_53\":\n",
    "                r = 'I'\n",
    "            return_sentence += r + \" \"\n",
    "        else:\n",
    "            return_sentence += word + ' '\n",
    "    return return_sentence\n",
    "\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    sen = random.choice(all_posts)\n",
    "    if len(sen[1]) > 10:\n",
    "        print('-------------------------')\n",
    "        print(sen[1])\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURVES generation\n",
    "\n",
    "random.seed(1)\n",
    "#render_plain('1306_removed_digits_entrances_01_ORIGINAL.txt', 500, 3860)\n",
    "#render_by_osc('osc_config_500_late_slowexp.csv', '1306_cleaner_group1&2&3_01.txt', 3860)\n",
    "print('finito')\n",
    "#generate_lines_linear(20)\n",
    "\n",
    "generate_lines_from(0.00, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.10, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.20, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.50, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.80, 0.0001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving word groups\n",
    "lists = []\n",
    "#lists.append((1, 5))\n",
    "#lists.append((6, 20))\n",
    "#lists.append((21, 50))\n",
    "#lists.append((51, 100))\n",
    "#lists.append((101, 500))\n",
    "lists.append((3000, 10000))\n",
    "\n",
    "outfile = codecs.open('3000_10000_groups.txt', 'w')\n",
    "\n",
    "for l in lists:\n",
    "    possible_words = []\n",
    "    #_config = config[index]\n",
    "    _min = l[0]\n",
    "    _max = l[1]\n",
    "    app_list = []\n",
    "    for i in range(_min, _max+1):\n",
    "        app_list.append(i)\n",
    "\n",
    "    # all allowed words\n",
    "    selected_appearances = get_by_appearance_list(app_list)\n",
    "    outfile.write(' '.join(selected_appearances))\n",
    "    #possible_words.append((selected_appearances, _config))\n",
    "    \n",
    "outfile.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess act1&2\n",
    "\n",
    "for sentence in all_posts_501:\n",
    "    print(sentence[0] + \" -> \" + remove_non_frequent(get_topn_percent(0.01), sentence[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
