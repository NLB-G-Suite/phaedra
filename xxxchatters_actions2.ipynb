{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479404\n",
      "158959\n",
      "29086\n",
      "40\n",
      "73\n",
      "304\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import codecs\n",
    "from utils import in_blocklist\n",
    "\n",
    "blocklist = []\n",
    "blocklist.append('AWAYLEN=307 MAXTARGETS=20 WALLCHOPS WATCH=128 WATCHOPTS=A SILENCE=15 MODES=12 CHANTYPES=# PREFIX')\n",
    "blocklist.append('- * -')\n",
    "blocklist.append('XxXChatters.Com')\n",
    "blocklist.append('XxXChatters')\n",
    "blocklist.append('This server was created')\n",
    "blocklist.append('operator(s) online')\n",
    "blocklist.append('is now your displayed host')\n",
    "blocklist.append('Caps set:')\n",
    "blocklist.append('MAXCHANNELS')\n",
    "blocklist.append('http')\n",
    "blocklist.append('type it in the main room or a pm to the')\n",
    "blocklist.append('The topic is')\n",
    "blocklist.append('Topic set by')\n",
    "\n",
    "all_separated = []\n",
    "all_posts = []\n",
    "all_posts_410 = []\n",
    "all_posts_501 = []\n",
    "authors = {}\n",
    "authors_410 = {}\n",
    "authors_501 = {}\n",
    "\n",
    "def parse_all(files, separate):\n",
    "    for file in glob.glob(files):\n",
    "        lines = codecs.open(file, mode='r', encoding='utf-8')\n",
    "\n",
    "        for line in lines:\n",
    "            if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "                if line[0] is '[' and line.find(\"<\") is not -1:\n",
    "                    s = line\n",
    "                    sentence = s[s.find(\">\")+2:]\n",
    "                    time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "                    author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                    \n",
    "                    p = (time+' ' + author, sentence.strip().lower())\n",
    "                    all_posts.append(p)\n",
    "                    if separate in file:\n",
    "                        all_separated.append(p)\n",
    "\n",
    "                    if author not in authors: # save things\n",
    "                        authors[author] = set() # init list for author\n",
    "                    authors[author].add(sentence.strip().lower()) \n",
    "                else:\n",
    "                    if line.find('*') is not -1 and \") has joined #\" not in line and \") Quit (\" not in line and \") has left #\" not in line and \"6A\u000314thena\u000314, \u000f\u000306y\u000314o\u000314u \u000f\u000306n\u000314eve\u000314r\" not in line and \" sets mode: +\" not in line and \" is now known as \" not in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        sentence = line[aa_end + 1:]\n",
    "                        \n",
    "                        p = (time+' ' + author, sentence.strip().lower())\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "\n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence.strip().lower()) \n",
    "                    if \") has joined #\" in line or \") Quit (\" in line or \") has left #\" in line:\n",
    "                        time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                        author_beginning = line.find(\"* \")\n",
    "                        aa = line[author_beginning+2:]\n",
    "                        aa_end = author_beginning + 2 + aa.find(\" \")\n",
    "                        author = line[author_beginning+2:aa_end]\n",
    "                        if \") has joined #\" in line:\n",
    "                            author += \" JOINED THE ROOM\"\n",
    "                        else:\n",
    "                            author += \" LEFT THE ROOM\"\n",
    "                        sentence = \"\"\n",
    "                        p = (time+' ' + author, sentence)\n",
    "                        all_posts.append(p)\n",
    "                        if separate in file:\n",
    "                            all_separated.append(p)\n",
    "                            \n",
    "                        if author not in authors: # save things\n",
    "                            authors[author] = set() # init list for author\n",
    "                        authors[author].add(sentence) \n",
    "\n",
    "    for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "        if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "            sentence = line[line.find(\":\"):]\n",
    "            all_posts.append((author, sentence.strip().lower()))\n",
    "            all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "            author = line[:line.find(\":\")]\n",
    "            if author not in authors: # save things\n",
    "                authors[author] = set() # init list for author\n",
    "            if author not in authors_410:\n",
    "                authors_410[author] = set()\n",
    "            authors[author].add(sentence.strip().lower())\n",
    "            authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "    for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "        if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "            if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "                sentence = s[s.find(\">\")+2:]\n",
    "                all_posts.append((author, sentence.strip().lower()))\n",
    "                all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "                author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                if author not in authors: # save things\n",
    "                    authors[author] = set() # init list for author\n",
    "                if author not in authors_501:\n",
    "                    authors_501[author] = set()\n",
    "                authors[author].add(sentence.strip().lower())\n",
    "                authors_501[author].add(sentence.strip().lower()) \n",
    "        \n",
    "parse_all(\"xxxchatters_logs/*.log\", \"#chat\")\n",
    "#for file in glob.glob(\"xxxchatters_logs/*.log\"):\n",
    "print(len(all_posts))\n",
    "print(len(all_separated))\n",
    "print(len(authors))\n",
    "print(len(authors_410))\n",
    "print(len(authors_501))\n",
    "print(len(all_posts_410))\n",
    "print(len(all_posts_501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in authors['carrol']:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49231\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import string\n",
    "word_counts = dict()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess_word(word):\n",
    "    word = word.replace('\\x02', '')\n",
    "    word = word.replace('\\x03', '')\n",
    "    word = word.replace('.', '')\n",
    "    word = ''.join([i for i in word if not i.isdigit()])\n",
    "\n",
    "    #word = word.translate(translator)\n",
    "    word = ''.join(e for e in word if e.isalnum())\n",
    "    return word\n",
    "\n",
    "\n",
    "for post in all_posts:\n",
    "    word_list = post[1].split()\n",
    "    for word in word_list:\n",
    "        word = preprocess_word(word)\n",
    "        #word = word.replace('\\x03', '')\n",
    "        #word = word.replace('.', '')\n",
    "\n",
    "        #word = word.translate(translator)\n",
    "        #word = ''.join(e for e in word if e.isalnum())\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "def get_sorted_word_counts():\n",
    "    return sorted_word_counts\n",
    "\n",
    "v = get_sorted_word_counts()\n",
    "print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extra', 'arm', 'large', 'past', 'quickly', 'irc', 'human', 'rl', 'size', 'wine', 'leaves', 'firm', 'able', 'summer', 'dylan', 'devil', 'naderaax', 'kinky', 'jag']\n"
     ]
    }
   ],
   "source": [
    "def get_topn_percent(perc):\n",
    "    counts = get_sorted_word_counts()\n",
    "    l = len(counts)\n",
    "    max_index = int(perc*l)\n",
    "    top_n = []\n",
    "    #print(counts[:max_index])\n",
    "    for t in counts[:max_index]:\n",
    "        top_n.append(t[0])\n",
    "    return top_n\n",
    "\n",
    "def get_all_by_max_appearance(app):\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] >= app:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "\n",
    "def get_by_appearance_list(appearance):\n",
    "    # appearance = [2, 3, 4] # number indicating times of appearance\n",
    "    counts = get_sorted_word_counts()\n",
    "    processed_counts = []\n",
    "    for c in counts:\n",
    "        if c[1] in appearance:\n",
    "            processed_counts.append(c[0])\n",
    "        \n",
    "    return processed_counts\n",
    "    \n",
    "#topn = get_topn_percent(0.001)\n",
    "\n",
    "selected_appearances = get_by_appearance_list([110, 112])\n",
    "print(selected_appearances)\n",
    "\n",
    "#maxapp = get_all_by_max_appearance(2)\n",
    "#file = open(\"1106/chat_room_word_appearance.txt\", 'w')\n",
    "#for e in maxapp:\n",
    "#    try:\n",
    "#        file.write(e[0] + \", \" + str(e[1]) + '\\n')\n",
    "#    except:\n",
    "#        pass\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- to keep being evil ---- athena on the -------- with a new nerf bat\n",
      "--- to keep being evil<<<< ---- athena on the -------- with a new nerf bat\n"
     ]
    }
   ],
   "source": [
    "def remove_non_frequent(allowed_words, post, replace=True):\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        pword = preprocess_word(word)\n",
    "        if pword in allowed_words:\n",
    "            good_words.append(pword)\n",
    "        else:\n",
    "            if replace:\n",
    "                replacement = \"\"\n",
    "                for char in word:\n",
    "                    replacement += \"-\"\n",
    "                good_words.append(replacement)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "\n",
    "def remove_by_probability(allowed_words, always_allowed, post, chance, replace=True):\n",
    "    import random\n",
    "    good_words = []\n",
    "    for word in post.lower().split():\n",
    "        #pword = preprocess_word(word)\n",
    "        if word in always_allowed:\n",
    "            good_words.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in allowed_words:\n",
    "            good_words.append(word)\n",
    "        else:\n",
    "            if replace:\n",
    "                rv = random.uniform(0.0, 1.0)\n",
    "                if rv < chance:\n",
    "                    replacement = \"\"\n",
    "                    for char in word:\n",
    "                        replacement += \"-\"\n",
    "                    good_words.append(replacement)\n",
    "                else:\n",
    "                    good_words.append(word)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "sentence_to_test = \"Jus to keep being evil<<<< wops Athena on the backside with a new nerf bat\"\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n",
    "\n",
    "print(remove_by_probability(get_topn_percent(0.1), get_topn_percent(0.01), sentence_to_test, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ------- on the -------- with one of my new ---- ----\n",
      "bops saffron on the -------- with one of my new nerf bats\n"
     ]
    }
   ],
   "source": [
    "sentence_to_test = \"bops saffron on the backside with one of my new nerf bats\"\n",
    "print(remove_non_frequent(get_topn_percent(0.01), sentence_to_test))\n",
    "print(remove_non_frequent(get_topn_percent(0.1), sentence_to_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@erotic_kitty', 65), ('carrol', 53), ('+DJ`Mercury', 36), ('Louise', 28), ('@erotic_kitty{S}', 27), ('Valkyrie', 21), ('Threeleggedcat', 20), ('PlayfulBBC', 18), ('@saffron{WH}', 17), ('WhoGiveSaDamn', 15), ('Silver_haired`Fox', 14), ('sweet_teresa', 10), ('Woman', 8), ('Jothom', 8), ('Cruel`Intentions', 7), ('guynextdoor', 7), ('Anastatia', 7), ('^fran', 7), ('Sanger', 7), ('Jaems', 6), ('+DJ`South', 6), ('JFetish', 5), ('FunkyBoogieKing', 5), ('gracie', 4), ('@DJ`liltech', 4), ('Handyman', 4), ('MeanMark1', 4), ('Athena', 3), ('saffron{WH}', 3), ('TricksyM', 3)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_by_probability(authors_list, count, topn = 30):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for authors in authors_list:\n",
    "        for key, value in authors.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "                \n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #print(author_probability_counts[:topn])\n",
    "\n",
    "    sum_sentences = 0\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        sum_sentences += v[1]\n",
    "\n",
    "    author_probability_final = {}\n",
    "    author_list = []\n",
    "    prob_list = []\n",
    "\n",
    "    for v in author_probability_counts[:topn]:\n",
    "        author_probability_final[v[0]] = v[1]/float(sum_sentences)\n",
    "        author_list.append(v[0])\n",
    "        prob_list.append(v[1]/float(sum_sentences))\n",
    "\n",
    "    final_autor_list = np.random.choice(\n",
    "      author_list, \n",
    "      count,\n",
    "      p=prob_list\n",
    "    )\n",
    "\n",
    "    return final_autor_list\n",
    "\n",
    "def get_topn_authors(author_lists, n):\n",
    "    author_probability = {}\n",
    "    \n",
    "    for author_list in author_lists:\n",
    "        for key, value in author_list.items():\n",
    "            if key not in author_probability:\n",
    "                author_probability[key] = len(value)\n",
    "            else:\n",
    "                author_probability[key] += len(value)\n",
    "\n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return author_probability_counts[:n]\n",
    "\n",
    "#print(get_by_probability(authors, 20))\n",
    "#print(get_by_probability(authors_410, 20))\n",
    "#print(get_by_probability(authors_501, 20))\n",
    "\n",
    "print(get_topn_authors([authors_410,authors_501], 30))\n",
    "#print(get_by_probability([authors_410,authors_501], 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load osc config and parse for later use\n",
    "import csv\n",
    "\n",
    "def load_osc_config(file_name):\n",
    "    config = {}\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter='\\t', quotechar='|')\n",
    "        index = 0\n",
    "        for row in reader:\n",
    "            if index > 0:\n",
    "                #print(row)\n",
    "                for i in range(len(row)):\n",
    "                    config[i].append(row[i])\n",
    "            else:\n",
    "                config[0] = []\n",
    "                config[1] = []\n",
    "                config[2] = []\n",
    "                config[3] = []\n",
    "                config[4] = []\n",
    "            index += 1\n",
    "    \n",
    "    return config\n",
    "            \n",
    "osc_config = load_osc_config('osc_config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-47a6c1561ee5>:33: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "def generate_lines_linear(count):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        perc = len(return_lines)/float(count)\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(perc)\n",
    "        #print(len(topn))\n",
    "        #print(sentence)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        if len(stripped_post) > 20 and stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "        \n",
    "def generate_lines_from(fromp, step, count):\n",
    "    for i in range(count):\n",
    "        sentence = random.choice(all_posts)\n",
    "        perc = fromp + i * step\n",
    "\n",
    "        topn = get_topn_percent(perc)\n",
    "        #print(len(topn))\n",
    "        stripped_post = remove_non_frequent(topn, sentence[1].strip().lower())\n",
    "        print(\"{:.6f}\".format(perc) + \": \" + stripped_post)\n",
    "        \n",
    "def generate_by_osc(config, lists, offset=90):\n",
    "    assert(len(config) is not len(lists), \"len of config and lists needs to be the same\")\n",
    "\n",
    "    index = 0\n",
    "    possible_words = []\n",
    "    # length of generated sentences is determined by osc/curve data\n",
    "    length = len(config[0])\n",
    "    for l in lists:\n",
    "        _config = config[index]\n",
    "        _min = l[0]\n",
    "        _max = l[1]\n",
    "        app_list = []\n",
    "        for i in range(_min, _max+1):\n",
    "            app_list.append(i)\n",
    "        \n",
    "        # all allowed words\n",
    "        selected_appearances = get_by_appearance_list(app_list)\n",
    "        possible_words.append((selected_appearances, _config))\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    # all words with min appearance of 500 are always allowed\n",
    "    always_allowed = get_all_by_max_appearance(500)\n",
    "        \n",
    "    finished_sentences = []\n",
    "    for i in range(length):\n",
    "        #s = random.choice(all_posts)[1]\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        # don't process djs\n",
    "        if 'DJ`Mercury' in a or 'DJ`Protea' in a:\n",
    "            finished_sentences.append((a, s))\n",
    "            continue\n",
    "            \n",
    "        for p in possible_words:\n",
    "            _words = p[0]\n",
    "            probability = float(p[1][i])\n",
    "            s = remove_by_probability(_words, always_allowed, s, probability)\n",
    "        \n",
    "        finished_sentences.append((a, s))\n",
    "    \n",
    "    return finished_sentences\n",
    "\n",
    "def render_plain(out_filename, line_count, offset):\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for i in range(line_count):\n",
    "        s = all_separated[offset+i][1]\n",
    "        a = all_separated[offset+i][0]\n",
    "        outfile.write(a + ': ' + s + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "def render_by_osc(osc_config, out_filename, offset):\n",
    "    lists = []\n",
    "    lists.append((1, 5))\n",
    "    lists.append((6, 20))\n",
    "    lists.append((21, 50))\n",
    "    #lists.append((51, 100))\n",
    "    #lists.append((101, 500))\n",
    "    osc_config = load_osc_config(osc_config)\n",
    "    generated = generate_by_osc(osc_config, lists, offset)\n",
    "\n",
    "    outfile = codecs.open(out_filename, mode='w', encoding='utf-8')\n",
    "    for line in generated:\n",
    "        outfile.write(line[0] + ': ' + line[1] + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@erotic_kitty' '@erotic_kitty' '@DJ`liltech']\n"
     ]
    }
   ],
   "source": [
    "auth = get_by_probability([authors_410,authors_501], 300)\n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% (6583 of 130000) |                  | Elapsed Time: 0:02:32 ETA:   0:47:39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86% (111899 of 130000) |#############   | Elapsed Time: 0:02:34 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "# samples random lines from corpus, add a author from act1&2 by distribution and outout after a certain length of the line\n",
    "# randomly add entrences and exits from the last 10 of the top active 30 authors of act1&2\n",
    "\n",
    "#random.seed(1)\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "max_count = 130000\n",
    "auth = get_by_probability([authors_410,authors_501], max_count)\n",
    "\n",
    "def generate_lines_maxapp(count, maxapp):\n",
    "    return_lines = []\n",
    "    while len(return_lines) < count:\n",
    "#    for i in range(1, count):\n",
    "        #perc = len(return_lines)/float(count)\n",
    "        #perc = 0.5\n",
    "        sentence = random.choice(all_posts)[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        #topn = get_all_by_max_appearance(maxapp)\n",
    "        topn = get_topn_percent(maxapp)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        #if len(stripped_post) > 20 and len(stripped_post) < 50 and stripped_post not in return_lines:\n",
    "        if stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "def process_all(maxapp):\n",
    "    return_lines = []\n",
    "    for p in all_posts:\n",
    "#    for i in range(1, count):\n",
    "        #perc = len(return_lines)/float(count)\n",
    "        #perc = 0.5\n",
    "        sentence = p[1]\n",
    "        #sentence = random.choice(all_posts)[1]\n",
    "        if 'radio meltdown' in sentence:\n",
    "            continue\n",
    "        #topn = get_all_by_max_appearance(maxapp)\n",
    "        topn = get_topn_percent(maxapp)\n",
    "        stripped_post = remove_non_frequent(topn, sentence.strip().lower())\n",
    "        #if len(stripped_post) > 20 and len(stripped_post) < 50 and stripped_post not in return_lines:\n",
    "        if stripped_post not in return_lines:\n",
    "            return_lines.append(stripped_post)\n",
    "        #print(str(perc) + \": \" + stripped_post)\n",
    "        \n",
    "    return return_lines\n",
    "\n",
    "outfile = codecs.open('1506_only_top0.005_all.txt', mode='w')\n",
    "\n",
    "# 20 top authors from 401 & 510\n",
    "auth_text = get_by_probability([authors_410,authors_501], max_count, 20)\n",
    "auth_io = get_topn_authors([authors_410,authors_501], 30)[-10:] # get_by_probability([authors_410,authors_501], 50, 30)[-10:]\n",
    "#print(auth_text)\n",
    "#print(auth_io)\n",
    "\n",
    "\n",
    "pbar = ProgressBar(widget=[Percentage(), Bar()], maxval=max_count).start()\n",
    "\n",
    "# 300 lines\n",
    "index = 0\n",
    "#lines = generate_lines_maxapp(max_count, 0.025)\n",
    "lines = process_all(0.005)\n",
    "print(len(lines))\n",
    "for line in lines:\n",
    "    l = ''.join(line)\n",
    "    outfile.write(auth[index] + ': ' + l + '\\n')\n",
    "    #print(auth_text[index] + ': ' + l)\n",
    "    #print(index)\n",
    "    pbar.update(index)\n",
    "    if random.uniform(0, 1) > 0.7:\n",
    "        lj = \"\"\n",
    "        if random.uniform(0, 1) > 0.5:\n",
    "            lj = \" LEFT THE ROOM\"\n",
    "        else:\n",
    "            lj = \" JOINED THE ROOM\"\n",
    "        #print(random.choice(auth_io)[0] + lj)\n",
    "        outfile.write(random.choice(auth_io)[0] + lj + '\\n')\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "pbar.update(max_count)\n",
    "outfile.close()\n",
    "# random in/outs for last 10\n",
    "# top_n 2000 / 2500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no no no\n",
      "annnnoooooooorrrrrraaaaaaaaaaaaa\n",
      "i know you knew, i just wanna know the interest of it, knowing that you know\n",
      "we have snow nordicgoddess but not enough to be snowed in .... yet lol\n",
      "no pwincess_aria no no no no\n",
      "pretends to know nothing then no one expects me to know anything\n",
      "no nononononono\n",
      "nnnnnoooo more poucing, hehe\n",
      "nnnnnooooooooooooooooooooooo\n",
      "yeah i know dj`annora not a good thing right now\n",
      "\u0002\u000307,01 limits: \u000300 no pee and scat, no permanent injuries, no animals nor kids\n",
      "lol no no no no\n",
      "nnnnnnnnnnnooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "# attempt to find similar sentences by matching vocabulary \n",
    "\n",
    "#li = ['her', 'her', 'her', 'he']\n",
    "li = ['are', 'fake']\n",
    "li = ['what', 'she', 'does']\n",
    "#li = ['what', 'what', 'what']\n",
    "li = ['is', 'is', 'is', 'is']\n",
    "li = ['being'] * 3\n",
    "\n",
    "#li = ['good', 'good', 'am']\n",
    "#li = ['here', 'here', 'here']\n",
    "#li = ['nice', 'nice', 'very']\n",
    "#li = ['well', 'well']\n",
    "#li = ['ll', 'll', 'll', 'll']\n",
    "li = ['no'] * 4\n",
    "#li = ['just', 'well', 'i']\n",
    "index = 0\n",
    "for post in all_posts:\n",
    "    p = post[1]\n",
    "    #print(p)\n",
    "    success = True\n",
    "    for l in li:\n",
    "        if l in p:\n",
    "            index = p.find(l)\n",
    "            length = len(l)\n",
    "            endindex = index + length\n",
    "            subs = p[index:endindex] \n",
    "            leftover = p[:index] + p[endindex:]\n",
    "            p = leftover\n",
    "        else:\n",
    "            success = False\n",
    "            #print(post[1] + ' failed')\n",
    "    if success and len(post[1]) < 80: \n",
    "        #print(len(post[1]))\n",
    "        print(post[1])\n",
    "    index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: to be ---- to see\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1281 of 164986) |                  | Elapsed Time: 0:00:15 ETA:   0:27:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Athena: -------- i enjoy the open air ---------- such as ------ or ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (3173 of 164986) |                  | Elapsed Time: 0:00:34 ETA:   0:26:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 1: Jothom: --- why im ------------ tired and body -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (16781 of 164986) |#                | Elapsed Time: 0:02:55 ETA:   0:25:18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 0: +DJ`Mercury: so by the ----- of ---- --- and she was --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% (45377 of 164986) |####             | Elapsed Time: 0:07:50 ETA:   0:21:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Silver_haired`Fox: damn i thought i was ---- thin at --- and ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (164985 of 164986) |############### | Elapsed Time: 0:29:25 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched pattern 2: Athena: -------- i enjoy the open air ---------- such as ------ or ---------\n",
      "matched pattern 1: Jothom: --- why im ------------ tired and body -----\n",
      "matched pattern 0: +DJ`Mercury: so by the ----- of ---- --- and she was --\n",
      "matched pattern 2: Silver_haired`Fox: damn i thought i was ---- thin at --- and ---\n"
     ]
    }
   ],
   "source": [
    "# todo: 1406\n",
    "# insert sentence into parser\n",
    "# extract gramatical structure\n",
    "# try to match through corpus\n",
    "# use pre-processed text, with dashes \n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "# punctation is NFP or PUNCT\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_tag_sequence(sen, detailed=False):\n",
    "    doc = nlp(sen)\n",
    "    sentence_pos_list = []\n",
    "    sentence_tag_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        sentence_pos_list.append(token.pos_)\n",
    "        sentence_tag_list.append(token.tag_)\n",
    "    \n",
    "    if detailed:\n",
    "        return sentence_tag_list\n",
    "    \n",
    "    return sentence_pos_list\n",
    "\n",
    "def match_sequence(seq_to_match, seq_of_target):\n",
    "    to_match = ''.join(seq_to_match)\n",
    "    of_target = ''.join(seq_of_target)\n",
    "    #print(to_match)\n",
    "    #print(of_target)\n",
    "    if to_match in of_target:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def match_sequences(sequences, seq_of_target):\n",
    "    mmm = []\n",
    "    for s in sequences:\n",
    "        mmm.append(match_sequence(s, seq_of_target))\n",
    "    #print(mmm)\n",
    "    return mmm\n",
    "\n",
    "    \n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "\n",
    "detailed = True    \n",
    "\n",
    "ssss = []\n",
    "ssss.append('---- and i was ----')\n",
    "ssss.append('---- green and red ----')\n",
    "ssss.append('---- such as ---- or ----')\n",
    "ssss.append('---- hi ---- and ----')\n",
    "ssss.append('---- of the wall ---- of the bank')\n",
    "taglist = []\n",
    "for s in ssss:\n",
    "    taglist.append(get_tag_sequence(s, detailed))\n",
    "    \n",
    "print('input: ' + sentence)\n",
    "matched_sentences = []\n",
    "index = 0\n",
    "bar = ProgressBar(widget=[Percentage(), Bar()], maxval=len(input_file)).start()\n",
    "for line in input_file:\n",
    "    if len(line) > 2:\n",
    "        tseq = get_tag_sequence(line.rstrip(), detailed)\n",
    "        matched = match_sequences(taglist, tseq)\n",
    "        ii = 0\n",
    "        for m in matched:\n",
    "            if m:\n",
    "                matched_sentences.append('matched pattern ' + str(ii) + ': ' + line.rstrip())\n",
    "                print('matched pattern ' + str(ii) + ': ' + line.rstrip())\n",
    "            else:\n",
    "                pass\n",
    "            ii += 1\n",
    "        index += 1\n",
    "        #if index > 2000:\n",
    "        #    break\n",
    "        bar.update(index)\n",
    "for m in matched_sentences:\n",
    "    print(m)\n",
    "if len(matched_sentences) == 0:\n",
    "    print('no results :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "max_line_counter = 0\n",
    "out_file = open('1406_min5_words_relative0.45.txt', 'w')\n",
    "perc = 0.45\n",
    "\n",
    "\n",
    "for line in input_file:\n",
    "    counter = 0\n",
    "    wordcount = len(line.split())\n",
    "    \n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "    if counter/float(wordcount) > perc and wordcount > 10:\n",
    "        out_file.write(line)\n",
    "        print(line)\n",
    "    max_line_counter += 1\n",
    "    #if max_line_counter > 90000:\n",
    "    #    break\n",
    "    \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.302:  winks at ---------------- as he must be here for ---- -------- ---- by his wife\n",
      "0.304:  grins from the ------ as her ass -------- back the ----- between her cheeks ------ with more panties as his ----- ---- deep between --------- as she ---- her hips to get -----------\n",
      "0.306:  his ------- as he ------ at her right ------ and pushing\n",
      "0.308:  a hand up into her ----- to feel that ---- ---- -------- her ------ as he sucks the back and left side of her neck\n",
      "0.31:  ---- in her hair as that hole has a good ----\n",
      "0.312:  as his finger ----- just a bit ------ now as she makes those\n",
      "0.314:  slide into that hair once more and pull her back sliding once more to his knees\n",
      "0.316:  coming as she ------- back down once more with her face and tits to the floor -------- that ------------ ass back up like she was made to do\n",
      "0.318:  ---- and ------ out sitting back up -------- his ------ with a warm smile looking --------- over as she ----- off\n",
      "0.32:  im --------- horny and not getting along as well as you it seems\n",
      "0.322:  thanks for asking but i just got here want to check out main room if you dont mine\n",
      "0.324:  takes a seat as she waves at --------- not bad how about you\n",
      "0.326:  you have ----- -------- around here and you just say hello\n",
      "0.328:  turns around with her ass to the room what about this one -------- her ass up and down\n",
      "0.33:  loves that moving her ass more up and down ------------- what i was talking about\n",
      "0.332:  smiles at bustylily do you have something for the room too\n",
      "0.334:  well the ---- on the door is all ----- down so i just ------ in out of ----------\n",
      "0.336:  very dead but it is late at least for me --\n",
      "0.338:  ---- three days with a cold do the ----- for me\n",
      "0.34:  not like when i was but im still in pretty good -----\n",
      "0.342:  well its good that a -------- thing comes out of a bad one --\n",
      "0.34400000000000003:  the ----- you ------ or have me o vikinglady the better she like it\n",
      "0.34600000000000003:  its quiet because people are busy im --------- no need for the -------- --------\n",
      "0.34800000000000003:  ------- his hand -------- into your hair his thick shaft slowly fucking your mouth\n",
      "0.35000000000000003:  im going to leave this channel as long as carla is here i dont feel ----------- here\n",
      "0.35200000000000004:  send a new one man come on dont be ---- like that\n",
      "0.35400000000000004:  - but hey its on the -------------- so it must be try right\n",
      "0.35600000000000004:  ---- up and down with a -------- sitting on me -----\n",
      "0.35800000000000004:  i was ------- waiting for your show djawesome but im just ----\n",
      "0.36000000000000004:  i know but im trying to ------ her while she busy\n",
      "0.36200000000000004:  i guess with this song you ------- out i was here\n",
      "0.36400000000000005:  i know shoobie im just that damn good ty ty d\n",
      "0.36600000000000005:  we all have an ----- wild side its just for some its ------- deep lol\n",
      "0.36800000000000005:  the damn ------- are -------- and no its not that get your ----- out of the damn ------ lol\n",
      "0.37000000000000005:  we have an angel its djwildkat someone send me her lol\n",
      "0.37200000000000005:  its too early for me to have a ----- -------- for that\n",
      "0.37400000000000005:  id go out for more but its about ----- one ------- or something\n",
      "0.37600000000000006:  hell if thats the case if i was her id be looking to ----- up lol\n",
      "0.37800000000000006:  oh ----- me she has her own she dont need me lol\n",
      "0.38000000000000006:  no i was going on what djkungfukitty did to me lol\n",
      "0.38200000000000006:  if ----- was a woman id be all down with that lol\n",
      "0.38400000000000006:  hey if i cant have her i have to take what i can get lol\n",
      "0.38600000000000007:  well sleep we dont need no -------- sleep thats for sure lol\n",
      "0.38800000000000007:  that just ------ me wild when she does that i dont know why lol\n",
      "0.39000000000000007:  then she ------- her nick to that ------ but well ------ lol\n",
      "0.39200000000000007:  if we are good ----- one day will be be on our heads\n",
      "0.3940000000000001:  shes been nothing but nice to me but well keep that our dirty ------\n",
      "0.3960000000000001:  see you all are nice ------ no wonder they dont let me be in -----------\n",
      "0.3980000000000001:  im cool with it if its me and a ---- ---------- me in her ass\n",
      "0.4000000000000001:  last year there --------------- was not as good as it use to be\n",
      "0.4020000000000001:  -------- have one that has -------- am so you can not be\n",
      "0.4040000000000001:  im not going im too ------- and still have things to do\n",
      "0.4060000000000001:  well not so much tonight but not as bad as last night\n",
      "0.4080000000000001:  im good thanks just -------- just got home from work how are you\n",
      "0.4100000000000001:  so if no ------ you will have ---- --------- on your dick\n",
      "0.4120000000000001:  i just need money dont have to ---- me at all lol\n",
      "0.4140000000000001:  she was all like thank you for ------- me up it was really nice of you and im like yeah yeah ------------ get out my --- you ----- like a bar\n",
      "0.4160000000000001:  was it a nice time wanna ---- me in on what we did ---------\n",
      "0.4180000000000001:  yeah we have an awesome ------ here but they dont make ------\n",
      "0.4200000000000001:  and dont go around -------- all ------- are ------- or ---------- for that -------\n",
      "0.4220000000000001:  wasnt she the one that was going on and on with that cat ----- -------\n",
      "0.4240000000000001:  there was a --- in ------- for a time that ------ that for me\n",
      "0.4260000000000001:  yeah that was my idea but im not allowed in --------\n",
      "0.4280000000000001:  not yet but its ------ up on another --- for later roughone\n",
      "0.4300000000000001:  so i dont know if im sub or dom hell i dont know if im into ---- at all the thing is i just ------- that i like women with ------ and i think ------- are ------\n",
      "0.4320000000000001:  was good but --- not what i wanted a ------ of\n",
      "0.4340000000000001:  i dont know how to do that but the first thing that came up was ------------ ---- -------\n",
      "0.4360000000000001:  i find that to be -------- but what do i know\n",
      "0.4380000000000001:  well what they came at me with was pretty ---- shit so\n",
      "0.4400000000000001:  i ------ most the girls on bi are just here for women thats not a ----- but like\n",
      "0.4420000000000001:  im looking to get into something just not sure what yet\n",
      "0.4440000000000001:  oh nice how long are you on -------- for and how are you -------- it\n",
      "0.4460000000000001:  will do thank you working on some ----- my ---------- ---- me so im not all here or there as the case may be\n",
      "0.4480000000000001:  you do as your told and she ------- you if she -------\n",
      "0.4500000000000001:  we should be ---- all that now but were not yet\n",
      "0.4520000000000001:  its not like im trying to pick ya up well i am asking\n",
      "0.4540000000000001:  you have any questions for me since were at that -----\n",
      "0.4560000000000001:  ------- are not as nice to look at - they just do their job\n",
      "0.45800000000000013:  laughs at ----------- no need to roll your eyes we are all just making -------- ----- here\n",
      "0.46000000000000013:  she ----- a little with her free legs as her ----- are ------\n",
      "0.46200000000000013:  im on a ---------- so its not like they can help\n",
      "0.46400000000000013:  but im not ----------- we always went --- here cause of how much was ---------- out\n",
      "0.46600000000000014:  how do people think with a name like battlecat that im female\n",
      "0.46800000000000014:  nope not my way take me as i am and all of me or not at all --\n",
      "0.47000000000000014:  wow we have a ----- ---- that just came up here\n",
      "0.47200000000000014:  ------------ back into ------------- ----- in your ------- well n all that\n",
      "0.47400000000000014:  im not one for -------- just ------ and then only if its ------ right\n",
      "0.47600000000000015:  if biffyyy did any ------- ---- with what i have now she be on her butt\n",
      "0.47800000000000015:  she was in she was out she was in she was out\n",
      "0.48000000000000015:  i have heard her ------------------- almost as good as you are ------\n",
      "0.48200000000000015:  as long as its not to my --------------- its all good\n",
      "0.48400000000000015:  be better if i didnt have to work but its all good\n",
      "0.48600000000000015:  why come here at all if you have a ------- at all lmao\n",
      "0.48800000000000016:  i dont call myself that its what others have called me\n",
      "0.49000000000000016:  but if ya ---- on ------- here dont bring your --------\n",
      "0.49200000000000016:  i dont have a ------ door so im out of that ------------\n",
      "0.49400000000000016:  well your wrong nick so you just ------ we are right\n",
      "0.49600000000000016:  she just doesnt like those that -------- with her thats not -- problem lol\n",
      "0.49800000000000016:  no im just ------- --- im not in your house anastatia\n",
      "0.5000000000000001:  its warm where im at well ------ than what its been\n",
      "0.5020000000000001:  we are all alone now ta its just u and me\n",
      "0.5040000000000001:  in -------- --------- we have ---------- as well as ----- here\n",
      "0.5060000000000001:  so it aint all that bad if you look at it like that\n",
      "0.5080000000000001:  gracie she -------- be a ------- ------- but we have one like that\n",
      "0.5100000000000001:  was it as good for you as it was for me --\n",
      "0.5120000000000001:  as if i would know if that was --------- or not -- lol\n",
      "0.5140000000000001:  i dont know how he puts up with me at times\n",
      "0.5160000000000001:  ------ not ------- for that much here and your just ----- of me\n",
      "0.5180000000000001:  she need us so we did what we had to do for her\n",
      "0.5200000000000001:  maybe its two but its one of those that look like its just one\n",
      "0.5220000000000001:  as long as you dont have a dick ------ back im good\n",
      "0.5240000000000001:  well well ---- was --------- were huggss and all that was\n",
      "0.5260000000000001:  these guys have a good ----- but its just not that guys -----\n",
      "0.5280000000000001:  - im not sure what that has to do with me ----------------\n",
      "0.5300000000000001:  so your one with no ------- for anyone then eh --------\n",
      "0.5320000000000001:  no like i dont dog people if they dont like to read but\n",
      "0.5340000000000001:  so things have been good but are you up to no good bustylily\n",
      "0.5360000000000001:  its just me my wait what was ---- right back to me\n",
      "0.5380000000000001:  but its not doing that now because its -------- your ----\n",
      "0.5400000000000001:  its going ---- ---- so as good as it can be\n",
      "0.5420000000000001:  im good at what i do and you are all welcome\n",
      "0.5440000000000002:  well you ------ me so what are you gonna do now\n",
      "0.5460000000000002:  not at all im also thinking of her body as well\n",
      "0.5480000000000002:  but if im gonna be -------- i may as well be naked\n",
      "0.5500000000000002:  was it good for you as it was for me oh yes\n",
      "0.5520000000000002:  looks up at mrd as she bites her ----- or what sir\n",
      "0.5540000000000002:  well one that was easy or --------- but if you dont have one its cool\n",
      "0.5560000000000002:  thats why she dont like people ------- with her ---- that what she call me\n",
      "0.5580000000000002:  no ------- at all just not ur place to do so\n",
      "0.5600000000000002:  i ----- she would like for me to do that im not her type\n",
      "0.5620000000000002:  no i dont ---------- i dont like that word at all\n",
      "131\n",
      "0.5640000000000002:  hey hey djshoo im doing good how are you beautiful\n",
      "0.5660000000000002:  if we are good ----- one day will be be on our heads\n",
      "0.5680000000000002:  well not so much tonight but not as bad as last night\n",
      "0.5700000000000002:  well what they came at me with was pretty ---- shit so\n",
      "0.5720000000000002:  she was in she was out she was in she was out\n",
      "0.5740000000000002:  ---------------------------------- like that\n",
      "0.5760000000000002:  as long as its not to my --------------- its all good\n",
      "0.5780000000000002:  be better if i didnt have to work but its all good\n",
      "0.5800000000000002:  i dont know just have not been feeling so good\n",
      "0.5820000000000002:  and that will do what for me --------------------\n",
      "0.5840000000000002:  ----- funkyboogieking dont ---- at me like that\n",
      "0.5860000000000002:  gracie she -------- be a ------- ------- but we have one like that\n",
      "0.5880000000000002:  or just -------- up ------- with no ------\n",
      "0.5900000000000002:  she was like you dont love me no more\n",
      "0.5920000000000002:  and your not even very good at that\n",
      "0.5940000000000002:  ------ be back when she have her coffee\n",
      "0.5960000000000002:  we have that now --------------------\n",
      "0.5980000000000002:  was it as good for you as it was for me --\n",
      "0.6000000000000002:  hey we are good at ---- ------- lol\n",
      "0.6020000000000002:  good luck with that one simian\n",
      "0.6040000000000002:  im not good on ----- for long\n",
      "0.6060000000000002:  what no dj what are we to do\n",
      "0.6080000000000002:  do you like -------- as well\n",
      "0.6100000000000002:  i do but im not good at it\n",
      "0.6120000000000002:  so maybe not ------ at all\n",
      "0.6140000000000002:  i think we all ---- on that one\n",
      "0.6160000000000002:  its not that im into it im just ----- lolol\n",
      "0.6180000000000002:  probably not as ----- as me\n",
      "0.6200000000000002:  be back later all have a good afternoon\n",
      "0.6220000000000002:  or that ------------\n",
      "0.6240000000000002:  good one ----------\n",
      "0.6260000000000002:  well that works out well\n",
      "0.6280000000000002:  ------------- for me\n",
      "0.6300000000000002:  not just me then\n",
      "0.6320000000000002:  was that ----- for me\n",
      "0.6340000000000002:  with your ------------\n",
      "0.6360000000000002:  its ----- here\n",
      "0.6380000000000002:  not me ---------------\n",
      "0.6400000000000002:  damn that was good\n",
      "0.6420000000000002:  for how long\n",
      "0.6440000000000002:  ----- im good\n",
      "0.6460000000000002:  not ------- at all\n",
      "0.6480000000000002:  im not justme\n",
      "0.6500000000000002:  here we go\n",
      "0.6520000000000002:  stop looking at me like that\n",
      "0.6540000000000002:  hey hey justme\n",
      "0.6560000000000002:  for ----- on her\n",
      "0.6580000000000003:  nice one djscorpiouk\n",
      "0.6600000000000003:  im just that good ----\n",
      "0.6620000000000003:  just pick one\n",
      "0.6640000000000003:  no pwincessaria no no no no\n",
      "0.6660000000000003:  grins at her\n",
      "0.6680000000000003:  just leans back\n",
      "0.6700000000000003:  she was here too\n",
      "0.6720000000000003:  just one\n",
      "0.6740000000000003:  they dont have one\n",
      "0.6760000000000003:  she ---- up on me\n",
      "0.6780000000000003:  no im not naked\n",
      "0.6800000000000003:  we dont do days\n",
      "0.6820000000000003:  just ---- with her\n",
      "0.6840000000000003:  hey hello\n",
      "0.6860000000000003:  be well be safe\n",
      "0.6880000000000003:  that was good\n",
      "0.6900000000000003:  we not\n",
      "0.6920000000000003:  not all\n",
      "0.6940000000000003:  so good\n",
      "0.6960000000000003:  have\n",
      "0.6980000000000003:  or\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "\n",
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "max_line_counter = 0\n",
    "out_file = open('1506_structure_02.txt', 'w')\n",
    "\n",
    "perc = 0.3\n",
    "chosen = []\n",
    "wordcount_min = 10\n",
    "\n",
    "for line in input_file:\n",
    "    #perc += 0.002\n",
    "    counter = 0\n",
    "    #print(line)\n",
    "    line = line.split(':')\n",
    "    \n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    auth = line[0]\n",
    "    line = line[1]\n",
    "    if len(line) < 3:\n",
    "        continue\n",
    "    wordcount = len(line.split())\n",
    "\n",
    "\n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "\n",
    "    if counter / float(wordcount) > perc and wordcount > wordcount_min:\n",
    "        perc += 0.002\n",
    "        chosen.append(line)\n",
    "        out_file.write(auth + ': ' + line)\n",
    "        print(str(perc) + ': ' + line.rstrip())\n",
    "        \n",
    "    if len(chosen) > 150:\n",
    "        break\n",
    "    #max_line_counter += 1\n",
    "    #if max_line_counter > 90000:\n",
    "    #    break\n",
    "print(len(chosen))\n",
    "\n",
    "out_file.write('--------------------------------------------------------------------------------\\n')\n",
    "\n",
    "short_chosen = []\n",
    "min_len = 50\n",
    "max_len = 140\n",
    "for line in input_file:\n",
    "    line = line.split(':')\n",
    "    if len(line) < 2:\n",
    "        continue \n",
    "    auth = line[0]\n",
    "    line = line[1]\n",
    "    if len(line) < 3:\n",
    "        continue\n",
    "    counter = 0\n",
    "    wordcount = len(line.split())\n",
    "    linelen = len(line)\n",
    "    \n",
    "    if wordcount == 0:\n",
    "        continue\n",
    "    \n",
    "    for word in line.split():\n",
    "        for p in possible:\n",
    "            if word == p:\n",
    "                counter += 1\n",
    "                \n",
    "    \n",
    "    if counter / float(wordcount) > perc and linelen < max_len and linelen > min_len:\n",
    "        min_len -= 1\n",
    "        max_len -= 2\n",
    "        perc += 0.002\n",
    "        short_chosen.append(line)\n",
    "        out_file.write(auth + ': ' + line)\n",
    "        print(str(perc) + ': ' + line.rstrip())\n",
    "        \n",
    "    if len(short_chosen) > 70:\n",
    "        break\n",
    "    #max_line_counter += 1\n",
    "\n",
    "print(len(short_chosen))\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " its --------- for you to open your mouth in a room like this\n",
      " but im doing it out of ------- because my ----- rather long for me\n",
      " not yet but its ------ up on another --- for later roughone\n",
      " --------------------------------------------- saffronwh how are you -----\n",
      " -------- and ------- are pretty ----- but i have not heard that any of the -------- have been closed\n",
      " i have - ---------- ---------- -------- who have ------- to work all week so im bored\n",
      " ------ her lips kissing him back just as innocent and sweet\n",
      " well fuck youre not a damn --- anymore man the fuck up and ask her out its really ------- all she can do is say no then you just move on\n",
      " i think we all ------ it was in the ----- its well over its ---------\n",
      " ------- her neck -------- it to ---- ------ her lip to ----- her ---- his soft kisses feels so nice as she ----- her legs ever so slightly\n",
      " chuckles at the ------ seyyal im not -------- just ------- the best for you\n",
      " ------ up onto a ------ ------ ------ on all ------ then ---- on her chest\n",
      " slides his fingers up as ------ ------ her back -------- them down the front of her pants looking into her eyes as he ------- her naked wet pussy\n",
      " smiles and slides a hand into her hair just ------- it sitting on a ------- behind her his other hand around her ------ ---------- into her ear with a smile\n",
      " well i have a -------- for a reason if he was interested in that he would have ----- me but hey\n",
      " hey ------------- hey -------- hey ------- hey ------- hey -----\n",
      " -------- how she is sitting on him to -------- his lap and ----- her arms around his --------- as she ------- on his tongue between her own\n",
      " im sure it aint ------ ------- back on his ------ valkyrie lol\n",
      " was kicked by ---------------------- we do not ------- in here\n",
      " you show up as pink and ------ for me now ----------- pretty\n",
      " oh fuck it why dont we all just agree on chocolate --------\n",
      " no it was not it was just a question what you wanna to do smiles\n",
      " i have no idea if that was the ----- i was looking for but ill take it\n",
      " we are lucky we do not get ---------- ----------- high -----\n",
      " that works but you have to do it with every ------ that ------ your ---- and thats a lot p\n",
      " well have to give her a ------ -------- to make good on her --------\n",
      " hey ------- hey ----------- hey ------------- hey -------- hey -------- hey carla\n",
      " ------ down on her -------- as he ----- her body up and down on his cock ---------- her tight lil -------\n",
      " im going to leave this channel as long as carla is here i dont feel ----------- here\n",
      " - laughing out loud no he didnt do that but he is --------------\n",
      " now we need to be ----------- ------- its not ---- its ---------\n",
      " goes off to his own bed -- on his own --- with no one else --- not even amie ---\n",
      " moans softly again ------- her back as his big hand -------- her ------ body --------- her ------ ------ as we kiss\n",
      " truth name one of your bad --------- and one of your good ------------\n",
      " then they dont have to be -------------- ready with ----- or whatever\n",
      " takes him back in ---------- like the good slut she wants to be for him\n",
      " mine dont do that are you holding your finger on the ----- bar invictus\n",
      " lets herself be ------- for a ------- but then she pushes ------- back gently a ----- on her face\n",
      " they -------- them how to take up for the kids here when something like that happen\n",
      " dont we all want to ----- with the ------------- -----------\n",
      " my days just about ------- so says the ------ but yeah it was so so lol\n",
      " - good not the pain but that you are ----------------- help for it\n",
      " im not working this weekend so im here to get myself off for once\n",
      " its not tea if u dont sugar it then its just ----- --------- water\n",
      " yawns and drops down to her usual couch --------- out as she --------- what to do with her day\n",
      " if we could ------ --------------- ------------------- be --------\n",
      " ------ --------- up and slides her panties down her legs so she get on all ----- on the couch\n",
      " but im not ----------- we always went --- here cause of how much was ---------- out\n",
      " truth name one of your bad --------- and one of your good ------------\n",
      " yawns and drops down to her usual couch --------- out as she --------- what to do with her day\n",
      " im not too into chocolate ------- so ill ---- all that for you -------\n",
      " i like nice creative --------- that are not ------- by our -------- world\n",
      " i ------------ when i was -------- but its not where i ----- up\n",
      " ----- back to her couch still --------- and ------- that her ---- didnt work as well as she had -----\n",
      " someone doesnt know me if they are --------- at descriptive\n",
      " ------ up on her --------- as he keeps his face ------- against her --------\n",
      " looking down at her watching her ------- up at me as she ------- my cock balls -------- her ---- ------ lips ------\n",
      " ----------------------- hey hey watch out for my feet please\n",
      " dont we all want to ----- with the ------------- -----------\n",
      " okay im not feeling so well today ----- its the no coffee im ----------\n",
      " if all the ----- are more -------- that we dont then one would ------------\n",
      " as long as you dont start -------- ------- ------ or at least\n",
      " ------- her eyes on his ---------- again at his request sucking so hard on his tip as she -------\n",
      " not moving just ----- on her bar ----- ------- her eyes feeling his ------- on her ear just nods to him\n",
      " rolls her over on her back and ----- with her bar breasts with one hand using a ----- to clean her face with the other\n",
      " ----- his ----- pushing her lips down his shaft and back up\n",
      " --------- all this ------ has to ---- out you all be good dont do anything id do\n",
      " im not a metallica --- at all riogrande so kind of ---------\n",
      " i have no idea if that was the ----- i was looking for but ill take it\n",
      " kisses mrtlwolf --------------- her tongue into his mouth -------- with his\n",
      " blushes -------- as she does his hand ------- her cheek as she does\n",
      " i have no idea if that was the ----- i was looking for but ill take it\n",
      " blushes -------- as she does his hand ------- her cheek as she does\n",
      " ----- invictus your still here so how can -------- be gone p\n",
      " isnt that usually the case im not here and then ----- ------- im back\n",
      " ------------ if yes not so fucking innocent if not shakes head\n",
      " then they dont have to be -------------- ready with ----- or whatever\n",
      " --------- all this ------ has to ---- out you all be good dont do anything id do\n",
      " no it was not it was just a question what you wanna to do smiles\n",
      " her back --------- ----- a tight ---- as her head falls back with ------ pulling back ------- his cock with her ass\n",
      " slips erotickittys her ------- seeing its ------ for coffee for me\n",
      " if she starts taking off her ------- that will be ------------\n",
      " gets back on all ------ looking at him and licking up his shaft\n",
      " not much going to get ---------- ass up so we can get out and do something its nice out today\n",
      " gets back on all ------ looking at him and licking up his shaft\n",
      " what is that face for - looks like your ----- are -------- out ---\n",
      " im sure she is a ------ but im good you two ----- on with your ------ ------\n",
      " laughs at ----------- no need to roll your eyes we are all just making -------- ----- here\n",
      " what fantasy do you still have to ---------- with your -----------\n",
      " gracie she -------- be a ------- ------- but we have one like that\n",
      " we dont have face ------- ------- that will suck out your -----\n",
      " well they do come out as ----- ------ and are not really ------\n",
      " pulls back letting his cock rest on her face licking at his balls\n",
      " - so ----------------------- what was your ---------------- like any -------------------\n",
      " winks at --------------- as her body shakes as -- works her good\n",
      " if you are ---------- you are ------- with where and how you are\n",
      " we are lucky we do not get ---------- ----------- high -----\n",
      " slips erotickittys her ------- seeing its ------ for coffee for me\n",
      " her back --------- ----- a tight ---- as her head falls back with ------ pulling back ------- his cock with her ass\n",
      " we were talking about how she cant see what she is or what she really looks like\n",
      " but no if we really get into it the ------ --------- are -------\n",
      " ------- his cock back into her face and holds her on his cock again\n",
      " im just --------- if that ---- ------- at ------- actually works\n",
      " grins at her his hand on her --- for a moment before she moves away\n",
      " will do thank you working on some ----- my ---------- ---- me so im not all here or there as the case may be\n",
      " - ------------------- work i do with a glad heart or not at all\n",
      " these guys have a good ----- but its just not that guys -----\n",
      " -------- - love how she ----- over as she feels the cum on all over her back\n",
      " wasnt she the one that was going on and on with that cat ----- -------\n",
      " ------- her eyes on his ---------- again at his request sucking so hard on his tip as she -------\n",
      " no hi no how are you - no dinner or ------ just straight to it\n",
      " - they keep coming out with good ones im always like i have to get that one\n",
      " your --------- would be just as ------ with the ---- --- on\n",
      " or at least you should have used to be seeing as how its --- in here\n",
      " -------------------- i dont have that on my -----------------\n",
      " ------ not ------- for that much here and your just ----- of me\n",
      " - so ----------------------- what was your ---------------- like any -------------------\n",
      " wasnt she the one that was going on and on with that cat ----- -------\n",
      " ----- as the -------- makes her ---- her back and ------ her ---- out as she ------ up into his ------ eyes she licks her lips again\n",
      " ----- as the -------- makes her ---- her back and ------ her ---- out as she ------ up into his ------ eyes she licks her lips again\n",
      " all the ------ ----- are here is that what youre ------- me\n",
      " ------- his cock back into her face and holds her on his cock again\n",
      " so things have been good but are you up to no good bustylily\n",
      " ------- her head on his -------- ------- his cock again but not if its going to come with that\n",
      " ------- her head as much as she can as her ass is --------- out\n",
      " winks at --------------- as her body shakes as -- works her good\n",
      " maybe its two but its one of those that look like its just one\n",
      " im not one for -------- just ------ and then only if its ------ right\n",
      " i have heard her ------------------- almost as good as you are ------\n",
      " thats why she dont like people ------- with her ---- that what she call me\n",
      "###########################################################################################################################\n",
      " ------ not ------- for that much here and your just ----- of me\n",
      " well yeah she said she was ------ out of --------- didnt she\n",
      " ----- like ----- ----- ------- thighs like what what what\n",
      " what do you have ------- for your morning --------------\n",
      " not at all ------------- was just -------- something out p\n",
      " but its not doing that now because its -------- your ----\n",
      " so your one with no ------- for anyone then eh --------\n",
      " well your wrong nick so you just ------ we are right\n",
      " we all have an ---------- ------ with one ---------\n",
      " and no i do not have ------------------------------------\n",
      " well not so much tonight but not as bad as last night\n",
      " well so no ------ ------ i ------- well just move on\n",
      " im on a ---------- so its not like they can help\n",
      " if we are good ----- one day will be be on our heads\n",
      " giggles as she looks at her one hello master --\n",
      " make out with it like its your --------- ----------\n",
      " wow we have a ----- ---- that just came up here\n",
      " ---------- with what youre good at madmick\n",
      " giggles as she looks at her one hello master --\n",
      " i was ------- at ----- with that at least\n",
      " im not the -------- one im not the -------- one\n",
      " looks up at ----- then back at his cock\n",
      " awww no just as you were ------- me on\n",
      " go back to -------- me its all im good for\n",
      " then its a no if its not something -----\n",
      " rocks out with her ------ ----- out\n",
      " that too zyta but im not so ------ --\n",
      " cool im good too so what you up too\n",
      " at least its not one ----- or something\n",
      " we have ----- that ----- to that\n",
      " but im not that ----- -------------\n",
      " its the one she ----- me lol\n",
      " hey its --------- welcome back\n",
      " what are you into --------------\n",
      " -------------------- nice one\n",
      " -------- what was that for\n",
      " well we cant have that --------\n",
      " hey its your ------ not mine\n",
      " just ------- with her\n",
      " but its ------------\n",
      " im sorry for your ----\n",
      " nice to be here sir\n",
      " she was not ------\n",
      " im good how is everyone\n",
      " hey james how are you\n",
      " ---------- im good\n",
      " how do you do girl\n",
      " but a good one\n",
      " im well how are you\n",
      " always so nice\n",
      " so far so good --\n",
      " its me again\n",
      " but even so\n",
      " well go me\n",
      " how are doing\n",
      " how so\n",
      " hello here\n",
      " im at work\n",
      " but\n",
      " for what\n",
      " no\n",
      " all\n",
      " for\n",
      " but\n",
      " up\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "input_file = open('1406_only_top0.025_all.txt', mode='r', encoding='utf8').readlines()\n",
    "import random\n",
    "\n",
    "possible = ['for', 'her', 'that', 'hello', 'on', 'me', 'hey', 'are', 'with', 'good', 'all', 'im', 'have', 'as', 'up', 'at', 'your', \n",
    "            'not', 'like', 'his', 'be', 'just', 'no', 'so', 'its', 'how', 'back', 'out', 'one', 'but', 'was', 'what', 'if', 'dont', \n",
    "            'well', 'here', 'or', 'she', 'we', 'do', 'nice', 'into']\n",
    "\n",
    "def checkline(line):\n",
    "    line = line.split(':')\n",
    "    if len(line) < 2:\n",
    "        return False\n",
    "    line = line[1]\n",
    "    if len(line) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "dofirst = True\n",
    "\n",
    "finlines = []\n",
    "perc = 0.3\n",
    "wordlen_min = 60\n",
    "if dofirst:\n",
    "    for i in range(130):\n",
    "        while True:\n",
    "            line = random.choice(input_file)\n",
    "            if not checkline(line):\n",
    "                line = random.choice(input_file)\n",
    "                continue   \n",
    "            auth = line.split(':')[0]\n",
    "            line = line.split(':')[1]\n",
    "\n",
    "            counter = 0\n",
    "            wordcount = len(line.split())\n",
    "\n",
    "            if wordcount == 0:\n",
    "                continue\n",
    "\n",
    "            linelen = len(line)\n",
    "            for word in line.split():\n",
    "                for p in possible:\n",
    "                    if word == p:\n",
    "                        counter += 1\n",
    "\n",
    "            if counter / float(wordcount) > perc and linelen > wordlen_min and line not in finlines: \n",
    "                print(line.rstrip())\n",
    "                finlines.append(auth + ': ' + line)\n",
    "                perc += 0.0015\n",
    "                break\n",
    "\n",
    "print('###########################################################################################################################')\n",
    "        \n",
    "wordlen_min = 60\n",
    "wordlen_max = 70\n",
    "for i in range(70):\n",
    "    tried = 0\n",
    "    while True and tried < 10000:\n",
    "        tried += 1\n",
    "        line = random.choice(input_file)\n",
    "        if not checkline(line):\n",
    "            line = random.choice(input_file)\n",
    "            continue   \n",
    "        auth = line.split(':')[0]\n",
    "        line = line.split(':')[1]\n",
    "            \n",
    "        counter = 0\n",
    "        wordcount = len(line.split())\n",
    "        \n",
    "        if wordcount == 0:\n",
    "            continue\n",
    "        \n",
    "        linelen = len(line)\n",
    "        for word in line.split():\n",
    "            for p in possible:\n",
    "                if word == p:\n",
    "                    counter += 1\n",
    "        \n",
    "        if counter / float(wordcount) > perc and linelen > wordlen_min and linelen < wordlen_max and line not in finlines: \n",
    "            print(line.rstrip())\n",
    "            finlines.append(auth + ': ' + line)\n",
    "            perc += 0.0015\n",
    "            wordlen_min -= 1\n",
    "            wordlen_max -= 1\n",
    "            break\n",
    "print(len(finlines))\n",
    "\n",
    "outfile = open('1506_structure_02_03.txt', 'w')\n",
    "for line in finlines:\n",
    "    outfile.write(line)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mrzl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "111\n",
      "125\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# count i/s in 410/501\n",
    "leaves410 = 0\n",
    "entrances410 = 0\n",
    "leaves501 = 0\n",
    "entrances501 = 0\n",
    "\n",
    "for line in codecs.open('1006/410_utf8.txt', mode='r', encoding='utf-8'):\n",
    "    if 'has left' in line or 'has quit' in line:\n",
    "        leaves410 += 1\n",
    "    if 'joined the channel' in line:\n",
    "        entrances410 += 1\n",
    "    #if ':' in line and not in_blocklist(blocklist, line.strip()): # blocked?:\n",
    "        #sentence = line[line.find(\":\"):]\n",
    "        #all_posts.append((author, sentence.strip().lower()))\n",
    "        #all_posts_410.append((author, sentence.strip().lower()))\n",
    "\n",
    "        #author = line[:line.find(\":\")]\n",
    "        #if author not in authors: # save things\n",
    "            #authors[author] = set() # init list for author\n",
    "        #if author not in authors_410:\n",
    "            #authors_410[author] = set()\n",
    "        #authors[author].add(sentence.strip().lower())\n",
    "        #authors_410[author].add(sentence.strip().lower())\n",
    "\n",
    "\n",
    "for s in codecs.open(\"1006/501_utf8.txt\", mode='r', encoding='utf-8').readlines():\n",
    "    if 'has left' in s or 'has quit' in s:\n",
    "        leaves501 += 1\n",
    "    if 'joined the channel' in s:\n",
    "        entrances501 += 1\n",
    "    #if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "        #if not in_blocklist(blocklist, s.strip()): # blocked?\n",
    "            #sentence = s[s.find(\">\")+2:]\n",
    "            #all_posts.append((author, sentence.strip().lower()))\n",
    "            #all_posts_501.append((author, sentence.strip().lower()))\n",
    "\n",
    "            #author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "            #if author not in authors: # save things\n",
    "            #    authors[author] = set() # init list for author\n",
    "            #if author not in authors_501:\n",
    "            #    authors_501[author] = set()\n",
    "            #authors[author].add(sentence.strip().lower())\n",
    "            #authors_501[author].add(sentence.strip().lower()) \n",
    "            \n",
    "            \n",
    "         \n",
    "print(leaves410)\n",
    "print(entrances410)\n",
    "print(leaves501)\n",
    "print(entrances501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "\n",
    "def get_random_syn(word):\n",
    "    r = word\n",
    "    syns = wn.synsets(word)\n",
    "    lemmas = []\n",
    "    for ss in syns:\n",
    "        for lem in ss.lemma_names():\n",
    "            lemmas.append(lem)\n",
    "            #r = lem\n",
    "            #if lem is not word:\n",
    "            #    return lem\n",
    "    return lemmas\n",
    "\n",
    "def replace_sentence(sentence):\n",
    "    return_sentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        lemmas = get_random_syn(word)\n",
    "        if len(lemmas) > 0:\n",
    "            #print(word + str(lemmas))\n",
    "            r = random.choice(lemmas)\n",
    "            if r is \"atomic_number_53\":\n",
    "                r = 'I'\n",
    "            return_sentence += r + \" \"\n",
    "        else:\n",
    "            return_sentence += word + ' '\n",
    "    return return_sentence\n",
    "\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    sen = random.choice(all_posts)\n",
    "    if len(sen[1]) > 10:\n",
    "        print('-------------------------')\n",
    "        print(sen[1])\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))\n",
    "        print(replace_sentence(sen[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURVES generation\n",
    "\n",
    "random.seed(1)\n",
    "#render_plain('1306_removed_digits_entrances_01_ORIGINAL.txt', 500, 3860)\n",
    "#render_by_osc('osc_config_500_late_slowexp.csv', '1306_cleaner_group1&2&3_01.txt', 3860)\n",
    "print('finito')\n",
    "#generate_lines_linear(20)\n",
    "\n",
    "generate_lines_from(0.00, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.10, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.20, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.50, 0.0001, 10)\n",
    "print(\"####################################################################################\")\n",
    "generate_lines_from(0.80, 0.0001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving word groups\n",
    "lists = []\n",
    "#lists.append((1, 5))\n",
    "#lists.append((6, 20))\n",
    "#lists.append((21, 50))\n",
    "#lists.append((51, 100))\n",
    "#lists.append((101, 500))\n",
    "lists.append((3000, 10000))\n",
    "\n",
    "outfile = codecs.open('3000_10000_groups.txt', 'w')\n",
    "\n",
    "for l in lists:\n",
    "    possible_words = []\n",
    "    #_config = config[index]\n",
    "    _min = l[0]\n",
    "    _max = l[1]\n",
    "    app_list = []\n",
    "    for i in range(_min, _max+1):\n",
    "        app_list.append(i)\n",
    "\n",
    "    # all allowed words\n",
    "    selected_appearances = get_by_appearance_list(app_list)\n",
    "    outfile.write(' '.join(selected_appearances))\n",
    "    #possible_words.append((selected_appearances, _config))\n",
    "    \n",
    "outfile.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess act1&2\n",
    "\n",
    "for sentence in all_posts_501:\n",
    "    print(sentence[0] + \" -> \" + remove_non_frequent(get_topn_percent(0.01), sentence[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
