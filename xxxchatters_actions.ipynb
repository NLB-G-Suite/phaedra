{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.1 extract first sentences of what people are saying after they enter\n",
    "\n",
    "import glob\n",
    "from utils import in_blocklist\n",
    "\n",
    "blocklist = []\n",
    "blocklist.append('AWAYLEN=307 MAXTARGETS=20 WALLCHOPS WATCH=128 WATCHOPTS=A SILENCE=15 MODES=12 CHANTYPES=# PREFIX')\n",
    "blocklist.append('- * -')\n",
    "blocklist.append('XxXChatters.Com')\n",
    "blocklist.append('XxXChatters')\n",
    "blocklist.append('This server was created')\n",
    "blocklist.append('operator(s) online')\n",
    "blocklist.append('is now your displayed host')\n",
    "blocklist.append('Caps set:')\n",
    "blocklist.append('MAXCHANNELS')\n",
    "blocklist.append('http')\n",
    "blocklist.append('Meltdown')\n",
    "blocklist.append('14')\n",
    "\n",
    "first_sentences = []\n",
    "\n",
    "outfile = open('xxxchatters_entrance_first_words.txt', 'w')\n",
    "\n",
    "def find_post(lines_list, a):\n",
    "    for s in lines_list:\n",
    "        if s[0] is '[':\n",
    "            if s.find(\"<\") is not -1:\n",
    "                time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "                author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "                sentence = s[s.find(\">\")+2:]\n",
    "                if a == author and sentence not in first_sentences:\n",
    "                    first_sentences.append(sentence)\n",
    "                    out = time + \", \" + s\n",
    "                    outfile.write(out + \"\\n\")\n",
    "                    print(out)\n",
    "                    return\n",
    "\n",
    "for file in glob.glob(\"xxxchatters_logs/subset/*.log\"):\n",
    "    lines = open(file, 'r', encoding='utf-8').readlines()\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "            if \") Quit (\" in line or \") has left \" in line:\n",
    "                time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                author = line[line.find(\" * \")+3:line.find(\" (\")]\n",
    "                sentence = line[line.find(\">\")+2:]\n",
    "                \n",
    "                #print(\"left: \" + time + \", \" + author)# + \", \" + sentence)\n",
    "            if \") has joined \" in line:\n",
    "                time = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "                author = line[line.find(\" * \")+3:line.find(\" (\")]\n",
    "                #print(\"joined: \" + time + \", \" + author)\n",
    "                find_post(lines[index:], author)\n",
    "                \n",
    "        index += 1\n",
    "        \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219773\n"
     ]
    }
   ],
   "source": [
    "# 1.2 look at word frequency\n",
    "\n",
    "import glob\n",
    "from utils import in_blocklist\n",
    "\n",
    "blocklist = []\n",
    "blocklist.append('AWAYLEN=307 MAXTARGETS=20 WALLCHOPS WATCH=128 WATCHOPTS=A SILENCE=15 MODES=12 CHANTYPES=# PREFIX')\n",
    "blocklist.append('- * -')\n",
    "blocklist.append('XxXChatters.Com')\n",
    "blocklist.append('XxXChatters')\n",
    "blocklist.append('This server was created')\n",
    "blocklist.append('operator(s) online')\n",
    "blocklist.append('is now your displayed host')\n",
    "blocklist.append('Caps set:')\n",
    "blocklist.append('MAXCHANNELS')\n",
    "blocklist.append('http')\n",
    "#blocklist.append('Meltdown')\n",
    "#blocklist.append('14')\n",
    "blocklist.append('type it in the main room or a pm to the')\n",
    "\n",
    "\n",
    "full_lines = []\n",
    "\n",
    "for file in glob.glob(\"xxxchatters_logs/*.log\"):\n",
    "    lines = open(file, 'r', encoding='utf-8').readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if not in_blocklist(blocklist, line.strip()): # blocked?\n",
    "            if line[0] is '[':\n",
    "                full_lines.append(line)\n",
    "            \n",
    "\n",
    "lines = open(\"1006/410.txt\", 'r').readlines()\n",
    "for line in lines:\n",
    "    if ':' in line:\n",
    "        #author = line[:line.find(\":\")]\n",
    "        #authors.append(author)\n",
    "        sentence = line[line.find(\":\"):]\n",
    "        full_lines.append(line)\n",
    "\n",
    "lines = open(\"1006/501.txt\", 'r').readlines()\n",
    "for s in lines:\n",
    "    if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "        time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "        author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "        sentence = s[s.find(\">\")+2:]\n",
    "        #authors.append(author)\n",
    "        #sentences.append(sentence)\n",
    "        full_lines.append(s)\n",
    "print(len(full_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different Authors: 5819\n",
      "\n",
      "Lines: 219460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors = {}\n",
    "posts = set()\n",
    "all_posts = []\n",
    "\n",
    "for s in full_lines:\n",
    "    if s.find(\"<\") is not -1:\n",
    "        time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "        author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "        sentence = s[s.find(\">\")+2:]\n",
    "        all_posts.append(sentence.strip().lower())\n",
    "        posts.add(sentence.strip().lower())\n",
    "        if author not in authors: # save things\n",
    "            authors[author] = [] # init list for author\n",
    "        authors[author].append(sentence.strip()) \n",
    "    \n",
    "print('Different Authors: ' + str(len(authors)) + '\\n')\n",
    "print('Lines: ' + str(len(all_posts)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djmercury = authors['+DJ`Mercury']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import string\n",
    "word_counts = dict()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for post in posts:\n",
    "    word_list = post.split()\n",
    "    for word in word_list:\n",
    "        word = word.replace('\\x02', '')\n",
    "        word = word.replace('\\x03', '')\n",
    "        word = word.replace('.', '')\n",
    "        word = word.translate(translator)\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "sorted_word_counts = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list of top n words\n",
    "\n",
    "n = 2500\n",
    "print(sorted_word_counts[:n])\n",
    "top_n = []\n",
    "for t in sorted_word_counts[:n]:\n",
    "    top_n.append(t[0])\n",
    "    \n",
    "print(len(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only posts longer than 20 and only high frequent words\n",
    "\n",
    "def contains_all(_list, _words):\n",
    "    for word in _words:\n",
    "        if word not in _list:\n",
    "            return False\n",
    "    \n",
    "    return True;\n",
    "\n",
    "file = open(\"1006/only_top300_and_longer_than_20.txt\", \"w\")\n",
    "\n",
    "for post in posts:\n",
    "    words = post.split()\n",
    "    if contains_all(top_n, words):\n",
    "        if(len(post) > 20):\n",
    "            file.write(post + '\\n')\n",
    "            #print(post)\n",
    "            \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@erotic_kitty', 67), ('carrol', 54), ('+DJ`Mercury', 39), ('Louise', 33), ('@erotic_kitty{S}', 27), ('Valkyrie', 23), ('Threeleggedcat', 20), ('@saffron{WH}', 18), ('PlayfulBBC', 18), ('WhoGiveSaDamn', 15), ('Silver_haired`Fox', 14), ('sweet_teresa', 10), ('Woman', 8), ('Jothom', 8), ('Cruel`Intentions', 7), ('guynextdoor', 7), ('Anastatia', 7), ('^fran', 7), ('Sanger', 7), ('Jaems', 6), ('+DJ`South', 6), ('JFetish', 5), ('FunkyBoogieKing', 5), ('gracie', 4), ('@DJ`liltech', 4), ('Handyman', 4), ('MeanMark1', 4), ('Athena', 3), ('saffron{WH}', 3), ('TricksyM', 3)]\n",
      "returning: ['+DJ`Mercury' 'Woman' 'Athena' ... '+DJ`Mercury' 'Louise' '+DJ`Mercury']\n"
     ]
    }
   ],
   "source": [
    "# removes all non-frequent words from long ones\n",
    "import random\n",
    "\n",
    "def remove_non_frequent(_list, post, replace=True):\n",
    "    good_words = []\n",
    "    for word in post.split():\n",
    "        if word in _list:\n",
    "            good_words.append(word)\n",
    "        else:\n",
    "            if replace:\n",
    "                replacement = \"\"\n",
    "                for char in word:\n",
    "                    replacement += \"-\"\n",
    "                good_words.append(replacement)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "file = open(\"1006/7_named_removed_top2500_and_longer_than_80_before_replaced_unique_lines.txt\", \"w\")\n",
    "\n",
    "\n",
    "author_probs = get_author_probs(len(posts))\n",
    "for i in range(500):\n",
    "#for post in posts:\n",
    "    post = list(posts)[i]\n",
    "    if len(post) > 0:\n",
    "        perc = i / float(500)\n",
    "        n = 50000 * (1 - perc) \n",
    "        top_n = []\n",
    "        for t in sorted_word_counts[:int(n)]:\n",
    "            top_n.append(t[0])\n",
    "        stripped_post = remove_non_frequent(top_n, post.strip().lower(), True)\n",
    "        if len(stripped_post) > 0:\n",
    "            a = author_probs[i]\n",
    "            try:\n",
    "                if a == '+DJ`Mercury':\n",
    "                    file.write(a + \": \" + random.choice(authors['+DJ`Mercury']) + '\\n')\n",
    "                else:\n",
    "                    file.write(a + \": \" + stripped_post + '\\n')\n",
    "            except:\n",
    "                pass\n",
    "            #print(stripped_post)\n",
    "        \n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract authors probability\n",
    "import numpy as np\n",
    "\n",
    "def get_author_probs(count):\n",
    "\n",
    "    author_probability = {}\n",
    "\n",
    "    lines = open(\"1006/410.txt\", 'r').readlines()\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            author = line[:line.find(\":\")]\n",
    "            if author in author_probability.keys():\n",
    "                author_probability[author] += 1\n",
    "            else:\n",
    "                author_probability[author] = 1\n",
    "\n",
    "    lines = open(\"1006/501.txt\", 'r').readlines()\n",
    "    for s in lines:\n",
    "        if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "            time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "            author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "            sentence = s[s.find(\">\")+2:]\n",
    "            if not '[' in author:\n",
    "                if author in author_probability.keys():\n",
    "                    author_probability[author] += 1\n",
    "                else:\n",
    "                    author_probability[author] = 1\n",
    "    author_probability_counts = sorted(author_probability.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    top_authors = 30;\n",
    "    print(author_probability_counts[:top_authors])\n",
    "\n",
    "    sum_sentences = 0\n",
    "    for v in author_probability_counts[:top_authors]:\n",
    "        sum_sentences += v[1]\n",
    "\n",
    "    #print(sum_sentences)\n",
    "    author_probability_final = {}\n",
    "    author_list = []\n",
    "    prob_list = []\n",
    "\n",
    "    for v in author_probability_counts[:top_authors]:\n",
    "        author_probability_final[v[0]] = v[1]/float(sum_sentences)\n",
    "        author_list.append(v[0])\n",
    "        prob_list.append(v[1]/float(sum_sentences))\n",
    "\n",
    "    #print(author_probability_final)\n",
    "\n",
    "    final_autor_list = np.random.choice(\n",
    "      author_list, \n",
    "      count,\n",
    "      p=prob_list\n",
    "    )\n",
    "\n",
    "    print(\"returning: \" + str(final_autor_list))\n",
    "    return final_autor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "sentences = []\n",
    "\n",
    "def remove_non_frequent2(_list, post, replace=True):\n",
    "    good_words = []\n",
    "    for word in post.split():\n",
    "        if word in _list:\n",
    "            good_words.append(word)\n",
    "        else:\n",
    "            #print(word)\n",
    "            if replace:\n",
    "                replacement = \"\"\n",
    "                for char in word:\n",
    "                    replacement += \"-\"\n",
    "                good_words.append(replacement)\n",
    "    \n",
    "    return \" \".join(good_words)\n",
    "\n",
    "lines = open(\"1006/410.txt\", 'r').readlines()\n",
    "for line in lines:\n",
    "    if ':' in line:\n",
    "        author = line[:line.find(\":\")]\n",
    "        authors.append(author)\n",
    "        sentence = line[line.find(\":\"):]\n",
    "        sentences.append(sentence)\n",
    "\n",
    "lines = open(\"1006/501.txt\", 'r').readlines()\n",
    "for s in lines:\n",
    "    if ':' in s and 'has left the channel' not in s and 'has quit' not in s and 'joined the channel' not in s:\n",
    "        time = s[s.find(\"[\")+1:s.find(\"]\")]\n",
    "        author = s[s.find(\"<\")+1:s.find(\">\")]\n",
    "        sentence = s[s.find(\">\")+2:]\n",
    "        if not '[' in author:\n",
    "            authors.append(author)\n",
    "            sentences.append(sentence)\n",
    "            \n",
    "\n",
    "outfile = open('1006/5_gradient_text_410_501_2.txt', 'w')\n",
    "            \n",
    "for i in range(len(authors)):\n",
    "    perc = i/float(len(authors))\n",
    "    a = authors[i]\n",
    "    s = sentences[i]\n",
    "    \n",
    "    n = 100000 * (1 - perc) \n",
    "    top_n = []\n",
    "    for t in sorted_word_counts[:int(n)]:\n",
    "        top_n.append(t[0])\n",
    "    #print(len(top_n))\n",
    "    stripped_post = remove_non_frequent2(top_n, s.strip().lower(), True)\n",
    "    \n",
    "    outfile.write(a + \": \" + stripped_post + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
