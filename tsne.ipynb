{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t-SNE http://www.scikit-yb.org/en/latest/api/text/tsne.html\n",
    "\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import codecs\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.documents = codecs.open(sentences, encoding=\"utf-8\").readlines()\n",
    "        self.document_list = []\n",
    "        for d in self.documents:\n",
    "            self.document_list.append([d])\n",
    "\n",
    "def load_corpus():\n",
    "    c = Corpus(\"all_posts01.txt\")\n",
    "    return c\n",
    "    \n",
    "\n",
    "corpus = load_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf  = TfidfVectorizer(stop_words='english')\n",
    "from sklearn.cluster import KMeans\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                                 min_df=2,\n",
    "                                 use_idf=True)\n",
    "#transformer =  TfidfTransformer()\n",
    "#tfidf = make_pipeline(hasher,transformer)\n",
    "docs   = vectorizer.fit_transform(corpus.documents)\n",
    "\n",
    "print(docs)\n",
    "\n",
    "true_k = 500\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(docs)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for i in range(true_k):\n",
    "    #print(\"Cluster %d:\" % i)\n",
    "    #for ind in order_centroids[i, :10]:\n",
    "        #print(' %s' % terms[ind])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNEVisualizer(labels=[\"documents\"])\n",
    "tsne.fit(docs)\n",
    "tsne.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA, FastICA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "stri = 'Any girl being topless hey'\n",
    "response = vectorizer.transform([stri])\n",
    "r = [((i, j), response[i,j]) for i, j in zip(*response.nonzero())]\n",
    "for entry in r:\n",
    "    word = terms[entry[0][1]]\n",
    "    confidence = entry[1]\n",
    "    print(word)\n",
    "    print(float(confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61625 sentences\n",
      "building vocab\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "# used for loading or saving\n",
    "model_file = '/home/marcel/projects/phaedra/models/xxxchatters_posts05_-min20chars_20epochs_lower.d2v'\n",
    "filename = '/home/marcel/projects/phaedra/xxxchatters_posts05.txt'\n",
    "\n",
    "sentences = []\n",
    "from random import shuffle\n",
    "\n",
    "counter = 0\n",
    "for uid, line in enumerate(codecs.open(filename, encoding='utf-8')):\n",
    "    if len(line) > 20:\n",
    "        ls = gensim.models.doc2vec.LabeledSentence(words=line.lower().split(), tags=['SENT_%s' % counter])\n",
    "        sentences.append(ls)\n",
    "        counter += 1\n",
    "print(len(sentences),'sentences')\n",
    "\n",
    "\n",
    "# Training the doc2vec model\n",
    "\n",
    "# tutorial https://rare-technologies.com/doc2vec-tutorial/\n",
    "# proposes shuffling or learning reate adjustment. we gonna do both\n",
    "# in total 20 epochs\n",
    "model = gensim.models.Doc2Vec(size=100, window=8, min_count=2, workers=4)  # use fixed learning rate\n",
    "print('building vocab') \n",
    "model.build_vocab(sentences)\n",
    "\n",
    "base_alpha = model.alpha\n",
    "base_min_alpha = model.min_alpha\n",
    "\n",
    "for mepoch in range(2):\n",
    "    model.alpha = base_alpha \n",
    "    model.min_alpha = base_min_alpha\n",
    "    for epoch in range(10):\n",
    "        print('epoch',mepoch * 10 + epoch)\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "        model.alpha -= 0.002  # decrease the learning rate\n",
    "        model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "    #shuffle(sentences)\n",
    "\n",
    "# saving the model    \n",
    "model.save(model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108460 sentences\n",
      "14 Radio Meltdown: DJ`liltech is playing 6 George Thorogood - Bad To The Bone\n",
      "similar sentence 20\n",
      "\n",
      "SIMILAR SENTENCES\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-97f3abaa0638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nSIMILAR SENTENCES\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnice_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-97f3abaa0638>\u001b[0m in \u001b[0;36mnice_print\u001b[0;34m(tagged_doc)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Tiny helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnice_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Build sentence list (each sentence needs at least 1 tag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "# used for loading or saving\n",
    "#model_file = '/home/marcel/projects/phaedra/models/all_posts01.doc2vec'\n",
    "\n",
    "# Tiny helper\n",
    "def nice_print(tagged_doc):\n",
    "    return (' '.join(sentences[int(tagged_doc[0][5:])][0]))\n",
    "\n",
    "# Build sentence list (each sentence needs at least 1 tag)\n",
    "#filename = '/home/marcel/projects/phaedra/all_posts01.txt'\n",
    "\n",
    "#model_file = '/home/marcel/projects/phaedra/models/xxxchatters_longer_training-min15chars2.d2v'\n",
    "#filename = '/home/marcel/projects/phaedra/xxxchatters_posts.txt'\n",
    "\n",
    "#sentences = []\n",
    "#from random import shuffle\n",
    "\n",
    "#for uid, line in enumerate(codecs.open(filename, encoding='utf-8')):\n",
    "#    if len(line) > 15:\n",
    "#        ls = gensim.models.doc2vec.LabeledSentence(words=line.split(), tags=['SENT_%s' % uid])\n",
    "#        sentences.append(ls)\n",
    "#print(len(sentences),'sentences')\n",
    "\n",
    "#model_loaded = gensim.models.Doc2Vec.load(model_file)\n",
    "model_loaded = model\n",
    "\n",
    "number = 201\n",
    "print(' '.join(sentences[number][0]))\n",
    "sims = model_loaded.docvecs.most_similar('SENT_'+str(number),topn=20)\n",
    "print('similar sentence',len(sims))\n",
    "print('\\nSIMILAR SENTENCES\\n')\n",
    "for sim in sims:\n",
    "    print(nice_print(sim))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model_loaded[model_loaded.wv.vocab]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=0.1)\n",
    "plt.savefig(\"out_xxxchatters.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have to maintain some discretion lol\n",
      "kylie: lol i have some tettleys 28827\n",
      "i have to maintain some discretion lol 819\n",
      "some reciprocation, please, lol 32834\n",
      "watching some ofsilfs (olympic figure skater) 60334\n",
      "i can see some advantages 47594\n",
      "has anyone seen some lettuce? 60373\n",
      "we all have our moments lol 25576\n",
      "some nights more than others ... 52033\n",
      "but, i have some girth! 10892\n",
      "no accounting for some tastes. lol 8384\n",
      "13\u0002went dancing with some freinds\u0002 44877\n",
      "some follow that closely. 31295\n",
      "any females free to consider some rp roles, pm me please 12533\n",
      "tossed filthywife some soap 25933\n",
      "the kiwis have some rockin pipers 3587\n",
      "some would say wastefull 44095\n",
      "13recycled champayne lol 41046\n",
      "let me guess....too many? lol 53188\n",
      "look at all the colors! lol 7552\n",
      "better..scotsman lol 5187\n",
      "gotta have some standards. 32551\n",
      "lol there's some baha men in here 25572\n",
      "i've honestly never been lol 4607\n",
      "i have some pizza in the fridge 11217\n",
      "prepping`pudding: sprung a leak lol 9579\n",
      "lol i have coconut cookies 25852\n",
      "lol fitbunny[n]-around 4688\n",
      "some times... some days 16024\n",
      "some places anyways:p 7813\n",
      "i want some kona coffee 46129\n",
      "12,00at times, some guidance increaases the 'better' 7974\n",
      "como te pueda hacer entiendes, lol 35838\n",
      "eating some cherries 10554\n",
      "i have burial insurance, lol 51703\n",
      "i have a blocker anyway lol 21973\n",
      "had a 1918 solera some years ago 52855\n",
      "any ladies fancy some rp? 56151\n",
      "was some porridge she had 58893\n",
      "stir fry some veggies 59704\n",
      "listening some cats yowling 33314\n",
      "not the same thing lol 61206\n",
      "offers katiets some crackers 55292\n",
      "so are some dicks, malex lol 5237\n",
      "someone for some private roleplay ? 48402\n",
      "schlemie schlemazzle, lol 44358\n",
      "noooooooooo!!! i do have some standards!!!!!! 14569\n",
      "that is some serious action 8656\n",
      "harvey weinstein's here! lol 38632\n",
      "like some sailor and ports 7773\n",
      "you dont have to tell me lol 42773\n",
      "02wow. i didn't know that lol 3039\n",
      "some rapper probably 10954\n",
      "12came and went, lol 53679\n",
      "you have friends? lol 13714\n",
      "i think i should get some sleep 35959\n",
      "any ladies fancy some rp?> 4823\n",
      "start taking some vitamins 10270\n",
      "06hi, again....everyone? whoever is awake? lol 60830\n",
      "toys of mass destruction lol 48959\n",
      "ette..less growl lol 52208\n",
      "i should get some footage of the v65 i guess. 52353\n",
      "12into some wild mmmf? 2809\n",
      "for the first couple years lol 26811\n",
      "anyone interested in some rp? 39206\n",
      "bird surveys of some estuary 17157\n",
      "any ladies care for some rp? 24489\n",
      "you put on some weight? 27292\n",
      "ewwwwwwwwwwwwwwww lol 5178\n",
      "topppppppppppppppp lol 11519\n",
      "some are schitzo, some have group hysteria 54509\n",
      "someon for some steamy private ? 31217\n",
      "someone else want to get some of sara{abs}? 17482\n",
      "ouch battercat!! lol 31702\n",
      "ewwwwwwwwwwwwwwwwww lol 34817\n",
      "that is some nick! :) 14390\n",
      "i'm going to be glued to the pc. lol 16201\n",
      "some people just have the most inappropriate nicks. 38216\n",
      "shud is that english or some form swahili lol 1862\n",
      "12,00needs lunch, of some sort 52566\n",
      "12,00a conservative cheesehead.... lol 36121\n",
      "lol angelaa to sirdutch lol 26538\n",
      "and kisssssssssses lol 28657\n",
      "that sounds like pregnancy cravings lol 39595\n",
      "it was carebear. lol 34720\n",
      "saving some for youuuuuu toooooo 25159\n",
      "sounds like errections, some have them, some having problems calming it down.. and some need help getting some. 57708\n",
      "i luvvv strawberrys lol 39246\n",
      "06she pissfarted lol 3327\n",
      "i have two trophies tho..oldest and most perverted lol 42136\n",
      "someone for some pv ? 25060\n",
      "ooohh, diverwolf lol 58205\n",
      "what have i missed, lol 59144\n",
      "i didnt know that lol 60940\n",
      "any dominant f for some creative rp fun? pm me your ideas 51363\n",
      "apparently, roughone. lol 46899\n",
      "i should make some pizza right now 19583\n",
      "\u0002man i could go for some wings tho\u0002 25211\n",
      "i can't come on that often, but i have some downtime 46784\n",
      "okay...maybe i should give my pillow some head 7930\n",
      "monosodium glutamate invictus lol 28268\n",
      "i have solved my dilemma! 34249\n",
      "gigabit in some places 7039\n",
      "wanna get in some trouble 46231\n",
      "impatiently impatient jamal is impatient, lol 58397\n",
      "12,00on a negaive note.... lol 49058\n",
      "crustyyyyyyyyyyyyyyyyyyy lol 34664\n",
      "that coming from a kangaroo lol 60360\n",
      "\"ich bin ein berliner\" lol 2742\n",
      "bought some bottles of it 35312\n",
      "all i have is some sunny-d 59928\n",
      "hiscumtastessweet..liar lol 51016\n",
      "wish i knew what that was and that i had some lol 9242\n",
      "dibs on the unicorn! lol 59205\n",
      "10i have some james patterson 29401\n",
      "!pp scorpionrose for some 4929\n",
      "some poorly skilled drivers these days 21353\n",
      "yikes, some wackos in here 2820\n",
      "any f for some creative rp fun? pm me you rideas 19711\n",
      "any dominant females? 7425\n",
      "do you have pictures? lol 30537\n",
      "any domme ladies here for chat with newbie 36414\n",
      "lol nonbinary-fox[k_n] 22453\n",
      "gee whos following who, lol 56472\n",
      "too many j's in here lol 1117\n",
      "lol the factory of sadness 1309\n",
      "i never said he couldnt lol 44277\n",
      "10i didn't know that lol 13704\n",
      "wake up finally lebo? lol 20796\n",
      "any female for roleplay? 22387\n",
      "anyone interested in some rp? 18471\n",
      "get some stomp in your boots 43382\n",
      "who said you were lol 5128\n",
      "true, some more than others today 35222\n",
      "lol nothing wrong with sipping 22407\n",
      "no worries, have fun lol 12530\n",
      "i dont drink gin. lol 15915\n",
      "ill make up some poutine 12011\n",
      "doctor howzer recommends some rest. doctor demento recommends some ray stevens 51062\n",
      "we need some chaff to fill the void 27500\n",
      "any tops for chat/rp? pm me 40897\n",
      "hugggggggggggsssssss shynica lol 17754\n",
      "fox you need to learn some manners 44268\n",
      "did i miss anything? lol 37591\n",
      "oyfs tisch sett dreidle, lol 5851\n",
      "f looking for a creative m roleplayer or two. pm me with some ideas please 53152\n",
      "lol justme....... hint taken. 23646\n",
      "some guys enjoy hairy pussys. 11460\n",
      "any ladies care for some rp? 21831\n",
      "was that a mock clap missy lol 50669\n",
      "not here yet dear. lol 42297\n",
      "in your cooter mazzy lol 53524\n",
      "any ladies fancy some rp? 37932\n",
      "or well.. some outgoing, end of the queue for incoming 37765\n",
      "some interesting nicknames 21567\n",
      "there were some flaws i guess 14731\n",
      "when he was on house arrest lol 4159\n",
      "speaking of eating lol 22237\n",
      "its my own design lol 40403\n",
      "thats not what i said lol 57972\n",
      "!request chevelle - the red 51954\n",
      "i seem to draw those lol 43354\n",
      "used to be alot of fannys, lol 39838\n",
      "\u0002been looking online for some more\u0002 35218\n",
      "there is no other way ;) 60330\n",
      "don't know your preferences. lol 55280\n",
      "isnt that colorado? lol 42216\n",
      "lol roughone no kidding eh 43343\n",
      "i'm not paying annnyyyyyyyy attention lol 13644\n",
      "entertain wookie lol 51758\n",
      "heyyyyyyyyyyyyyy no tickling lol 196\n",
      "calm down there scooby lol 32873\n",
      "some yes. i prefer massages 35329\n",
      "how are you harumi_ ? 35691\n",
      "hollywood has no logic. lol 26708\n",
      "10i expect some perks ;) 4795\n",
      "some people's kids .... 8236\n",
      "welcome back davram :) 44725\n",
      "jj45: didin notice lol 22472\n",
      "someone for some steamy pv? 5250\n",
      "i acquired some irish music for my st paddy's show on the 17th 49503\n",
      "i donâ€™t have monies. 17696\n",
      "xdjio no posting of links 50971\n",
      "i only have 3 tattoos 9585\n",
      "hiya nordicgoddess :) 30918\n",
      "at least let him grab your ass lol 51361\n",
      "why is this happening? lol 5312\n",
      "lol its flick your bic 9826\n",
      "im glad somone cought that lol 17973\n",
      "i had some pasta with my chicken last night 53470\n",
      "oooooo noooooo .. no ouchies :p 45761\n",
      "well im waiting for an opportunity for the highest % of success wildkat lol 44054\n",
      "i need to get some chicken 46663\n",
      "i want to sniff some armpits 45586\n",
      "sheesh, everyone has bikini weather, lol 40750\n",
      "this one has potential 46935\n",
      "bonnie tyler mad_mick lol ? 39279\n",
      "!seen lebo_getting_handjob 36758\n",
      "korean is just korean, lol 13070\n",
      "whisers in simians ear lol 44707\n",
      "any bi guys wanna cum pvt for me ?? 26624\n"
     ]
    }
   ],
   "source": [
    "def get_similar(model, words, top_n):\n",
    "    inferred_vector = model.infer_vector(words, steps=1000, alpha=0.025)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=top_n)\n",
    "    return sims\n",
    "\n",
    "#sen = \"i love it when ladies want top shelf\".lower()\n",
    "#sen = \"toilet shit\".lower()\n",
    "sen = \"i have to maintain some discretion lol\".lower()\n",
    "print(sen)\n",
    "result = get_similar(model, sen.split(), 200)\n",
    "#print(result)\n",
    "for res in result:\n",
    "    index = res[0].split('_')[1]\n",
    "    print(' '.join(sentences[int(index)].words), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have to maintain some discretion lol\n",
      "\n",
      "SIMILAR SENTENCES\n",
      "\n",
      "kylie: lol i have some tettleys\n",
      "any females free to consider some rp roles, pm me please\n",
      "some places anyways:p\n",
      "some times... some days\n",
      "offers katiets some crackers\n",
      "any ladies fancy some rp?>\n",
      "some reciprocation, please, lol\n",
      "12into some wild mmmf?\n",
      "any ladies fancy some rp?\n",
      "tossed filthywife some soap\n",
      "some nights more than others ...\n",
      "anyone interested in some rp?\n",
      "doctor howzer recommends some rest. doctor demento recommends some ray stevens\n",
      "fox you need to learn some manners\n",
      "the kiwis have some rockin pipers\n",
      "gotta have some standards.\n",
      "i can see some advantages\n",
      "some follow that closely.\n",
      "watching some ofsilfs (olympic figure skater)\n",
      "you put on some weight?\n",
      "12,00at times, some guidance increaases the 'better'\n",
      "stir fry some veggies\n",
      "listening some cats yowling\n",
      "saving some for youuuuuu toooooo\n",
      "someone for some steamy pv\n",
      "had a 1918 solera some years ago\n",
      "some would say wastefull\n",
      "gigabit in some places\n",
      "12,00needs lunch, of some sort\n",
      "yikes, some wackos in here\n",
      "dj`vu you need some moonshine\n",
      "some people just have the most inappropriate nicks.\n",
      "has anyone seen some lettuce?\n",
      "like some sailor and ports\n",
      "any f for some creative rp fun? pm me you rideas\n",
      "someone for some steamy pv?\n",
      "13\u0002went dancing with some freinds\u0002\n",
      "any ladies care for some rp?\n",
      "was some porridge she had\n",
      "i want some kona coffee\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "number = random.randint(0, len(sentences))\n",
    "number = 819\n",
    "print(' '.join(sentences[number][0]))\n",
    "sims = model.docvecs.most_similar('SENT_'+str(number),topn=40)\n",
    "print('\\nSIMILAR SENTENCES\\n')\n",
    "for sim in sims:\n",
    "    print(nice_print(sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (' '.join(sentences[9][0]))\n",
    "#sims = model.docvecs.most_similar('SENT_10')\n",
    "#print('similar sentence',len(sims))\n",
    "#print('\\nSIMILAR SENTENCES\\n')\n",
    "#for sim in sims:\n",
    "#    print(nice_print(sim))\n",
    "\n",
    "\n",
    "# Tiny helper\n",
    "def nice_print(tagged_doc, print_doc):\n",
    "    if print_doc:\n",
    "        print(tagged_doc)\n",
    "    return(' '.join(sentences[int(tagged_doc[0][5:])][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
